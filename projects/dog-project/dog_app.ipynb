{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM7634\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "#print(sorted(glob(\"dogImages/train/*/\")))\n",
    "#print(dog_names)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lfw\\\\Muhammad_Ali\\\\Muhammad_Ali_0001.jpg'\n",
      " 'lfw\\\\Yoko_Ono\\\\Yoko_Ono_0001.jpg'\n",
      " 'lfw\\\\Svetlana_Koroleva\\\\Svetlana_Koroleva_0002.jpg'\n",
      " 'lfw\\\\Laurence_Fishburne\\\\Laurence_Fishburne_0001.jpg'\n",
      " 'lfw\\\\Jennifer_Capriati\\\\Jennifer_Capriati_0007.jpg'\n",
      " 'lfw\\\\George_W_Bush\\\\George_W_Bush_0399.jpg'\n",
      " 'lfw\\\\Tom_Ridge\\\\Tom_Ridge_0001.jpg'\n",
      " 'lfw\\\\Catherine_Woodard\\\\Catherine_Woodard_0001.jpg'\n",
      " 'lfw\\\\Fabricio_Oberto\\\\Fabricio_Oberto_0001.jpg'\n",
      " 'lfw\\\\Pete_Sampras\\\\Pete_Sampras_0011.jpg']\n",
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "#print(human_files)\n",
    "random.shuffle(human_files)\n",
    "print(human_files[:10])\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfw\\George_W_Bush\\George_W_Bush_0399.jpg\n",
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x749358d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[5])\n",
    "print(human_files[5])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of human faces detected in first 100 human images:99%\n",
      "Percentage of human faces detected in first 100 dog images:12%\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "img_seq=0\n",
    "human_faces_found=0\n",
    "dog_faces_found=0\n",
    "while (img_seq < len(human_files_short)):\n",
    "    if face_detector(human_files_short[img_seq]):\n",
    "        #print(\"human_files_short[img_seq] face found\")\n",
    "        human_faces_found +=1\n",
    "    if face_detector(dog_files_short[img_seq]):\n",
    "        #print(\"dog_files_short face found\", img_seq)\n",
    "        dog_faces_found +=1\n",
    "    img_seq +=1\n",
    "\n",
    "## on the images in human_files_short and dog_files_short.\n",
    "CBLUE = '\\33[34m'\n",
    "CEND = '\\33[0m'\n",
    "print(CBLUE+\"Percentage of human faces detected in first 100 human images:{0:.0f}%\".format(human_faces_found/len(human_files_short)*100) + CEND)\n",
    "print(CBLUE+\"Percentage of human faces detected in first 100 dog images:{0:.0f}%\".format(dog_faces_found/len(dog_files_short)*100)+ CEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_files_short face found 0\n",
      "dog_files_short face found 1\n",
      "dog_files_short face found 2\n",
      "dog_files_short face found 3\n",
      "dog_files_short face found 4\n",
      "dog_files_short face found 5\n",
      "dog_files_short face found 6\n",
      "dog_files_short face found 7\n",
      "dog_files_short face found 8\n",
      "dog_files_short face found 9\n",
      "dog_files_short face found 10\n",
      "dog_files_short face found 11\n",
      "dog_files_short face found 12\n",
      "dog_files_short face found 13\n",
      "dog_files_short face found 14\n",
      "dog_files_short face found 15\n",
      "dog_files_short face found 16\n",
      "dog_files_short face found 17\n",
      "dog_files_short face found 18\n",
      "dog_files_short face found 19\n",
      "dog_files_short face found 20\n",
      "dog_files_short face found 21\n",
      "dog_files_short face found 22\n",
      "dog_files_short face found 23\n",
      "dog_files_short face found 24\n",
      "dog_files_short face found 25\n",
      "dog_files_short face found 26\n",
      "dog_files_short face found 27\n",
      "dog_files_short face found 28\n",
      "dog_files_short face found 29\n",
      "dog_files_short face found 30\n",
      "dog_files_short face found 31\n",
      "dog_files_short face found 32\n",
      "dog_files_short face found 33\n",
      "dog_files_short face found 34\n",
      "dog_files_short face found 35\n",
      "dog_files_short face found 36\n",
      "dog_files_short face found 37\n",
      "dog_files_short face found 38\n",
      "dog_files_short face found 39\n",
      "dog_files_short face found 40\n",
      "dog_files_short face found 41\n",
      "dog_files_short face found 42\n",
      "dog_files_short face found 43\n",
      "dog_files_short face found 44\n",
      "dog_files_short face found 45\n",
      "dog_files_short face found 46\n",
      "dog_files_short face found 47\n",
      "dog_files_short face found 48\n",
      "dog_files_short face found 49\n",
      "dog_files_short face found 50\n",
      "dog_files_short face found 51\n",
      "dog_files_short face found 52\n",
      "dog_files_short face found 53\n",
      "dog_files_short face found 54\n",
      "dog_files_short face found 55\n",
      "dog_files_short face found 56\n",
      "dog_files_short face found 57\n",
      "dog_files_short face found 58\n",
      "dog_files_short face found 59\n",
      "dog_files_short face found 60\n",
      "dog_files_short face found 61\n",
      "dog_files_short face found 62\n",
      "dog_files_short face found 63\n",
      "dog_files_short face found 64\n",
      "dog_files_short face found 65\n",
      "dog_files_short face found 66\n",
      "dog_files_short face found 67\n",
      "dog_files_short face found 68\n",
      "dog_files_short face found 69\n",
      "dog_files_short face found 70\n",
      "dog_files_short face found 71\n",
      "dog_files_short face found 72\n",
      "dog_files_short face found 73\n",
      "dog_files_short face found 74\n",
      "dog_files_short face found 75\n",
      "dog_files_short face found 76\n",
      "dog_files_short face found 77\n",
      "dog_files_short face found 78\n",
      "dog_files_short face found 79\n",
      "dog_files_short face found 80\n",
      "dog_files_short face found 81\n",
      "dog_files_short face found 82\n",
      "dog_files_short face found 83\n",
      "dog_files_short face found 84\n",
      "dog_files_short face found 85\n",
      "dog_files_short face found 86\n",
      "dog_files_short face found 87\n",
      "dog_files_short face found 88\n",
      "dog_files_short face found 89\n",
      "dog_files_short face found 90\n",
      "dog_files_short face found 91\n",
      "dog_files_short face found 92\n",
      "dog_files_short face found 93\n",
      "dog_files_short face found 94\n",
      "dog_files_short face found 95\n",
      "dog_files_short face found 96\n",
      "dog_files_short face found 97\n",
      "dog_files_short face found 98\n",
      "dog_files_short face found 99\n",
      "Percentage of dog faces detected in first 100 human images:1%\n",
      "Percentage of dog faces detected in first 100 dog images:100%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "\n",
    "img_seq=0\n",
    "human_faces_found=0\n",
    "dog_faces_found=0\n",
    "while (img_seq < len(human_files_short)):\n",
    "    if dog_detector(human_files_short[img_seq]):\n",
    "        #print(\"human_files_short[img_seq] face found\")\n",
    "        human_faces_found +=1\n",
    "    if dog_detector(dog_files_short[img_seq]):\n",
    "        print(\"dog_files_short face found\", img_seq)\n",
    "        dog_faces_found +=1\n",
    "    img_seq +=1\n",
    "\n",
    "## on the images in human_files_short and dog_files_short.\n",
    "CBLUE = '\\33[34m'\n",
    "CEND = '\\33[0m'\n",
    "print(CBLUE+\"Percentage of dog faces detected in first 100 human images:{0:.0f}%\".format(human_faces_found/len(human_files_short)*100) + CEND)\n",
    "print(CBLUE+\"Percentage of dog faces detected in first 100 dog images:{0:.0f}%\".format(dog_faces_found/len(dog_files_short)*100)+ CEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [01:03<00:00, 104.56it/s]\n",
      "100%|| 835/835 [00:08<00:00, 103.32it/s]\n",
      "100%|| 836/836 [00:07<00:00, 107.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 220, 220, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 219, 219, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 186624)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               37325000  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               26733     \n",
      "=================================================================\n",
      "Total params: 37,362,277\n",
      "Trainable params: 37,362,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2, strides=1, padding='valid', activation='relu', \n",
    "                 input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=1, padding='valid', data_format=None))\n",
    "\n",
    "### Conv2d -- second layer\n",
    "model.add(Conv2D(32, kernel_size=2, strides=1, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "### Conv2d -- third layer\n",
    "model.add(Conv2D(64, kernel_size=2, strides=1, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "\n",
    "### Global average\n",
    "#model.add(GlobalAveragePooling2D(data_format=None))\n",
    "\n",
    "###Convert 3D into 1D features vector\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "4020/6680 [=================>............] - ETA: 29:58 - loss: 4.9043 - acc: 0.0000e+ - ETA: 22:30 - loss: 10.5112 - acc: 0.0000e+0 - ETA: 19:57 - loss: 12.3801 - acc: 0.0000e+0 - ETA: 18:51 - loss: 13.3146 - acc: 0.0000e+0 - ETA: 18:05 - loss: 13.8753 - acc: 0.0000e+0 - ETA: 17:37 - loss: 14.2491 - acc: 0.0000e+0 - ETA: 17:13 - loss: 14.2522 - acc: 0.0143    - ETA: 16:56 - loss: 14.4854 - acc: 0.012 - ETA: 16:39 - loss: 14.5773 - acc: 0.016 - ETA: 16:27 - loss: 14.6737 - acc: 0.015 - ETA: 16:13 - loss: 14.7083 - acc: 0.013 - ETA: 16:02 - loss: 14.2605 - acc: 0.016 - ETA: 15:53 - loss: 13.5533 - acc: 0.015 - ETA: 15:45 - loss: 12.9454 - acc: 0.014 - ETA: 15:38 - loss: 12.4102 - acc: 0.016 - ETA: 15:30 - loss: 11.9457 - acc: 0.015 - ETA: 15:23 - loss: 11.5310 - acc: 0.017 - ETA: 15:16 - loss: 11.1643 - acc: 0.016 - ETA: 15:10 - loss: 10.8359 - acc: 0.015 - ETA: 15:04 - loss: 10.5379 - acc: 0.017 - ETA: 14:59 - loss: 10.2681 - acc: 0.019 - ETA: 14:54 - loss: 10.0249 - acc: 0.018 - ETA: 14:48 - loss: 9.8036 - acc: 0.017 - ETA: 14:44 - loss: 9.5990 - acc: 0.01 - ETA: 14:39 - loss: 9.4110 - acc: 0.01 - ETA: 14:35 - loss: 9.2369 - acc: 0.01 - ETA: 14:30 - loss: 9.0755 - acc: 0.01 - ETA: 14:26 - loss: 8.9255 - acc: 0.01 - ETA: 14:22 - loss: 8.7857 - acc: 0.01 - ETA: 14:18 - loss: 8.6546 - acc: 0.01 - ETA: 14:14 - loss: 8.5345 - acc: 0.01 - ETA: 14:10 - loss: 8.4200 - acc: 0.01 - ETA: 14:06 - loss: 8.3132 - acc: 0.01 - ETA: 14:03 - loss: 8.2133 - acc: 0.01 - ETA: 13:59 - loss: 8.1200 - acc: 0.01 - ETA: 13:55 - loss: 8.0301 - acc: 0.01 - ETA: 13:52 - loss: 7.9451 - acc: 0.01 - ETA: 13:48 - loss: 7.8653 - acc: 0.01 - ETA: 13:44 - loss: 7.7892 - acc: 0.01 - ETA: 13:41 - loss: 7.7163 - acc: 0.01 - ETA: 13:37 - loss: 7.6470 - acc: 0.01 - ETA: 13:34 - loss: 7.5816 - acc: 0.01 - ETA: 13:31 - loss: 7.5191 - acc: 0.01 - ETA: 13:27 - loss: 7.4595 - acc: 0.01 - ETA: 13:24 - loss: 7.4027 - acc: 0.01 - ETA: 13:21 - loss: 7.3476 - acc: 0.01 - ETA: 13:18 - loss: 7.2954 - acc: 0.01 - ETA: 13:15 - loss: 7.2452 - acc: 0.01 - ETA: 13:11 - loss: 7.1980 - acc: 0.01 - ETA: 13:08 - loss: 7.1514 - acc: 0.01 - ETA: 13:05 - loss: 7.1099 - acc: 0.01 - ETA: 13:02 - loss: 7.0671 - acc: 0.01 - ETA: 12:59 - loss: 7.0261 - acc: 0.01 - ETA: 12:55 - loss: 6.9869 - acc: 0.01 - ETA: 12:52 - loss: 6.9490 - acc: 0.01 - ETA: 12:49 - loss: 6.9123 - acc: 0.01 - ETA: 12:46 - loss: 6.8769 - acc: 0.01 - ETA: 12:43 - loss: 6.8425 - acc: 0.01 - ETA: 12:40 - loss: 6.8093 - acc: 0.01 - ETA: 12:37 - loss: 6.7774 - acc: 0.01 - ETA: 12:34 - loss: 6.7465 - acc: 0.01 - ETA: 12:31 - loss: 6.7164 - acc: 0.01 - ETA: 12:29 - loss: 6.6871 - acc: 0.01 - ETA: 12:25 - loss: 6.6595 - acc: 0.01 - ETA: 12:22 - loss: 6.6336 - acc: 0.01 - ETA: 12:20 - loss: 6.6072 - acc: 0.01 - ETA: 12:17 - loss: 6.5815 - acc: 0.01 - ETA: 12:14 - loss: 6.5564 - acc: 0.01 - ETA: 12:11 - loss: 6.5321 - acc: 0.01 - ETA: 12:08 - loss: 6.5088 - acc: 0.01 - ETA: 12:05 - loss: 6.4866 - acc: 0.01 - ETA: 12:02 - loss: 6.4646 - acc: 0.01 - ETA: 11:59 - loss: 6.4429 - acc: 0.01 - ETA: 11:56 - loss: 6.4218 - acc: 0.01 - ETA: 11:53 - loss: 6.4015 - acc: 0.01 - ETA: 11:50 - loss: 6.3816 - acc: 0.01 - ETA: 11:47 - loss: 6.3622 - acc: 0.01 - ETA: 11:44 - loss: 6.3433 - acc: 0.01 - ETA: 11:41 - loss: 6.3250 - acc: 0.01 - ETA: 11:39 - loss: 6.3069 - acc: 0.01 - ETA: 11:36 - loss: 6.2905 - acc: 0.01 - ETA: 11:33 - loss: 6.2736 - acc: 0.01 - ETA: 11:30 - loss: 6.2565 - acc: 0.01 - ETA: 11:27 - loss: 6.2403 - acc: 0.01 - ETA: 11:24 - loss: 6.2241 - acc: 0.01 - ETA: 11:21 - loss: 6.2097 - acc: 0.01 - ETA: 11:19 - loss: 6.1942 - acc: 0.01 - ETA: 11:16 - loss: 6.1792 - acc: 0.01 - ETA: 11:13 - loss: 6.1648 - acc: 0.01 - ETA: 11:10 - loss: 6.1513 - acc: 0.01 - ETA: 11:07 - loss: 6.1371 - acc: 0.01 - ETA: 11:04 - loss: 6.1235 - acc: 0.01 - ETA: 11:01 - loss: 6.1100 - acc: 0.01 - ETA: 10:58 - loss: 6.0972 - acc: 0.01 - ETA: 10:56 - loss: 6.0851 - acc: 0.01 - ETA: 10:53 - loss: 6.0726 - acc: 0.01 - ETA: 10:50 - loss: 6.0602 - acc: 0.01 - ETA: 10:47 - loss: 6.0475 - acc: 0.01 - ETA: 10:44 - loss: 6.0358 - acc: 0.01 - ETA: 10:41 - loss: 6.0243 - acc: 0.01 - ETA: 10:38 - loss: 6.0136 - acc: 0.01 - ETA: 10:36 - loss: 6.0025 - acc: 0.01 - ETA: 10:33 - loss: 5.9917 - acc: 0.01 - ETA: 10:30 - loss: 5.9811 - acc: 0.01 - ETA: 10:27 - loss: 5.9708 - acc: 0.01 - ETA: 10:24 - loss: 5.9604 - acc: 0.01 - ETA: 10:22 - loss: 5.9504 - acc: 0.01 - ETA: 10:19 - loss: 5.9412 - acc: 0.01 - ETA: 10:16 - loss: 5.9314 - acc: 0.01 - ETA: 10:13 - loss: 5.9223 - acc: 0.01 - ETA: 10:10 - loss: 5.9129 - acc: 0.01 - ETA: 10:08 - loss: 5.9038 - acc: 0.01 - ETA: 10:05 - loss: 5.8948 - acc: 0.01 - ETA: 10:02 - loss: 5.8857 - acc: 0.01 - ETA: 9:59 - loss: 5.8770 - acc: 0.0130 - ETA: 9:57 - loss: 5.8684 - acc: 0.012 - ETA: 9:54 - loss: 5.8602 - acc: 0.013 - ETA: 9:51 - loss: 5.8519 - acc: 0.013 - ETA: 9:48 - loss: 5.8442 - acc: 0.013 - ETA: 9:46 - loss: 5.8364 - acc: 0.013 - ETA: 9:43 - loss: 5.8285 - acc: 0.013 - ETA: 9:40 - loss: 5.8209 - acc: 0.013 - ETA: 9:37 - loss: 5.8132 - acc: 0.013 - ETA: 9:34 - loss: 5.8057 - acc: 0.013 - ETA: 9:31 - loss: 5.7982 - acc: 0.013 - ETA: 9:28 - loss: 5.7909 - acc: 0.013 - ETA: 9:25 - loss: 5.7839 - acc: 0.013 - ETA: 9:23 - loss: 5.7767 - acc: 0.012 - ETA: 9:20 - loss: 5.7710 - acc: 0.012 - ETA: 9:17 - loss: 5.7643 - acc: 0.012 - ETA: 9:14 - loss: 5.7574 - acc: 0.013 - ETA: 9:11 - loss: 5.7508 - acc: 0.012 - ETA: 9:08 - loss: 5.7442 - acc: 0.012 - ETA: 9:05 - loss: 5.7376 - acc: 0.012 - ETA: 9:02 - loss: 5.7312 - acc: 0.012 - ETA: 9:00 - loss: 5.7244 - acc: 0.012 - ETA: 8:57 - loss: 5.7185 - acc: 0.012 - ETA: 8:54 - loss: 5.7125 - acc: 0.012 - ETA: 8:51 - loss: 5.7065 - acc: 0.012 - ETA: 8:48 - loss: 5.7010 - acc: 0.012 - ETA: 8:46 - loss: 5.6949 - acc: 0.012 - ETA: 8:43 - loss: 5.6891 - acc: 0.012 - ETA: 8:40 - loss: 5.6842 - acc: 0.012 - ETA: 8:37 - loss: 5.6784 - acc: 0.012 - ETA: 8:34 - loss: 5.6732 - acc: 0.012 - ETA: 8:31 - loss: 5.6679 - acc: 0.012 - ETA: 8:28 - loss: 5.6626 - acc: 0.012 - ETA: 8:26 - loss: 5.6573 - acc: 0.012 - ETA: 8:23 - loss: 5.6520 - acc: 0.012 - ETA: 8:20 - loss: 5.6468 - acc: 0.012 - ETA: 8:17 - loss: 5.6415 - acc: 0.011 - ETA: 8:14 - loss: 5.6361 - acc: 0.011 - ETA: 8:12 - loss: 5.6335 - acc: 0.012 - ETA: 8:09 - loss: 5.6284 - acc: 0.012 - ETA: 8:06 - loss: 5.6236 - acc: 0.012 - ETA: 8:03 - loss: 5.6189 - acc: 0.012 - ETA: 8:01 - loss: 5.6137 - acc: 0.012 - ETA: 7:58 - loss: 5.6087 - acc: 0.012 - ETA: 7:55 - loss: 5.6041 - acc: 0.011 - ETA: 7:52 - loss: 5.5996 - acc: 0.011 - ETA: 7:49 - loss: 5.5949 - acc: 0.011 - ETA: 7:47 - loss: 5.5911 - acc: 0.011 - ETA: 7:44 - loss: 5.5868 - acc: 0.011 - ETA: 7:41 - loss: 5.5824 - acc: 0.012 - ETA: 7:38 - loss: 5.5778 - acc: 0.012 - ETA: 7:35 - loss: 5.5740 - acc: 0.012 - ETA: 7:33 - loss: 5.5701 - acc: 0.012 - ETA: 7:30 - loss: 5.5661 - acc: 0.011 - ETA: 7:27 - loss: 5.5620 - acc: 0.011 - ETA: 7:24 - loss: 5.5581 - acc: 0.011 - ETA: 7:22 - loss: 5.5539 - acc: 0.012 - ETA: 7:19 - loss: 5.5498 - acc: 0.012 - ETA: 7:16 - loss: 5.5463 - acc: 0.012 - ETA: 7:13 - loss: 5.5425 - acc: 0.012 - ETA: 7:10 - loss: 5.5382 - acc: 0.012 - ETA: 7:08 - loss: 5.5343 - acc: 0.012 - ETA: 7:05 - loss: 5.5307 - acc: 0.012 - ETA: 7:02 - loss: 5.5269 - acc: 0.012 - ETA: 6:59 - loss: 5.5230 - acc: 0.012 - ETA: 6:57 - loss: 5.5200 - acc: 0.012 - ETA: 6:54 - loss: 5.5165 - acc: 0.012 - ETA: 6:51 - loss: 5.5131 - acc: 0.012 - ETA: 6:48 - loss: 5.5089 - acc: 0.012 - ETA: 6:46 - loss: 5.5076 - acc: 0.012 - ETA: 6:43 - loss: 5.5041 - acc: 0.012 - ETA: 6:40 - loss: 5.5004 - acc: 0.012 - ETA: 6:37 - loss: 5.4975 - acc: 0.012 - ETA: 6:34 - loss: 5.4941 - acc: 0.012 - ETA: 6:32 - loss: 5.4908 - acc: 0.012 - ETA: 6:29 - loss: 5.4879 - acc: 0.012 - ETA: 6:26 - loss: 5.4845 - acc: 0.012 - ETA: 6:23 - loss: 5.4810 - acc: 0.012 - ETA: 6:21 - loss: 5.4781 - acc: 0.012 - ETA: 6:18 - loss: 5.4748 - acc: 0.012 - ETA: 6:15 - loss: 5.4715 - acc: 0.012 - ETA: 6:13 - loss: 5.4684 - acc: 0.012 - ETA: 6:10 - loss: 5.4651 - acc: 0.012 - ETA: 6:07 - loss: 5.4617 - acc: 0.012 - ETA: 6:04 - loss: 5.4590 - acc: 0.012 - ETA: 6:02 - loss: 5.4556 - acc: 0.012 - ETA: 5:59 - loss: 5.4523 - acc: 0.0126680/6680 [==============================] - ETA: 5:56 - loss: 5.4493 - acc: 0.012 - ETA: 5:53 - loss: 5.4463 - acc: 0.012 - ETA: 5:51 - loss: 5.4438 - acc: 0.012 - ETA: 5:48 - loss: 5.4412 - acc: 0.012 - ETA: 5:45 - loss: 5.4380 - acc: 0.012 - ETA: 5:42 - loss: 5.4350 - acc: 0.012 - ETA: 5:40 - loss: 5.4324 - acc: 0.012 - ETA: 5:37 - loss: 5.4299 - acc: 0.012 - ETA: 5:34 - loss: 5.4268 - acc: 0.012 - ETA: 5:31 - loss: 5.4238 - acc: 0.012 - ETA: 5:29 - loss: 5.4210 - acc: 0.013 - ETA: 5:26 - loss: 5.4187 - acc: 0.012 - ETA: 5:23 - loss: 5.4163 - acc: 0.012 - ETA: 5:20 - loss: 5.4145 - acc: 0.012 - ETA: 5:18 - loss: 5.4121 - acc: 0.012 - ETA: 5:15 - loss: 5.4098 - acc: 0.012 - ETA: 5:12 - loss: 5.4076 - acc: 0.012 - ETA: 5:10 - loss: 5.4049 - acc: 0.012 - ETA: 5:07 - loss: 5.4026 - acc: 0.012 - ETA: 5:04 - loss: 5.4004 - acc: 0.012 - ETA: 5:01 - loss: 5.3980 - acc: 0.012 - ETA: 4:59 - loss: 5.3957 - acc: 0.012 - ETA: 4:56 - loss: 5.3930 - acc: 0.012 - ETA: 4:53 - loss: 5.3905 - acc: 0.012 - ETA: 4:51 - loss: 5.3885 - acc: 0.012 - ETA: 4:48 - loss: 5.3860 - acc: 0.012 - ETA: 4:45 - loss: 5.3837 - acc: 0.012 - ETA: 4:42 - loss: 5.3814 - acc: 0.012 - ETA: 4:40 - loss: 5.3786 - acc: 0.012 - ETA: 4:37 - loss: 5.3763 - acc: 0.012 - ETA: 4:34 - loss: 5.3743 - acc: 0.012 - ETA: 4:32 - loss: 5.3718 - acc: 0.012 - ETA: 4:29 - loss: 5.3696 - acc: 0.012 - ETA: 4:26 - loss: 5.3674 - acc: 0.012 - ETA: 4:23 - loss: 5.3648 - acc: 0.012 - ETA: 4:21 - loss: 5.3621 - acc: 0.012 - ETA: 4:18 - loss: 5.3602 - acc: 0.012 - ETA: 4:15 - loss: 5.3582 - acc: 0.012 - ETA: 4:12 - loss: 5.3560 - acc: 0.012 - ETA: 4:10 - loss: 5.3534 - acc: 0.012 - ETA: 4:07 - loss: 5.3510 - acc: 0.012 - ETA: 4:04 - loss: 5.3488 - acc: 0.012 - ETA: 4:01 - loss: 5.3465 - acc: 0.012 - ETA: 3:58 - loss: 5.3443 - acc: 0.012 - ETA: 3:56 - loss: 5.3426 - acc: 0.012 - ETA: 3:53 - loss: 5.3404 - acc: 0.012 - ETA: 3:50 - loss: 5.3384 - acc: 0.012 - ETA: 3:47 - loss: 5.3367 - acc: 0.012 - ETA: 3:45 - loss: 5.3343 - acc: 0.012 - ETA: 3:42 - loss: 5.3328 - acc: 0.012 - ETA: 3:39 - loss: 5.3309 - acc: 0.012 - ETA: 3:36 - loss: 5.3287 - acc: 0.012 - ETA: 3:34 - loss: 5.3267 - acc: 0.012 - ETA: 3:31 - loss: 5.3248 - acc: 0.012 - ETA: 3:28 - loss: 5.3230 - acc: 0.012 - ETA: 3:25 - loss: 5.3215 - acc: 0.012 - ETA: 3:23 - loss: 5.3196 - acc: 0.012 - ETA: 3:20 - loss: 5.3178 - acc: 0.012 - ETA: 3:17 - loss: 5.3161 - acc: 0.012 - ETA: 3:15 - loss: 5.3143 - acc: 0.012 - ETA: 3:12 - loss: 5.3116 - acc: 0.012 - ETA: 3:09 - loss: 5.3102 - acc: 0.012 - ETA: 3:06 - loss: 5.3084 - acc: 0.012 - ETA: 3:04 - loss: 5.3064 - acc: 0.012 - ETA: 3:01 - loss: 5.3050 - acc: 0.012 - ETA: 2:58 - loss: 5.3036 - acc: 0.012 - ETA: 2:55 - loss: 5.3019 - acc: 0.012 - ETA: 2:53 - loss: 5.3000 - acc: 0.012 - ETA: 2:50 - loss: 5.2979 - acc: 0.013 - ETA: 2:47 - loss: 5.2968 - acc: 0.012 - ETA: 2:45 - loss: 5.2952 - acc: 0.012 - ETA: 2:42 - loss: 5.2933 - acc: 0.013 - ETA: 2:39 - loss: 5.2914 - acc: 0.013 - ETA: 2:36 - loss: 5.2901 - acc: 0.012 - ETA: 2:34 - loss: 5.2886 - acc: 0.012 - ETA: 2:31 - loss: 5.2867 - acc: 0.012 - ETA: 2:28 - loss: 5.2854 - acc: 0.012 - ETA: 2:26 - loss: 5.2838 - acc: 0.012 - ETA: 2:23 - loss: 5.2817 - acc: 0.012 - ETA: 2:20 - loss: 5.2799 - acc: 0.012 - ETA: 2:18 - loss: 5.2787 - acc: 0.012 - ETA: 2:15 - loss: 5.2770 - acc: 0.012 - ETA: 2:12 - loss: 5.2758 - acc: 0.012 - ETA: 2:09 - loss: 5.2738 - acc: 0.012 - ETA: 2:07 - loss: 5.2728 - acc: 0.012 - ETA: 2:04 - loss: 5.2713 - acc: 0.012 - ETA: 2:01 - loss: 5.2699 - acc: 0.012 - ETA: 1:59 - loss: 5.2684 - acc: 0.012 - ETA: 1:56 - loss: 5.2668 - acc: 0.012 - ETA: 1:53 - loss: 5.2652 - acc: 0.013 - ETA: 1:51 - loss: 5.2641 - acc: 0.013 - ETA: 1:48 - loss: 5.2625 - acc: 0.013 - ETA: 1:46 - loss: 5.2611 - acc: 0.012 - ETA: 1:43 - loss: 5.2593 - acc: 0.012 - ETA: 1:40 - loss: 5.2571 - acc: 0.012 - ETA: 1:37 - loss: 5.2560 - acc: 0.012 - ETA: 1:35 - loss: 5.2538 - acc: 0.012 - ETA: 1:32 - loss: 5.2525 - acc: 0.012 - ETA: 1:29 - loss: 5.2509 - acc: 0.012 - ETA: 1:27 - loss: 5.2496 - acc: 0.012 - ETA: 1:24 - loss: 5.2477 - acc: 0.012 - ETA: 1:21 - loss: 5.2485 - acc: 0.012 - ETA: 1:19 - loss: 5.2468 - acc: 0.012 - ETA: 1:16 - loss: 5.2451 - acc: 0.013 - ETA: 1:14 - loss: 5.2431 - acc: 0.013 - ETA: 1:11 - loss: 5.2419 - acc: 0.013 - ETA: 1:08 - loss: 5.2420 - acc: 0.013 - ETA: 1:06 - loss: 5.2401 - acc: 0.013 - ETA: 1:03 - loss: 5.2382 - acc: 0.013 - ETA: 1:00 - loss: 5.2367 - acc: 0.013 - ETA: 58s - loss: 5.2351 - acc: 0.013 - ETA: 55s - loss: 5.2339 - acc: 0.01 - ETA: 52s - loss: 5.2317 - acc: 0.01 - ETA: 50s - loss: 5.2304 - acc: 0.01 - ETA: 47s - loss: 5.2288 - acc: 0.01 - ETA: 44s - loss: 5.2279 - acc: 0.01 - ETA: 42s - loss: 5.2265 - acc: 0.01 - ETA: 39s - loss: 5.2252 - acc: 0.01 - ETA: 36s - loss: 5.2240 - acc: 0.01 - ETA: 34s - loss: 5.2224 - acc: 0.01 - ETA: 31s - loss: 5.2215 - acc: 0.01 - ETA: 29s - loss: 5.2200 - acc: 0.01 - ETA: 26s - loss: 5.2189 - acc: 0.01 - ETA: 23s - loss: 5.2177 - acc: 0.01 - ETA: 21s - loss: 5.2165 - acc: 0.01 - ETA: 18s - loss: 5.2152 - acc: 0.01 - ETA: 15s - loss: 5.2141 - acc: 0.01 - ETA: 13s - loss: 5.2127 - acc: 0.01 - ETA: 10s - loss: 5.2108 - acc: 0.01 - ETA: 7s - loss: 5.2102 - acc: 0.0144 - ETA: 5s - loss: 5.2088 - acc: 0.014 - ETA: 2s - loss: 5.2073 - acc: 0.014 - 906s 136ms/step - loss: 5.2051 - acc: 0.0142 - val_loss: 4.8512 - val_acc: 0.0263\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.85117, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 14:18 - loss: 4.6119 - acc: 0.05 - ETA: 13:50 - loss: 4.5997 - acc: 0.02 - ETA: 13:37 - loss: 4.6034 - acc: 0.03 - ETA: 13:56 - loss: 4.5504 - acc: 0.05 - ETA: 13:48 - loss: 4.6180 - acc: 0.04 - ETA: 13:41 - loss: 4.5955 - acc: 0.04 - ETA: 13:37 - loss: 4.5759 - acc: 0.05 - ETA: 13:34 - loss: 4.5617 - acc: 0.04 - ETA: 13:30 - loss: 4.5482 - acc: 0.03 - ETA: 13:27 - loss: 4.5655 - acc: 0.04 - ETA: 13:49 - loss: 4.5671 - acc: 0.05 - ETA: 13:43 - loss: 4.5688 - acc: 0.04 - ETA: 13:42 - loss: 4.5636 - acc: 0.05 - ETA: 13:37 - loss: 4.5882 - acc: 0.04 - ETA: 13:32 - loss: 4.6179 - acc: 0.04 - ETA: 13:28 - loss: 4.6110 - acc: 0.04 - ETA: 13:23 - loss: 4.6062 - acc: 0.04 - ETA: 13:19 - loss: 4.6175 - acc: 0.04 - ETA: 13:15 - loss: 4.6107 - acc: 0.04 - ETA: 13:11 - loss: 4.6141 - acc: 0.04 - ETA: 13:08 - loss: 4.6122 - acc: 0.04 - ETA: 13:04 - loss: 4.6168 - acc: 0.04 - ETA: 13:00 - loss: 4.6183 - acc: 0.04 - ETA: 12:57 - loss: 4.6158 - acc: 0.04 - ETA: 12:53 - loss: 4.6110 - acc: 0.04 - ETA: 12:50 - loss: 4.6070 - acc: 0.04 - ETA: 12:46 - loss: 4.6161 - acc: 0.04 - ETA: 12:43 - loss: 4.6086 - acc: 0.04 - ETA: 12:40 - loss: 4.6050 - acc: 0.04 - ETA: 12:37 - loss: 4.6150 - acc: 0.04 - ETA: 12:34 - loss: 4.6154 - acc: 0.04 - ETA: 12:32 - loss: 4.6140 - acc: 0.04 - ETA: 12:29 - loss: 4.6136 - acc: 0.04 - ETA: 12:26 - loss: 4.6039 - acc: 0.04 - ETA: 12:23 - loss: 4.6046 - acc: 0.04 - ETA: 12:20 - loss: 4.6029 - acc: 0.04 - ETA: 12:17 - loss: 4.6183 - acc: 0.03 - ETA: 12:15 - loss: 4.6216 - acc: 0.03 - ETA: 12:12 - loss: 4.6206 - acc: 0.03 - ETA: 12:10 - loss: 4.6157 - acc: 0.04 - ETA: 12:07 - loss: 4.6178 - acc: 0.03 - ETA: 12:05 - loss: 4.6100 - acc: 0.04 - ETA: 12:02 - loss: 4.6000 - acc: 0.04 - ETA: 12:01 - loss: 4.6209 - acc: 0.04 - ETA: 11:58 - loss: 4.6265 - acc: 0.04 - ETA: 11:55 - loss: 4.6188 - acc: 0.04 - ETA: 11:53 - loss: 4.6219 - acc: 0.04 - ETA: 11:50 - loss: 4.6182 - acc: 0.04 - ETA: 11:47 - loss: 4.6160 - acc: 0.04 - ETA: 11:45 - loss: 4.6170 - acc: 0.04 - ETA: 11:42 - loss: 4.6139 - acc: 0.04 - ETA: 11:40 - loss: 4.6114 - acc: 0.04 - ETA: 11:37 - loss: 4.6058 - acc: 0.04 - ETA: 11:34 - loss: 4.6075 - acc: 0.04 - ETA: 11:31 - loss: 4.6104 - acc: 0.04 - ETA: 11:29 - loss: 4.6092 - acc: 0.04 - ETA: 11:26 - loss: 4.6061 - acc: 0.04 - ETA: 11:24 - loss: 4.6047 - acc: 0.04 - ETA: 11:21 - loss: 4.6001 - acc: 0.04 - ETA: 11:18 - loss: 4.5996 - acc: 0.04 - ETA: 11:15 - loss: 4.6006 - acc: 0.04 - ETA: 11:13 - loss: 4.6033 - acc: 0.04 - ETA: 11:10 - loss: 4.6045 - acc: 0.04 - ETA: 11:08 - loss: 4.5996 - acc: 0.04 - ETA: 11:05 - loss: 4.6039 - acc: 0.04 - ETA: 11:03 - loss: 4.6022 - acc: 0.04 - ETA: 11:00 - loss: 4.6052 - acc: 0.04 - ETA: 10:58 - loss: 4.5960 - acc: 0.04 - ETA: 10:55 - loss: 4.5992 - acc: 0.04 - ETA: 10:52 - loss: 4.6005 - acc: 0.04 - ETA: 10:50 - loss: 4.6015 - acc: 0.04 - ETA: 10:47 - loss: 4.5995 - acc: 0.04 - ETA: 10:45 - loss: 4.5954 - acc: 0.04 - ETA: 10:42 - loss: 4.5932 - acc: 0.04 - ETA: 10:40 - loss: 4.5926 - acc: 0.04 - ETA: 10:37 - loss: 4.5922 - acc: 0.04 - ETA: 10:34 - loss: 4.5905 - acc: 0.04 - ETA: 10:32 - loss: 4.5874 - acc: 0.04 - ETA: 10:29 - loss: 4.5935 - acc: 0.04 - ETA: 10:27 - loss: 4.5905 - acc: 0.04 - ETA: 10:24 - loss: 4.5847 - acc: 0.04 - ETA: 10:22 - loss: 4.5842 - acc: 0.04 - ETA: 10:19 - loss: 4.5807 - acc: 0.05 - ETA: 10:17 - loss: 4.5767 - acc: 0.05 - ETA: 10:14 - loss: 4.5711 - acc: 0.05 - ETA: 10:12 - loss: 4.5693 - acc: 0.05 - ETA: 10:09 - loss: 4.5709 - acc: 0.05 - ETA: 10:08 - loss: 4.5705 - acc: 0.05 - ETA: 10:05 - loss: 4.5697 - acc: 0.05 - ETA: 10:03 - loss: 4.5693 - acc: 0.05 - ETA: 10:00 - loss: 4.5687 - acc: 0.05 - ETA: 9:58 - loss: 4.5664 - acc: 0.0511 - ETA: 9:55 - loss: 4.5679 - acc: 0.050 - ETA: 9:52 - loss: 4.5668 - acc: 0.050 - ETA: 9:50 - loss: 4.5646 - acc: 0.050 - ETA: 9:47 - loss: 4.5608 - acc: 0.051 - ETA: 9:45 - loss: 4.5586 - acc: 0.051 - ETA: 9:42 - loss: 4.5553 - acc: 0.054 - ETA: 9:40 - loss: 4.5569 - acc: 0.054 - ETA: 9:38 - loss: 4.5559 - acc: 0.054 - ETA: 9:35 - loss: 4.5553 - acc: 0.054 - ETA: 9:32 - loss: 4.5514 - acc: 0.054 - ETA: 9:30 - loss: 4.5504 - acc: 0.054 - ETA: 9:27 - loss: 4.5483 - acc: 0.054 - ETA: 9:25 - loss: 4.5480 - acc: 0.054 - ETA: 9:22 - loss: 4.5482 - acc: 0.054 - ETA: 9:20 - loss: 4.5477 - acc: 0.054 - ETA: 9:18 - loss: 4.5492 - acc: 0.054 - ETA: 9:16 - loss: 4.5471 - acc: 0.054 - ETA: 9:13 - loss: 4.5380 - acc: 0.056 - ETA: 9:11 - loss: 4.5319 - acc: 0.058 - ETA: 9:08 - loss: 4.5379 - acc: 0.058 - ETA: 9:06 - loss: 4.5376 - acc: 0.058 - ETA: 9:03 - loss: 4.5393 - acc: 0.058 - ETA: 9:00 - loss: 4.5374 - acc: 0.058 - ETA: 8:58 - loss: 4.5381 - acc: 0.059 - ETA: 8:55 - loss: 4.5348 - acc: 0.060 - ETA: 8:53 - loss: 4.5347 - acc: 0.060 - ETA: 8:50 - loss: 4.5334 - acc: 0.060 - ETA: 8:48 - loss: 4.5322 - acc: 0.060 - ETA: 8:45 - loss: 4.5316 - acc: 0.060 - ETA: 8:43 - loss: 4.5302 - acc: 0.061 - ETA: 8:40 - loss: 4.5267 - acc: 0.062 - ETA: 8:38 - loss: 4.5251 - acc: 0.062 - ETA: 8:35 - loss: 4.5252 - acc: 0.062 - ETA: 8:33 - loss: 4.5259 - acc: 0.062 - ETA: 8:30 - loss: 4.5253 - acc: 0.063 - ETA: 8:28 - loss: 4.5252 - acc: 0.062 - ETA: 8:25 - loss: 4.5258 - acc: 0.062 - ETA: 8:23 - loss: 4.5265 - acc: 0.062 - ETA: 8:20 - loss: 4.5261 - acc: 0.063 - ETA: 8:18 - loss: 4.5250 - acc: 0.063 - ETA: 8:15 - loss: 4.5241 - acc: 0.062 - ETA: 8:13 - loss: 4.5233 - acc: 0.062 - ETA: 8:10 - loss: 4.5205 - acc: 0.063 - ETA: 8:08 - loss: 4.5222 - acc: 0.063 - ETA: 8:05 - loss: 4.5218 - acc: 0.063 - ETA: 8:03 - loss: 4.5208 - acc: 0.063 - ETA: 8:00 - loss: 4.5161 - acc: 0.062 - ETA: 7:58 - loss: 4.5224 - acc: 0.062 - ETA: 7:55 - loss: 4.5243 - acc: 0.062 - ETA: 7:53 - loss: 4.5229 - acc: 0.062 - ETA: 7:50 - loss: 4.5213 - acc: 0.062 - ETA: 7:48 - loss: 4.5196 - acc: 0.062 - ETA: 7:45 - loss: 4.5159 - acc: 0.063 - ETA: 7:43 - loss: 4.5155 - acc: 0.063 - ETA: 7:40 - loss: 4.5207 - acc: 0.062 - ETA: 7:38 - loss: 4.5201 - acc: 0.062 - ETA: 7:35 - loss: 4.5166 - acc: 0.063 - ETA: 7:33 - loss: 4.5154 - acc: 0.063 - ETA: 7:30 - loss: 4.5240 - acc: 0.062 - ETA: 7:28 - loss: 4.5243 - acc: 0.062 - ETA: 7:25 - loss: 4.5243 - acc: 0.062 - ETA: 7:23 - loss: 4.5232 - acc: 0.063 - ETA: 7:20 - loss: 4.5227 - acc: 0.063 - ETA: 7:18 - loss: 4.5225 - acc: 0.063 - ETA: 7:15 - loss: 4.5205 - acc: 0.063 - ETA: 7:13 - loss: 4.5196 - acc: 0.063 - ETA: 7:10 - loss: 4.5170 - acc: 0.063 - ETA: 7:08 - loss: 4.5192 - acc: 0.062 - ETA: 7:05 - loss: 4.5191 - acc: 0.063 - ETA: 7:03 - loss: 4.5193 - acc: 0.063 - ETA: 7:00 - loss: 4.5204 - acc: 0.062 - ETA: 6:58 - loss: 4.5190 - acc: 0.062 - ETA: 6:56 - loss: 4.5174 - acc: 0.063 - ETA: 6:53 - loss: 4.5188 - acc: 0.063 - ETA: 6:51 - loss: 4.5167 - acc: 0.064 - ETA: 6:48 - loss: 4.5145 - acc: 0.063 - ETA: 6:46 - loss: 4.5131 - acc: 0.063 - ETA: 6:43 - loss: 4.5125 - acc: 0.063 - ETA: 6:41 - loss: 4.5114 - acc: 0.063 - ETA: 6:38 - loss: 4.5122 - acc: 0.063 - ETA: 6:36 - loss: 4.5121 - acc: 0.063 - ETA: 6:33 - loss: 4.5123 - acc: 0.062 - ETA: 6:31 - loss: 4.5082 - acc: 0.064 - ETA: 6:28 - loss: 4.5090 - acc: 0.063 - ETA: 6:26 - loss: 4.5102 - acc: 0.063 - ETA: 6:23 - loss: 4.5086 - acc: 0.064 - ETA: 6:21 - loss: 4.5100 - acc: 0.064 - ETA: 6:18 - loss: 4.5083 - acc: 0.064 - ETA: 6:16 - loss: 4.5066 - acc: 0.064 - ETA: 6:13 - loss: 4.5069 - acc: 0.063 - ETA: 6:11 - loss: 4.5048 - acc: 0.064 - ETA: 6:09 - loss: 4.5059 - acc: 0.064 - ETA: 6:06 - loss: 4.5045 - acc: 0.064 - ETA: 6:04 - loss: 4.5019 - acc: 0.064 - ETA: 6:01 - loss: 4.5012 - acc: 0.064 - ETA: 5:59 - loss: 4.5000 - acc: 0.064 - ETA: 5:57 - loss: 4.4994 - acc: 0.064 - ETA: 5:54 - loss: 4.4996 - acc: 0.063 - ETA: 5:52 - loss: 4.4972 - acc: 0.064 - ETA: 5:49 - loss: 4.4989 - acc: 0.064 - ETA: 5:47 - loss: 4.4984 - acc: 0.064 - ETA: 5:44 - loss: 4.4968 - acc: 0.065 - ETA: 5:42 - loss: 4.4938 - acc: 0.065 - ETA: 5:39 - loss: 4.4957 - acc: 0.065 - ETA: 5:37 - loss: 4.4977 - acc: 0.065 - ETA: 5:34 - loss: 4.4983 - acc: 0.065 - ETA: 5:32 - loss: 4.4972 - acc: 0.065 - ETA: 5:29 - loss: 4.4977 - acc: 0.065 - ETA: 5:27 - loss: 4.4953 - acc: 0.065 - ETA: 5:24 - loss: 4.4958 - acc: 0.065 - ETA: 5:22 - loss: 4.4931 - acc: 0.065 - ETA: 5:19 - loss: 4.4924 - acc: 0.0657"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5:17 - loss: 4.4910 - acc: 0.066 - ETA: 5:14 - loss: 4.4903 - acc: 0.066 - ETA: 5:12 - loss: 4.4916 - acc: 0.065 - ETA: 5:10 - loss: 4.4891 - acc: 0.066 - ETA: 5:07 - loss: 4.4867 - acc: 0.066 - ETA: 5:05 - loss: 4.4854 - acc: 0.065 - ETA: 5:02 - loss: 4.4850 - acc: 0.065 - ETA: 5:00 - loss: 4.4846 - acc: 0.065 - ETA: 4:57 - loss: 4.4835 - acc: 0.065 - ETA: 4:55 - loss: 4.4842 - acc: 0.065 - ETA: 4:52 - loss: 4.4826 - acc: 0.065 - ETA: 4:50 - loss: 4.4829 - acc: 0.065 - ETA: 4:47 - loss: 4.4826 - acc: 0.066 - ETA: 4:45 - loss: 4.4829 - acc: 0.066 - ETA: 4:42 - loss: 4.4816 - acc: 0.066 - ETA: 4:40 - loss: 4.4818 - acc: 0.066 - ETA: 4:38 - loss: 4.4802 - acc: 0.066 - ETA: 4:35 - loss: 4.4795 - acc: 0.066 - ETA: 4:33 - loss: 4.4768 - acc: 0.067 - ETA: 4:30 - loss: 4.4748 - acc: 0.067 - ETA: 4:28 - loss: 4.4753 - acc: 0.067 - ETA: 4:25 - loss: 4.4744 - acc: 0.068 - ETA: 4:23 - loss: 4.4753 - acc: 0.068 - ETA: 4:21 - loss: 4.4751 - acc: 0.068 - ETA: 4:18 - loss: 4.4741 - acc: 0.069 - ETA: 4:16 - loss: 4.4729 - acc: 0.069 - ETA: 4:13 - loss: 4.4718 - acc: 0.069 - ETA: 4:11 - loss: 4.4714 - acc: 0.069 - ETA: 4:08 - loss: 4.4727 - acc: 0.069 - ETA: 4:06 - loss: 4.4716 - acc: 0.069 - ETA: 4:03 - loss: 4.4727 - acc: 0.068 - ETA: 4:01 - loss: 4.4711 - acc: 0.069 - ETA: 3:58 - loss: 4.4691 - acc: 0.069 - ETA: 3:56 - loss: 4.4668 - acc: 0.070 - ETA: 3:53 - loss: 4.4671 - acc: 0.070 - ETA: 3:51 - loss: 4.4645 - acc: 0.070 - ETA: 3:49 - loss: 4.4642 - acc: 0.071 - ETA: 3:46 - loss: 4.4622 - acc: 0.071 - ETA: 3:44 - loss: 4.4594 - acc: 0.071 - ETA: 3:41 - loss: 4.4583 - acc: 0.071 - ETA: 3:39 - loss: 4.4557 - acc: 0.072 - ETA: 3:36 - loss: 4.4550 - acc: 0.072 - ETA: 3:34 - loss: 4.4537 - acc: 0.072 - ETA: 3:31 - loss: 4.4529 - acc: 0.072 - ETA: 3:29 - loss: 4.4524 - acc: 0.072 - ETA: 3:26 - loss: 4.4512 - acc: 0.072 - ETA: 3:24 - loss: 4.4510 - acc: 0.072 - ETA: 3:21 - loss: 4.4497 - acc: 0.072 - ETA: 3:19 - loss: 4.4494 - acc: 0.072 - ETA: 3:16 - loss: 4.4478 - acc: 0.073 - ETA: 3:14 - loss: 4.4481 - acc: 0.072 - ETA: 3:12 - loss: 4.4480 - acc: 0.072 - ETA: 3:09 - loss: 4.4472 - acc: 0.072 - ETA: 3:07 - loss: 4.4465 - acc: 0.072 - ETA: 3:04 - loss: 4.4461 - acc: 0.072 - ETA: 3:02 - loss: 4.4455 - acc: 0.072 - ETA: 2:59 - loss: 4.4450 - acc: 0.072 - ETA: 2:57 - loss: 4.4438 - acc: 0.072 - ETA: 2:54 - loss: 4.4431 - acc: 0.072 - ETA: 2:52 - loss: 4.4427 - acc: 0.072 - ETA: 2:49 - loss: 4.4426 - acc: 0.072 - ETA: 2:47 - loss: 4.4422 - acc: 0.072 - ETA: 2:45 - loss: 4.4425 - acc: 0.072 - ETA: 2:42 - loss: 4.4408 - acc: 0.072 - ETA: 2:40 - loss: 4.4409 - acc: 0.072 - ETA: 2:37 - loss: 4.4397 - acc: 0.073 - ETA: 2:35 - loss: 4.4382 - acc: 0.073 - ETA: 2:32 - loss: 4.4380 - acc: 0.073 - ETA: 2:30 - loss: 4.4394 - acc: 0.073 - ETA: 2:27 - loss: 4.4395 - acc: 0.073 - ETA: 2:25 - loss: 4.4378 - acc: 0.073 - ETA: 2:22 - loss: 4.4381 - acc: 0.073 - ETA: 2:20 - loss: 4.4398 - acc: 0.073 - ETA: 2:17 - loss: 4.4389 - acc: 0.072 - ETA: 2:15 - loss: 4.4382 - acc: 0.072 - ETA: 2:12 - loss: 4.4360 - acc: 0.073 - ETA: 2:10 - loss: 4.4358 - acc: 0.073 - ETA: 2:08 - loss: 4.4350 - acc: 0.073 - ETA: 2:05 - loss: 4.4344 - acc: 0.073 - ETA: 2:03 - loss: 4.4329 - acc: 0.073 - ETA: 2:00 - loss: 4.4323 - acc: 0.073 - ETA: 1:58 - loss: 4.4318 - acc: 0.073 - ETA: 1:55 - loss: 4.4305 - acc: 0.073 - ETA: 1:53 - loss: 4.4307 - acc: 0.073 - ETA: 1:50 - loss: 4.4286 - acc: 0.074 - ETA: 1:48 - loss: 4.4264 - acc: 0.074 - ETA: 1:45 - loss: 4.4259 - acc: 0.075 - ETA: 1:43 - loss: 4.4247 - acc: 0.075 - ETA: 1:40 - loss: 4.4240 - acc: 0.075 - ETA: 1:38 - loss: 4.4222 - acc: 0.075 - ETA: 1:36 - loss: 4.4216 - acc: 0.075 - ETA: 1:33 - loss: 4.4194 - acc: 0.075 - ETA: 1:31 - loss: 4.4186 - acc: 0.075 - ETA: 1:28 - loss: 4.4175 - acc: 0.075 - ETA: 1:26 - loss: 4.4182 - acc: 0.075 - ETA: 1:23 - loss: 4.4172 - acc: 0.075 - ETA: 1:21 - loss: 4.4172 - acc: 0.075 - ETA: 1:18 - loss: 4.4161 - acc: 0.075 - ETA: 1:16 - loss: 4.4169 - acc: 0.075 - ETA: 1:13 - loss: 4.4158 - acc: 0.075 - ETA: 1:11 - loss: 4.4165 - acc: 0.075 - ETA: 1:08 - loss: 4.4163 - acc: 0.075 - ETA: 1:06 - loss: 4.4149 - acc: 0.075 - ETA: 1:04 - loss: 4.4141 - acc: 0.075 - ETA: 1:01 - loss: 4.4135 - acc: 0.075 - ETA: 59s - loss: 4.4130 - acc: 0.075 - ETA: 56s - loss: 4.4123 - acc: 0.07 - ETA: 54s - loss: 4.4112 - acc: 0.07 - ETA: 51s - loss: 4.4111 - acc: 0.07 - ETA: 49s - loss: 4.4108 - acc: 0.07 - ETA: 46s - loss: 4.4111 - acc: 0.07 - ETA: 44s - loss: 4.4108 - acc: 0.07 - ETA: 41s - loss: 4.4110 - acc: 0.07 - ETA: 39s - loss: 4.4097 - acc: 0.07 - ETA: 36s - loss: 4.4098 - acc: 0.07 - ETA: 34s - loss: 4.4091 - acc: 0.07 - ETA: 32s - loss: 4.4081 - acc: 0.07 - ETA: 29s - loss: 4.4071 - acc: 0.07 - ETA: 27s - loss: 4.4047 - acc: 0.07 - ETA: 24s - loss: 4.4041 - acc: 0.07 - ETA: 22s - loss: 4.4041 - acc: 0.07 - ETA: 19s - loss: 4.4028 - acc: 0.07 - ETA: 17s - loss: 4.4023 - acc: 0.07 - ETA: 14s - loss: 4.4018 - acc: 0.07 - ETA: 12s - loss: 4.4015 - acc: 0.07 - ETA: 9s - loss: 4.4008 - acc: 0.0761 - ETA: 7s - loss: 4.3999 - acc: 0.076 - ETA: 4s - loss: 4.4001 - acc: 0.076 - ETA: 2s - loss: 4.3989 - acc: 0.076 - 849s 127ms/step - loss: 4.3987 - acc: 0.0763 - val_loss: 4.6359 - val_acc: 0.0527\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.85117 to 4.63587, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "4080/6680 [=================>............] - ETA: 17:13 - loss: 3.4471 - acc: 0.30 - ETA: 15:19 - loss: 3.1183 - acc: 0.35 - ETA: 14:39 - loss: 2.9743 - acc: 0.36 - ETA: 14:20 - loss: 3.0006 - acc: 0.33 - ETA: 14:05 - loss: 3.0783 - acc: 0.33 - ETA: 13:55 - loss: 3.0052 - acc: 0.35 - ETA: 13:44 - loss: 2.9894 - acc: 0.35 - ETA: 13:38 - loss: 3.1867 - acc: 0.32 - ETA: 13:34 - loss: 3.1321 - acc: 0.33 - ETA: 13:30 - loss: 3.1251 - acc: 0.33 - ETA: 13:29 - loss: 3.0562 - acc: 0.34 - ETA: 13:23 - loss: 3.0836 - acc: 0.33 - ETA: 13:19 - loss: 3.0634 - acc: 0.33 - ETA: 13:15 - loss: 3.0279 - acc: 0.33 - ETA: 13:11 - loss: 3.0548 - acc: 0.32 - ETA: 13:08 - loss: 3.0527 - acc: 0.33 - ETA: 13:04 - loss: 3.0522 - acc: 0.32 - ETA: 13:01 - loss: 3.0428 - acc: 0.32 - ETA: 12:58 - loss: 2.9883 - acc: 0.34 - ETA: 12:55 - loss: 2.9565 - acc: 0.34 - ETA: 12:51 - loss: 2.9599 - acc: 0.34 - ETA: 12:48 - loss: 2.9483 - acc: 0.34 - ETA: 12:46 - loss: 2.9309 - acc: 0.35 - ETA: 12:43 - loss: 2.8955 - acc: 0.36 - ETA: 12:40 - loss: 2.8660 - acc: 0.36 - ETA: 12:37 - loss: 2.8334 - acc: 0.36 - ETA: 12:34 - loss: 2.8332 - acc: 0.36 - ETA: 12:32 - loss: 2.8399 - acc: 0.37 - ETA: 12:29 - loss: 2.8616 - acc: 0.36 - ETA: 12:26 - loss: 2.8556 - acc: 0.36 - ETA: 12:23 - loss: 2.8597 - acc: 0.36 - ETA: 12:21 - loss: 2.8509 - acc: 0.36 - ETA: 12:18 - loss: 2.8294 - acc: 0.36 - ETA: 12:16 - loss: 2.8152 - acc: 0.37 - ETA: 12:14 - loss: 2.8033 - acc: 0.37 - ETA: 12:12 - loss: 2.7786 - acc: 0.37 - ETA: 12:09 - loss: 2.7839 - acc: 0.37 - ETA: 12:06 - loss: 2.7753 - acc: 0.37 - ETA: 12:03 - loss: 2.7847 - acc: 0.37 - ETA: 12:01 - loss: 2.7831 - acc: 0.37 - ETA: 11:58 - loss: 2.7987 - acc: 0.37 - ETA: 11:56 - loss: 2.7972 - acc: 0.37 - ETA: 11:53 - loss: 2.7915 - acc: 0.37 - ETA: 11:50 - loss: 2.8042 - acc: 0.37 - ETA: 11:48 - loss: 2.8001 - acc: 0.37 - ETA: 11:45 - loss: 2.7874 - acc: 0.37 - ETA: 11:43 - loss: 2.7763 - acc: 0.37 - ETA: 11:40 - loss: 2.7663 - acc: 0.37 - ETA: 11:38 - loss: 2.7621 - acc: 0.37 - ETA: 11:35 - loss: 2.7536 - acc: 0.38 - ETA: 11:33 - loss: 2.7473 - acc: 0.37 - ETA: 11:30 - loss: 2.7673 - acc: 0.37 - ETA: 11:27 - loss: 2.7659 - acc: 0.37 - ETA: 11:25 - loss: 2.7680 - acc: 0.37 - ETA: 11:22 - loss: 2.7571 - acc: 0.37 - ETA: 11:20 - loss: 2.7416 - acc: 0.37 - ETA: 11:17 - loss: 2.7421 - acc: 0.37 - ETA: 11:15 - loss: 2.7417 - acc: 0.37 - ETA: 11:13 - loss: 2.7466 - acc: 0.37 - ETA: 11:10 - loss: 2.7530 - acc: 0.37 - ETA: 11:08 - loss: 2.7472 - acc: 0.37 - ETA: 11:05 - loss: 2.7437 - acc: 0.37 - ETA: 11:03 - loss: 2.7431 - acc: 0.37 - ETA: 11:00 - loss: 2.7460 - acc: 0.37 - ETA: 10:58 - loss: 2.7390 - acc: 0.37 - ETA: 10:55 - loss: 2.7256 - acc: 0.37 - ETA: 10:53 - loss: 2.7158 - acc: 0.37 - ETA: 10:50 - loss: 2.7148 - acc: 0.38 - ETA: 10:48 - loss: 2.7141 - acc: 0.37 - ETA: 10:45 - loss: 2.7062 - acc: 0.38 - ETA: 10:43 - loss: 2.7039 - acc: 0.38 - ETA: 10:40 - loss: 2.7095 - acc: 0.37 - ETA: 10:38 - loss: 2.7132 - acc: 0.37 - ETA: 10:35 - loss: 2.7130 - acc: 0.37 - ETA: 10:33 - loss: 2.7083 - acc: 0.37 - ETA: 10:31 - loss: 2.7147 - acc: 0.37 - ETA: 10:28 - loss: 2.7248 - acc: 0.37 - ETA: 10:26 - loss: 2.7134 - acc: 0.37 - ETA: 10:23 - loss: 2.7108 - acc: 0.37 - ETA: 10:21 - loss: 2.7077 - acc: 0.37 - ETA: 10:18 - loss: 2.7114 - acc: 0.37 - ETA: 10:16 - loss: 2.7135 - acc: 0.37 - ETA: 10:14 - loss: 2.7103 - acc: 0.37 - ETA: 10:11 - loss: 2.7048 - acc: 0.38 - ETA: 10:08 - loss: 2.6987 - acc: 0.38 - ETA: 10:06 - loss: 2.6917 - acc: 0.38 - ETA: 10:04 - loss: 2.6960 - acc: 0.38 - ETA: 10:01 - loss: 2.6873 - acc: 0.38 - ETA: 9:59 - loss: 2.6772 - acc: 0.3860 - ETA: 9:56 - loss: 2.6769 - acc: 0.386 - ETA: 9:54 - loss: 2.6857 - acc: 0.385 - ETA: 9:51 - loss: 2.6801 - acc: 0.388 - ETA: 9:49 - loss: 2.6804 - acc: 0.387 - ETA: 9:46 - loss: 2.6811 - acc: 0.386 - ETA: 9:44 - loss: 2.6950 - acc: 0.384 - ETA: 9:41 - loss: 2.6946 - acc: 0.383 - ETA: 9:39 - loss: 2.6998 - acc: 0.383 - ETA: 9:36 - loss: 2.6962 - acc: 0.385 - ETA: 9:34 - loss: 2.6968 - acc: 0.384 - ETA: 9:31 - loss: 2.6975 - acc: 0.384 - ETA: 9:29 - loss: 2.6992 - acc: 0.383 - ETA: 9:27 - loss: 2.6982 - acc: 0.383 - ETA: 9:24 - loss: 2.6933 - acc: 0.384 - ETA: 9:22 - loss: 2.6884 - acc: 0.384 - ETA: 9:19 - loss: 2.6861 - acc: 0.385 - ETA: 9:17 - loss: 2.6744 - acc: 0.388 - ETA: 9:14 - loss: 2.6716 - acc: 0.388 - ETA: 9:12 - loss: 2.6648 - acc: 0.389 - ETA: 9:09 - loss: 2.6667 - acc: 0.388 - ETA: 9:07 - loss: 2.6698 - acc: 0.388 - ETA: 9:05 - loss: 2.6662 - acc: 0.389 - ETA: 9:02 - loss: 2.6641 - acc: 0.390 - ETA: 9:00 - loss: 2.6636 - acc: 0.389 - ETA: 8:57 - loss: 2.6614 - acc: 0.391 - ETA: 8:55 - loss: 2.6609 - acc: 0.391 - ETA: 8:52 - loss: 2.6589 - acc: 0.391 - ETA: 8:50 - loss: 2.6561 - acc: 0.392 - ETA: 8:47 - loss: 2.6494 - acc: 0.394 - ETA: 8:45 - loss: 2.6464 - acc: 0.395 - ETA: 8:43 - loss: 2.6515 - acc: 0.394 - ETA: 8:40 - loss: 2.6426 - acc: 0.396 - ETA: 8:38 - loss: 2.6395 - acc: 0.395 - ETA: 8:35 - loss: 2.6474 - acc: 0.395 - ETA: 8:33 - loss: 2.6434 - acc: 0.396 - ETA: 8:30 - loss: 2.6477 - acc: 0.396 - ETA: 8:28 - loss: 2.6457 - acc: 0.395 - ETA: 8:25 - loss: 2.6470 - acc: 0.395 - ETA: 8:23 - loss: 2.6477 - acc: 0.394 - ETA: 8:20 - loss: 2.6505 - acc: 0.393 - ETA: 8:18 - loss: 2.6581 - acc: 0.392 - ETA: 8:15 - loss: 2.6542 - acc: 0.392 - ETA: 8:13 - loss: 2.6476 - acc: 0.394 - ETA: 8:11 - loss: 2.6408 - acc: 0.396 - ETA: 8:08 - loss: 2.6417 - acc: 0.395 - ETA: 8:06 - loss: 2.6441 - acc: 0.395 - ETA: 8:03 - loss: 2.6479 - acc: 0.396 - ETA: 8:01 - loss: 2.6462 - acc: 0.395 - ETA: 7:58 - loss: 2.6511 - acc: 0.395 - ETA: 7:56 - loss: 2.6530 - acc: 0.395 - ETA: 7:54 - loss: 2.6550 - acc: 0.395 - ETA: 7:51 - loss: 2.6501 - acc: 0.395 - ETA: 7:49 - loss: 2.6485 - acc: 0.395 - ETA: 7:46 - loss: 2.6494 - acc: 0.395 - ETA: 7:44 - loss: 2.6459 - acc: 0.396 - ETA: 7:41 - loss: 2.6440 - acc: 0.397 - ETA: 7:39 - loss: 2.6450 - acc: 0.396 - ETA: 7:36 - loss: 2.6407 - acc: 0.398 - ETA: 7:34 - loss: 2.6457 - acc: 0.396 - ETA: 7:31 - loss: 2.6433 - acc: 0.397 - ETA: 7:29 - loss: 2.6442 - acc: 0.396 - ETA: 7:27 - loss: 2.6464 - acc: 0.395 - ETA: 7:24 - loss: 2.6484 - acc: 0.395 - ETA: 7:22 - loss: 2.6471 - acc: 0.396 - ETA: 7:19 - loss: 2.6524 - acc: 0.395 - ETA: 7:17 - loss: 2.6537 - acc: 0.394 - ETA: 7:14 - loss: 2.6509 - acc: 0.394 - ETA: 7:12 - loss: 2.6464 - acc: 0.396 - ETA: 7:10 - loss: 2.6407 - acc: 0.397 - ETA: 7:07 - loss: 2.6395 - acc: 0.397 - ETA: 7:05 - loss: 2.6407 - acc: 0.397 - ETA: 7:02 - loss: 2.6393 - acc: 0.397 - ETA: 7:00 - loss: 2.6403 - acc: 0.396 - ETA: 6:57 - loss: 2.6409 - acc: 0.396 - ETA: 6:55 - loss: 2.6431 - acc: 0.396 - ETA: 6:52 - loss: 2.6384 - acc: 0.397 - ETA: 6:50 - loss: 2.6350 - acc: 0.397 - ETA: 6:48 - loss: 2.6439 - acc: 0.396 - ETA: 6:45 - loss: 2.6471 - acc: 0.396 - ETA: 6:43 - loss: 2.6502 - acc: 0.395 - ETA: 6:40 - loss: 2.6478 - acc: 0.396 - ETA: 6:38 - loss: 2.6525 - acc: 0.395 - ETA: 6:35 - loss: 2.6504 - acc: 0.397 - ETA: 6:33 - loss: 2.6516 - acc: 0.397 - ETA: 6:30 - loss: 2.6495 - acc: 0.397 - ETA: 6:28 - loss: 2.6484 - acc: 0.397 - ETA: 6:25 - loss: 2.6491 - acc: 0.397 - ETA: 6:23 - loss: 2.6474 - acc: 0.398 - ETA: 6:21 - loss: 2.6466 - acc: 0.398 - ETA: 6:18 - loss: 2.6491 - acc: 0.398 - ETA: 6:16 - loss: 2.6481 - acc: 0.398 - ETA: 6:13 - loss: 2.6480 - acc: 0.398 - ETA: 6:11 - loss: 2.6455 - acc: 0.399 - ETA: 6:08 - loss: 2.6430 - acc: 0.398 - ETA: 6:06 - loss: 2.6438 - acc: 0.398 - ETA: 6:03 - loss: 2.6504 - acc: 0.396 - ETA: 6:01 - loss: 2.6507 - acc: 0.396 - ETA: 5:59 - loss: 2.6491 - acc: 0.396 - ETA: 5:56 - loss: 2.6517 - acc: 0.396 - ETA: 5:54 - loss: 2.6506 - acc: 0.395 - ETA: 5:51 - loss: 2.6498 - acc: 0.396 - ETA: 5:49 - loss: 2.6509 - acc: 0.396 - ETA: 5:46 - loss: 2.6505 - acc: 0.396 - ETA: 5:44 - loss: 2.6522 - acc: 0.395 - ETA: 5:41 - loss: 2.6527 - acc: 0.395 - ETA: 5:39 - loss: 2.6497 - acc: 0.396 - ETA: 5:37 - loss: 2.6483 - acc: 0.396 - ETA: 5:34 - loss: 2.6483 - acc: 0.397 - ETA: 5:32 - loss: 2.6451 - acc: 0.397 - ETA: 5:29 - loss: 2.6432 - acc: 0.398 - ETA: 5:27 - loss: 2.6443 - acc: 0.397 - ETA: 5:24 - loss: 2.6411 - acc: 0.398 - ETA: 5:22 - loss: 2.6381 - acc: 0.399 - ETA: 5:19 - loss: 2.6331 - acc: 0.400 - ETA: 5:17 - loss: 2.6317 - acc: 0.40106680/6680 [==============================] - ETA: 5:15 - loss: 2.6285 - acc: 0.402 - ETA: 5:12 - loss: 2.6295 - acc: 0.401 - ETA: 5:10 - loss: 2.6280 - acc: 0.401 - ETA: 5:07 - loss: 2.6269 - acc: 0.401 - ETA: 5:05 - loss: 2.6264 - acc: 0.401 - ETA: 5:02 - loss: 2.6257 - acc: 0.401 - ETA: 5:00 - loss: 2.6274 - acc: 0.400 - ETA: 4:57 - loss: 2.6248 - acc: 0.400 - ETA: 4:55 - loss: 2.6231 - acc: 0.401 - ETA: 4:53 - loss: 2.6233 - acc: 0.400 - ETA: 4:50 - loss: 2.6226 - acc: 0.400 - ETA: 4:48 - loss: 2.6218 - acc: 0.400 - ETA: 4:45 - loss: 2.6205 - acc: 0.400 - ETA: 4:43 - loss: 2.6224 - acc: 0.400 - ETA: 4:40 - loss: 2.6246 - acc: 0.400 - ETA: 4:38 - loss: 2.6245 - acc: 0.400 - ETA: 4:35 - loss: 2.6235 - acc: 0.399 - ETA: 4:33 - loss: 2.6219 - acc: 0.400 - ETA: 4:31 - loss: 2.6211 - acc: 0.400 - ETA: 4:28 - loss: 2.6236 - acc: 0.400 - ETA: 4:26 - loss: 2.6243 - acc: 0.400 - ETA: 4:23 - loss: 2.6252 - acc: 0.399 - ETA: 4:21 - loss: 2.6246 - acc: 0.399 - ETA: 4:18 - loss: 2.6239 - acc: 0.399 - ETA: 4:16 - loss: 2.6235 - acc: 0.399 - ETA: 4:13 - loss: 2.6236 - acc: 0.398 - ETA: 4:11 - loss: 2.6203 - acc: 0.399 - ETA: 4:09 - loss: 2.6259 - acc: 0.398 - ETA: 4:06 - loss: 2.6230 - acc: 0.399 - ETA: 4:04 - loss: 2.6233 - acc: 0.399 - ETA: 4:01 - loss: 2.6231 - acc: 0.399 - ETA: 3:59 - loss: 2.6223 - acc: 0.399 - ETA: 3:56 - loss: 2.6198 - acc: 0.399 - ETA: 3:54 - loss: 2.6215 - acc: 0.399 - ETA: 3:51 - loss: 2.6223 - acc: 0.399 - ETA: 3:49 - loss: 2.6254 - acc: 0.398 - ETA: 3:47 - loss: 2.6266 - acc: 0.398 - ETA: 3:44 - loss: 2.6269 - acc: 0.397 - ETA: 3:42 - loss: 2.6272 - acc: 0.397 - ETA: 3:39 - loss: 2.6221 - acc: 0.398 - ETA: 3:37 - loss: 2.6210 - acc: 0.399 - ETA: 3:34 - loss: 2.6237 - acc: 0.397 - ETA: 3:32 - loss: 2.6195 - acc: 0.398 - ETA: 3:29 - loss: 2.6191 - acc: 0.399 - ETA: 3:27 - loss: 2.6199 - acc: 0.398 - ETA: 3:25 - loss: 2.6214 - acc: 0.398 - ETA: 3:22 - loss: 2.6204 - acc: 0.398 - ETA: 3:20 - loss: 2.6191 - acc: 0.398 - ETA: 3:17 - loss: 2.6164 - acc: 0.399 - ETA: 3:15 - loss: 2.6181 - acc: 0.398 - ETA: 3:12 - loss: 2.6158 - acc: 0.399 - ETA: 3:10 - loss: 2.6150 - acc: 0.400 - ETA: 3:08 - loss: 2.6126 - acc: 0.400 - ETA: 3:05 - loss: 2.6136 - acc: 0.399 - ETA: 3:03 - loss: 2.6136 - acc: 0.399 - ETA: 3:00 - loss: 2.6139 - acc: 0.399 - ETA: 2:58 - loss: 2.6101 - acc: 0.401 - ETA: 2:55 - loss: 2.6112 - acc: 0.400 - ETA: 2:53 - loss: 2.6085 - acc: 0.401 - ETA: 2:50 - loss: 2.6071 - acc: 0.400 - ETA: 2:48 - loss: 2.6089 - acc: 0.400 - ETA: 2:46 - loss: 2.6097 - acc: 0.400 - ETA: 2:43 - loss: 2.6112 - acc: 0.400 - ETA: 2:41 - loss: 2.6139 - acc: 0.399 - ETA: 2:38 - loss: 2.6139 - acc: 0.400 - ETA: 2:36 - loss: 2.6123 - acc: 0.400 - ETA: 2:33 - loss: 2.6114 - acc: 0.400 - ETA: 2:31 - loss: 2.6134 - acc: 0.399 - ETA: 2:28 - loss: 2.6112 - acc: 0.399 - ETA: 2:26 - loss: 2.6116 - acc: 0.399 - ETA: 2:24 - loss: 2.6106 - acc: 0.399 - ETA: 2:21 - loss: 2.6101 - acc: 0.399 - ETA: 2:19 - loss: 2.6108 - acc: 0.399 - ETA: 2:16 - loss: 2.6167 - acc: 0.398 - ETA: 2:14 - loss: 2.6181 - acc: 0.398 - ETA: 2:11 - loss: 2.6179 - acc: 0.398 - ETA: 2:09 - loss: 2.6174 - acc: 0.398 - ETA: 2:07 - loss: 2.6166 - acc: 0.398 - ETA: 2:04 - loss: 2.6168 - acc: 0.398 - ETA: 2:02 - loss: 2.6177 - acc: 0.398 - ETA: 1:59 - loss: 2.6174 - acc: 0.398 - ETA: 1:57 - loss: 2.6167 - acc: 0.399 - ETA: 1:54 - loss: 2.6157 - acc: 0.399 - ETA: 1:52 - loss: 2.6158 - acc: 0.399 - ETA: 1:49 - loss: 2.6181 - acc: 0.398 - ETA: 1:47 - loss: 2.6167 - acc: 0.399 - ETA: 1:45 - loss: 2.6182 - acc: 0.399 - ETA: 1:42 - loss: 2.6175 - acc: 0.399 - ETA: 1:40 - loss: 2.6189 - acc: 0.399 - ETA: 1:37 - loss: 2.6195 - acc: 0.399 - ETA: 1:35 - loss: 2.6208 - acc: 0.398 - ETA: 1:32 - loss: 2.6188 - acc: 0.398 - ETA: 1:30 - loss: 2.6173 - acc: 0.398 - ETA: 1:28 - loss: 2.6162 - acc: 0.398 - ETA: 1:25 - loss: 2.6157 - acc: 0.398 - ETA: 1:23 - loss: 2.6145 - acc: 0.398 - ETA: 1:20 - loss: 2.6142 - acc: 0.398 - ETA: 1:18 - loss: 2.6151 - acc: 0.398 - ETA: 1:15 - loss: 2.6164 - acc: 0.398 - ETA: 1:13 - loss: 2.6160 - acc: 0.397 - ETA: 1:10 - loss: 2.6179 - acc: 0.397 - ETA: 1:08 - loss: 2.6147 - acc: 0.398 - ETA: 1:06 - loss: 2.6131 - acc: 0.398 - ETA: 1:03 - loss: 2.6134 - acc: 0.398 - ETA: 1:01 - loss: 2.6146 - acc: 0.398 - ETA: 58s - loss: 2.6134 - acc: 0.398 - ETA: 56s - loss: 2.6114 - acc: 0.39 - ETA: 53s - loss: 2.6109 - acc: 0.39 - ETA: 51s - loss: 2.6086 - acc: 0.39 - ETA: 48s - loss: 2.6074 - acc: 0.39 - ETA: 46s - loss: 2.6058 - acc: 0.40 - ETA: 44s - loss: 2.6048 - acc: 0.40 - ETA: 41s - loss: 2.6053 - acc: 0.40 - ETA: 39s - loss: 2.6060 - acc: 0.40 - ETA: 36s - loss: 2.6065 - acc: 0.40 - ETA: 34s - loss: 2.6075 - acc: 0.40 - ETA: 31s - loss: 2.6068 - acc: 0.40 - ETA: 29s - loss: 2.6068 - acc: 0.40 - ETA: 26s - loss: 2.6077 - acc: 0.40 - ETA: 24s - loss: 2.6084 - acc: 0.40 - ETA: 22s - loss: 2.6072 - acc: 0.40 - ETA: 19s - loss: 2.6067 - acc: 0.40 - ETA: 17s - loss: 2.6074 - acc: 0.40 - ETA: 14s - loss: 2.6068 - acc: 0.40 - ETA: 12s - loss: 2.6065 - acc: 0.40 - ETA: 9s - loss: 2.6051 - acc: 0.4014 - ETA: 7s - loss: 2.6051 - acc: 0.401 - ETA: 4s - loss: 2.6049 - acc: 0.401 - ETA: 2s - loss: 2.6056 - acc: 0.401 - 842s 126ms/step - loss: 2.6078 - acc: 0.4010 - val_loss: 5.2135 - val_acc: 0.0455\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 13:26 - loss: 0.7781 - acc: 0.90 - ETA: 13:25 - loss: 0.8827 - acc: 0.87 - ETA: 13:22 - loss: 0.9671 - acc: 0.86 - ETA: 13:21 - loss: 1.0183 - acc: 0.82 - ETA: 13:19 - loss: 1.0070 - acc: 0.82 - ETA: 13:17 - loss: 0.9376 - acc: 0.84 - ETA: 13:16 - loss: 1.0602 - acc: 0.81 - ETA: 13:14 - loss: 1.0475 - acc: 0.80 - ETA: 13:12 - loss: 1.0271 - acc: 0.81 - ETA: 13:10 - loss: 0.9778 - acc: 0.82 - ETA: 13:07 - loss: 1.0175 - acc: 0.80 - ETA: 13:05 - loss: 0.9703 - acc: 0.81 - ETA: 13:02 - loss: 0.9788 - acc: 0.80 - ETA: 13:00 - loss: 0.9709 - acc: 0.80 - ETA: 12:58 - loss: 0.9389 - acc: 0.80 - ETA: 12:56 - loss: 0.9032 - acc: 0.80 - ETA: 12:54 - loss: 0.9193 - acc: 0.80 - ETA: 12:51 - loss: 0.9044 - acc: 0.81 - ETA: 12:49 - loss: 0.9230 - acc: 0.80 - ETA: 12:46 - loss: 0.9099 - acc: 0.81 - ETA: 12:44 - loss: 0.8885 - acc: 0.81 - ETA: 12:41 - loss: 0.9106 - acc: 0.81 - ETA: 12:38 - loss: 0.9290 - acc: 0.80 - ETA: 12:36 - loss: 0.9076 - acc: 0.81 - ETA: 12:34 - loss: 0.9011 - acc: 0.81 - ETA: 12:31 - loss: 0.8861 - acc: 0.81 - ETA: 12:29 - loss: 0.8774 - acc: 0.82 - ETA: 12:26 - loss: 0.8589 - acc: 0.82 - ETA: 12:24 - loss: 0.8531 - acc: 0.82 - ETA: 12:22 - loss: 0.8712 - acc: 0.81 - ETA: 12:19 - loss: 0.8563 - acc: 0.81 - ETA: 12:17 - loss: 0.8489 - acc: 0.82 - ETA: 12:15 - loss: 0.8367 - acc: 0.82 - ETA: 12:12 - loss: 0.8190 - acc: 0.82 - ETA: 12:10 - loss: 0.8216 - acc: 0.82 - ETA: 12:07 - loss: 0.8240 - acc: 0.82 - ETA: 12:05 - loss: 0.8189 - acc: 0.82 - ETA: 12:02 - loss: 0.8455 - acc: 0.81 - ETA: 12:00 - loss: 0.8625 - acc: 0.81 - ETA: 11:58 - loss: 0.8599 - acc: 0.81 - ETA: 11:55 - loss: 0.8562 - acc: 0.81 - ETA: 11:53 - loss: 0.8494 - acc: 0.81 - ETA: 11:50 - loss: 0.8487 - acc: 0.81 - ETA: 11:48 - loss: 0.8368 - acc: 0.81 - ETA: 11:45 - loss: 0.8256 - acc: 0.82 - ETA: 11:43 - loss: 0.8260 - acc: 0.82 - ETA: 11:40 - loss: 0.8151 - acc: 0.82 - ETA: 11:38 - loss: 0.8295 - acc: 0.81 - ETA: 11:35 - loss: 0.8209 - acc: 0.82 - ETA: 11:33 - loss: 0.8102 - acc: 0.82 - ETA: 11:30 - loss: 0.8010 - acc: 0.82 - ETA: 11:28 - loss: 0.7972 - acc: 0.82 - ETA: 11:26 - loss: 0.7876 - acc: 0.82 - ETA: 11:23 - loss: 0.7887 - acc: 0.82 - ETA: 11:21 - loss: 0.7957 - acc: 0.82 - ETA: 11:18 - loss: 0.8041 - acc: 0.82 - ETA: 11:16 - loss: 0.8004 - acc: 0.82 - ETA: 11:13 - loss: 0.7941 - acc: 0.82 - ETA: 11:11 - loss: 0.7987 - acc: 0.82 - ETA: 11:09 - loss: 0.7934 - acc: 0.82 - ETA: 11:06 - loss: 0.7861 - acc: 0.83 - ETA: 11:04 - loss: 0.7824 - acc: 0.83 - ETA: 11:01 - loss: 0.7812 - acc: 0.82 - ETA: 10:59 - loss: 0.7807 - acc: 0.82 - ETA: 10:56 - loss: 0.7711 - acc: 0.83 - ETA: 10:54 - loss: 0.7687 - acc: 0.83 - ETA: 10:51 - loss: 0.7645 - acc: 0.83 - ETA: 10:49 - loss: 0.7622 - acc: 0.83 - ETA: 10:46 - loss: 0.7593 - acc: 0.83 - ETA: 10:44 - loss: 0.7499 - acc: 0.83 - ETA: 10:41 - loss: 0.7491 - acc: 0.83 - ETA: 10:39 - loss: 0.7493 - acc: 0.83 - ETA: 10:37 - loss: 0.7467 - acc: 0.83 - ETA: 10:34 - loss: 0.7511 - acc: 0.83 - ETA: 10:32 - loss: 0.7537 - acc: 0.83 - ETA: 10:29 - loss: 0.7491 - acc: 0.83 - ETA: 10:27 - loss: 0.7464 - acc: 0.83 - ETA: 10:24 - loss: 0.7500 - acc: 0.83 - ETA: 10:22 - loss: 0.7502 - acc: 0.83 - ETA: 10:19 - loss: 0.7539 - acc: 0.83 - ETA: 10:17 - loss: 0.7510 - acc: 0.83 - ETA: 10:14 - loss: 0.7613 - acc: 0.83 - ETA: 10:12 - loss: 0.7581 - acc: 0.83 - ETA: 10:10 - loss: 0.7534 - acc: 0.83 - ETA: 10:07 - loss: 0.7539 - acc: 0.83 - ETA: 10:05 - loss: 0.7501 - acc: 0.83 - ETA: 10:03 - loss: 0.7549 - acc: 0.83 - ETA: 10:00 - loss: 0.7505 - acc: 0.83 - ETA: 9:58 - loss: 0.7595 - acc: 0.8331 - ETA: 9:55 - loss: 0.7606 - acc: 0.833 - ETA: 9:53 - loss: 0.7640 - acc: 0.831 - ETA: 9:51 - loss: 0.7656 - acc: 0.831 - ETA: 9:48 - loss: 0.7701 - acc: 0.830 - ETA: 9:46 - loss: 0.7661 - acc: 0.830 - ETA: 9:43 - loss: 0.7626 - acc: 0.831 - ETA: 9:41 - loss: 0.7593 - acc: 0.831 - ETA: 9:38 - loss: 0.7580 - acc: 0.830 - ETA: 9:36 - loss: 0.7555 - acc: 0.831 - ETA: 9:33 - loss: 0.7568 - acc: 0.832 - ETA: 9:31 - loss: 0.7532 - acc: 0.833 - ETA: 9:29 - loss: 0.7519 - acc: 0.833 - ETA: 9:26 - loss: 0.7524 - acc: 0.833 - ETA: 9:24 - loss: 0.7524 - acc: 0.834 - ETA: 9:21 - loss: 0.7533 - acc: 0.834 - ETA: 9:19 - loss: 0.7581 - acc: 0.833 - ETA: 9:16 - loss: 0.7572 - acc: 0.834 - ETA: 9:14 - loss: 0.7570 - acc: 0.834 - ETA: 9:11 - loss: 0.7553 - acc: 0.835 - ETA: 9:09 - loss: 0.7559 - acc: 0.834 - ETA: 9:07 - loss: 0.7525 - acc: 0.835 - ETA: 9:04 - loss: 0.7527 - acc: 0.835 - ETA: 9:02 - loss: 0.7561 - acc: 0.834 - ETA: 8:59 - loss: 0.7547 - acc: 0.835 - ETA: 8:57 - loss: 0.7523 - acc: 0.834 - ETA: 8:54 - loss: 0.7534 - acc: 0.833 - ETA: 8:52 - loss: 0.7512 - acc: 0.833 - ETA: 8:50 - loss: 0.7564 - acc: 0.832 - ETA: 8:47 - loss: 0.7543 - acc: 0.832 - ETA: 8:45 - loss: 0.7550 - acc: 0.832 - ETA: 8:42 - loss: 0.7542 - acc: 0.832 - ETA: 8:40 - loss: 0.7549 - acc: 0.832 - ETA: 8:37 - loss: 0.7514 - acc: 0.833 - ETA: 8:35 - loss: 0.7514 - acc: 0.833 - ETA: 8:33 - loss: 0.7557 - acc: 0.831 - ETA: 8:30 - loss: 0.7555 - acc: 0.831 - ETA: 8:28 - loss: 0.7529 - acc: 0.832 - ETA: 8:25 - loss: 0.7530 - acc: 0.831 - ETA: 8:23 - loss: 0.7601 - acc: 0.830 - ETA: 8:20 - loss: 0.7566 - acc: 0.831 - ETA: 8:18 - loss: 0.7611 - acc: 0.830 - ETA: 8:15 - loss: 0.7607 - acc: 0.830 - ETA: 8:13 - loss: 0.7593 - acc: 0.831 - ETA: 8:10 - loss: 0.7599 - acc: 0.832 - ETA: 8:08 - loss: 0.7582 - acc: 0.832 - ETA: 8:06 - loss: 0.7573 - acc: 0.832 - ETA: 8:03 - loss: 0.7714 - acc: 0.830 - ETA: 8:01 - loss: 0.7715 - acc: 0.830 - ETA: 7:58 - loss: 0.7714 - acc: 0.830 - ETA: 7:56 - loss: 0.7723 - acc: 0.829 - ETA: 7:53 - loss: 0.7726 - acc: 0.829 - ETA: 7:51 - loss: 0.7719 - acc: 0.829 - ETA: 7:49 - loss: 0.7711 - acc: 0.829 - ETA: 7:46 - loss: 0.7672 - acc: 0.830 - ETA: 7:44 - loss: 0.7673 - acc: 0.830 - ETA: 7:41 - loss: 0.7642 - acc: 0.831 - ETA: 7:39 - loss: 0.7629 - acc: 0.831 - ETA: 7:36 - loss: 0.7656 - acc: 0.830 - ETA: 7:34 - loss: 0.7651 - acc: 0.830 - ETA: 7:31 - loss: 0.7611 - acc: 0.830 - ETA: 7:29 - loss: 0.7589 - acc: 0.831 - ETA: 7:27 - loss: 0.7592 - acc: 0.830 - ETA: 7:24 - loss: 0.7596 - acc: 0.829 - ETA: 7:22 - loss: 0.7598 - acc: 0.829 - ETA: 7:19 - loss: 0.7599 - acc: 0.828 - ETA: 7:17 - loss: 0.7581 - acc: 0.829 - ETA: 7:14 - loss: 0.7598 - acc: 0.829 - ETA: 7:12 - loss: 0.7575 - acc: 0.829 - ETA: 7:10 - loss: 0.7550 - acc: 0.830 - ETA: 7:07 - loss: 0.7548 - acc: 0.830 - ETA: 7:05 - loss: 0.7529 - acc: 0.830 - ETA: 7:02 - loss: 0.7575 - acc: 0.828 - ETA: 7:00 - loss: 0.7588 - acc: 0.828 - ETA: 6:57 - loss: 0.7561 - acc: 0.829 - ETA: 6:55 - loss: 0.7555 - acc: 0.829 - ETA: 6:52 - loss: 0.7532 - acc: 0.829 - ETA: 6:50 - loss: 0.7511 - acc: 0.830 - ETA: 6:47 - loss: 0.7509 - acc: 0.829 - ETA: 6:45 - loss: 0.7498 - acc: 0.829 - ETA: 6:43 - loss: 0.7474 - acc: 0.830 - ETA: 6:40 - loss: 0.7480 - acc: 0.830 - ETA: 6:38 - loss: 0.7466 - acc: 0.830 - ETA: 6:35 - loss: 0.7440 - acc: 0.830 - ETA: 6:33 - loss: 0.7413 - acc: 0.830 - ETA: 6:30 - loss: 0.7452 - acc: 0.829 - ETA: 6:28 - loss: 0.7441 - acc: 0.830 - ETA: 6:25 - loss: 0.7436 - acc: 0.830 - ETA: 6:23 - loss: 0.7450 - acc: 0.830 - ETA: 6:21 - loss: 0.7440 - acc: 0.830 - ETA: 6:18 - loss: 0.7440 - acc: 0.831 - ETA: 6:16 - loss: 0.7471 - acc: 0.830 - ETA: 6:13 - loss: 0.7486 - acc: 0.830 - ETA: 6:11 - loss: 0.7509 - acc: 0.829 - ETA: 6:08 - loss: 0.7523 - acc: 0.829 - ETA: 6:06 - loss: 0.7510 - acc: 0.829 - ETA: 6:04 - loss: 0.7522 - acc: 0.829 - ETA: 6:01 - loss: 0.7524 - acc: 0.829 - ETA: 5:59 - loss: 0.7516 - acc: 0.829 - ETA: 5:56 - loss: 0.7514 - acc: 0.829 - ETA: 5:54 - loss: 0.7528 - acc: 0.828 - ETA: 5:51 - loss: 0.7533 - acc: 0.828 - ETA: 5:49 - loss: 0.7559 - acc: 0.828 - ETA: 5:46 - loss: 0.7562 - acc: 0.828 - ETA: 5:44 - loss: 0.7574 - acc: 0.828 - ETA: 5:42 - loss: 0.7587 - acc: 0.827 - ETA: 5:39 - loss: 0.7597 - acc: 0.827 - ETA: 5:37 - loss: 0.7588 - acc: 0.827 - ETA: 5:34 - loss: 0.7604 - acc: 0.827 - ETA: 5:32 - loss: 0.7619 - acc: 0.827 - ETA: 5:29 - loss: 0.7621 - acc: 0.826 - ETA: 5:27 - loss: 0.7675 - acc: 0.826 - ETA: 5:24 - loss: 0.7665 - acc: 0.826 - ETA: 5:22 - loss: 0.7664 - acc: 0.826 - ETA: 5:20 - loss: 0.7714 - acc: 0.825 - ETA: 5:17 - loss: 0.7710 - acc: 0.8257"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5:15 - loss: 0.7717 - acc: 0.825 - ETA: 5:12 - loss: 0.7751 - acc: 0.824 - ETA: 5:10 - loss: 0.7740 - acc: 0.824 - ETA: 5:07 - loss: 0.7743 - acc: 0.824 - ETA: 5:05 - loss: 0.7724 - acc: 0.825 - ETA: 5:03 - loss: 0.7698 - acc: 0.825 - ETA: 5:00 - loss: 0.7720 - acc: 0.825 - ETA: 4:58 - loss: 0.7739 - acc: 0.825 - ETA: 4:55 - loss: 0.7789 - acc: 0.823 - ETA: 4:53 - loss: 0.7804 - acc: 0.822 - ETA: 4:50 - loss: 0.7801 - acc: 0.822 - ETA: 4:48 - loss: 0.7788 - acc: 0.822 - ETA: 4:45 - loss: 0.7807 - acc: 0.821 - ETA: 4:43 - loss: 0.7790 - acc: 0.822 - ETA: 4:41 - loss: 0.7776 - acc: 0.821 - ETA: 4:38 - loss: 0.7783 - acc: 0.821 - ETA: 4:36 - loss: 0.7826 - acc: 0.821 - ETA: 4:33 - loss: 0.7843 - acc: 0.821 - ETA: 4:31 - loss: 0.7850 - acc: 0.820 - ETA: 4:28 - loss: 0.7871 - acc: 0.819 - ETA: 4:26 - loss: 0.7851 - acc: 0.820 - ETA: 4:23 - loss: 0.7847 - acc: 0.820 - ETA: 4:21 - loss: 0.7857 - acc: 0.820 - ETA: 4:19 - loss: 0.7872 - acc: 0.820 - ETA: 4:16 - loss: 0.7875 - acc: 0.819 - ETA: 4:14 - loss: 0.7904 - acc: 0.818 - ETA: 4:11 - loss: 0.7903 - acc: 0.818 - ETA: 4:09 - loss: 0.7903 - acc: 0.818 - ETA: 4:06 - loss: 0.7886 - acc: 0.819 - ETA: 4:04 - loss: 0.7869 - acc: 0.819 - ETA: 4:01 - loss: 0.7878 - acc: 0.819 - ETA: 3:59 - loss: 0.7873 - acc: 0.819 - ETA: 3:57 - loss: 0.7884 - acc: 0.819 - ETA: 3:54 - loss: 0.7892 - acc: 0.818 - ETA: 3:52 - loss: 0.7886 - acc: 0.819 - ETA: 3:49 - loss: 0.7964 - acc: 0.817 - ETA: 3:47 - loss: 0.7960 - acc: 0.817 - ETA: 3:44 - loss: 0.7935 - acc: 0.817 - ETA: 3:42 - loss: 0.7930 - acc: 0.817 - ETA: 3:40 - loss: 0.7910 - acc: 0.818 - ETA: 3:37 - loss: 0.7931 - acc: 0.818 - ETA: 3:35 - loss: 0.7957 - acc: 0.817 - ETA: 3:32 - loss: 0.7942 - acc: 0.818 - ETA: 3:30 - loss: 0.7919 - acc: 0.818 - ETA: 3:27 - loss: 0.7921 - acc: 0.818 - ETA: 3:25 - loss: 0.7925 - acc: 0.818 - ETA: 3:22 - loss: 0.7910 - acc: 0.818 - ETA: 3:20 - loss: 0.7887 - acc: 0.819 - ETA: 3:18 - loss: 0.7900 - acc: 0.819 - ETA: 3:15 - loss: 0.7901 - acc: 0.818 - ETA: 3:13 - loss: 0.7901 - acc: 0.818 - ETA: 3:10 - loss: 0.7897 - acc: 0.818 - ETA: 3:08 - loss: 0.7880 - acc: 0.818 - ETA: 3:05 - loss: 0.7857 - acc: 0.819 - ETA: 3:03 - loss: 0.7850 - acc: 0.819 - ETA: 3:00 - loss: 0.7849 - acc: 0.819 - ETA: 2:58 - loss: 0.7842 - acc: 0.819 - ETA: 2:55 - loss: 0.7870 - acc: 0.819 - ETA: 2:53 - loss: 0.7899 - acc: 0.818 - ETA: 2:51 - loss: 0.7894 - acc: 0.818 - ETA: 2:48 - loss: 0.7911 - acc: 0.818 - ETA: 2:46 - loss: 0.7894 - acc: 0.818 - ETA: 2:43 - loss: 0.7903 - acc: 0.818 - ETA: 2:41 - loss: 0.7903 - acc: 0.818 - ETA: 2:38 - loss: 0.7900 - acc: 0.818 - ETA: 2:36 - loss: 0.7892 - acc: 0.818 - ETA: 2:34 - loss: 0.7922 - acc: 0.817 - ETA: 2:31 - loss: 0.7976 - acc: 0.816 - ETA: 2:29 - loss: 0.7969 - acc: 0.816 - ETA: 2:26 - loss: 0.7992 - acc: 0.816 - ETA: 2:24 - loss: 0.8006 - acc: 0.816 - ETA: 2:21 - loss: 0.8004 - acc: 0.816 - ETA: 2:19 - loss: 0.7991 - acc: 0.816 - ETA: 2:16 - loss: 0.8002 - acc: 0.815 - ETA: 2:14 - loss: 0.8000 - acc: 0.815 - ETA: 2:12 - loss: 0.7992 - acc: 0.815 - ETA: 2:09 - loss: 0.7995 - acc: 0.815 - ETA: 2:07 - loss: 0.8001 - acc: 0.815 - ETA: 2:04 - loss: 0.7998 - acc: 0.815 - ETA: 2:02 - loss: 0.8000 - acc: 0.815 - ETA: 1:59 - loss: 0.8024 - acc: 0.815 - ETA: 1:57 - loss: 0.8028 - acc: 0.814 - ETA: 1:54 - loss: 0.8051 - acc: 0.813 - ETA: 1:52 - loss: 0.8042 - acc: 0.813 - ETA: 1:49 - loss: 0.8022 - acc: 0.814 - ETA: 1:47 - loss: 0.8026 - acc: 0.814 - ETA: 1:45 - loss: 0.8079 - acc: 0.813 - ETA: 1:42 - loss: 0.8081 - acc: 0.812 - ETA: 1:40 - loss: 0.8106 - acc: 0.812 - ETA: 1:37 - loss: 0.8108 - acc: 0.812 - ETA: 1:35 - loss: 0.8105 - acc: 0.812 - ETA: 1:32 - loss: 0.8103 - acc: 0.812 - ETA: 1:30 - loss: 0.8131 - acc: 0.811 - ETA: 1:28 - loss: 0.8124 - acc: 0.811 - ETA: 1:25 - loss: 0.8142 - acc: 0.811 - ETA: 1:23 - loss: 0.8150 - acc: 0.810 - ETA: 1:20 - loss: 0.8144 - acc: 0.811 - ETA: 1:18 - loss: 0.8138 - acc: 0.811 - ETA: 1:15 - loss: 0.8156 - acc: 0.811 - ETA: 1:13 - loss: 0.8189 - acc: 0.809 - ETA: 1:10 - loss: 0.8191 - acc: 0.809 - ETA: 1:08 - loss: 0.8193 - acc: 0.809 - ETA: 1:06 - loss: 0.8185 - acc: 0.809 - ETA: 1:03 - loss: 0.8183 - acc: 0.809 - ETA: 1:01 - loss: 0.8180 - acc: 0.809 - ETA: 58s - loss: 0.8188 - acc: 0.809 - ETA: 56s - loss: 0.8169 - acc: 0.81 - ETA: 53s - loss: 0.8164 - acc: 0.81 - ETA: 51s - loss: 0.8153 - acc: 0.81 - ETA: 48s - loss: 0.8154 - acc: 0.81 - ETA: 46s - loss: 0.8147 - acc: 0.81 - ETA: 44s - loss: 0.8142 - acc: 0.81 - ETA: 41s - loss: 0.8133 - acc: 0.81 - ETA: 39s - loss: 0.8146 - acc: 0.81 - ETA: 36s - loss: 0.8146 - acc: 0.80 - ETA: 34s - loss: 0.8139 - acc: 0.81 - ETA: 31s - loss: 0.8156 - acc: 0.80 - ETA: 29s - loss: 0.8164 - acc: 0.80 - ETA: 26s - loss: 0.8176 - acc: 0.80 - ETA: 24s - loss: 0.8166 - acc: 0.80 - ETA: 22s - loss: 0.8165 - acc: 0.80 - ETA: 19s - loss: 0.8168 - acc: 0.80 - ETA: 17s - loss: 0.8165 - acc: 0.80 - ETA: 14s - loss: 0.8150 - acc: 0.80 - ETA: 12s - loss: 0.8150 - acc: 0.81 - ETA: 9s - loss: 0.8153 - acc: 0.8098 - ETA: 7s - loss: 0.8148 - acc: 0.809 - ETA: 4s - loss: 0.8176 - acc: 0.809 - ETA: 2s - loss: 0.8181 - acc: 0.809 - 842s 126ms/step - loss: 0.8187 - acc: 0.8090 - val_loss: 7.9471 - val_acc: 0.0443\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/10\n",
      "4080/6680 [=================>............] - ETA: 13:32 - loss: 0.1127 - acc: 1.00 - ETA: 13:26 - loss: 0.1093 - acc: 0.97 - ETA: 13:26 - loss: 0.1422 - acc: 0.96 - ETA: 13:22 - loss: 0.1090 - acc: 0.97 - ETA: 13:23 - loss: 0.1092 - acc: 0.97 - ETA: 13:20 - loss: 0.1074 - acc: 0.97 - ETA: 13:18 - loss: 0.1130 - acc: 0.97 - ETA: 13:15 - loss: 0.1743 - acc: 0.96 - ETA: 13:13 - loss: 0.1560 - acc: 0.97 - ETA: 13:11 - loss: 0.1710 - acc: 0.96 - ETA: 13:08 - loss: 0.2072 - acc: 0.95 - ETA: 13:04 - loss: 0.1962 - acc: 0.96 - ETA: 13:03 - loss: 0.1911 - acc: 0.96 - ETA: 13:01 - loss: 0.1847 - acc: 0.96 - ETA: 12:59 - loss: 0.1801 - acc: 0.97 - ETA: 12:56 - loss: 0.1723 - acc: 0.97 - ETA: 12:54 - loss: 0.1867 - acc: 0.96 - ETA: 12:52 - loss: 0.2278 - acc: 0.95 - ETA: 12:49 - loss: 0.2359 - acc: 0.95 - ETA: 12:47 - loss: 0.2661 - acc: 0.95 - ETA: 12:44 - loss: 0.2580 - acc: 0.95 - ETA: 12:42 - loss: 0.2778 - acc: 0.95 - ETA: 12:40 - loss: 0.2681 - acc: 0.95 - ETA: 12:38 - loss: 0.2816 - acc: 0.95 - ETA: 12:35 - loss: 0.2749 - acc: 0.95 - ETA: 12:33 - loss: 0.3200 - acc: 0.93 - ETA: 12:30 - loss: 0.3135 - acc: 0.94 - ETA: 12:28 - loss: 0.3100 - acc: 0.94 - ETA: 12:25 - loss: 0.3042 - acc: 0.94 - ETA: 12:23 - loss: 0.2985 - acc: 0.94 - ETA: 12:20 - loss: 0.3051 - acc: 0.94 - ETA: 12:18 - loss: 0.3003 - acc: 0.94 - ETA: 12:16 - loss: 0.2974 - acc: 0.94 - ETA: 12:13 - loss: 0.3031 - acc: 0.93 - ETA: 12:11 - loss: 0.2950 - acc: 0.94 - ETA: 12:08 - loss: 0.2888 - acc: 0.94 - ETA: 12:06 - loss: 0.2815 - acc: 0.94 - ETA: 12:03 - loss: 0.2804 - acc: 0.94 - ETA: 12:01 - loss: 0.2739 - acc: 0.94 - ETA: 11:58 - loss: 0.2694 - acc: 0.94 - ETA: 11:56 - loss: 0.2685 - acc: 0.94 - ETA: 11:53 - loss: 0.2725 - acc: 0.94 - ETA: 11:51 - loss: 0.3813 - acc: 0.93 - ETA: 11:48 - loss: 0.4163 - acc: 0.92 - ETA: 11:46 - loss: 0.4084 - acc: 0.92 - ETA: 11:43 - loss: 0.4029 - acc: 0.92 - ETA: 11:41 - loss: 0.4037 - acc: 0.92 - ETA: 11:38 - loss: 0.4157 - acc: 0.92 - ETA: 11:36 - loss: 0.4119 - acc: 0.92 - ETA: 11:34 - loss: 0.4146 - acc: 0.92 - ETA: 11:31 - loss: 0.4067 - acc: 0.92 - ETA: 11:29 - loss: 0.4010 - acc: 0.92 - ETA: 11:26 - loss: 0.3938 - acc: 0.92 - ETA: 11:24 - loss: 0.4066 - acc: 0.92 - ETA: 11:21 - loss: 0.4058 - acc: 0.92 - ETA: 11:19 - loss: 0.4004 - acc: 0.92 - ETA: 11:17 - loss: 0.4024 - acc: 0.92 - ETA: 11:14 - loss: 0.4007 - acc: 0.92 - ETA: 11:11 - loss: 0.4026 - acc: 0.92 - ETA: 11:09 - loss: 0.3998 - acc: 0.92 - ETA: 11:07 - loss: 0.3939 - acc: 0.92 - ETA: 11:04 - loss: 0.3896 - acc: 0.92 - ETA: 11:02 - loss: 0.3859 - acc: 0.92 - ETA: 10:59 - loss: 0.3809 - acc: 0.92 - ETA: 10:57 - loss: 0.3774 - acc: 0.92 - ETA: 10:55 - loss: 0.3733 - acc: 0.93 - ETA: 10:52 - loss: 0.3691 - acc: 0.93 - ETA: 10:50 - loss: 0.3649 - acc: 0.93 - ETA: 10:47 - loss: 0.3704 - acc: 0.93 - ETA: 10:45 - loss: 0.3678 - acc: 0.93 - ETA: 10:42 - loss: 0.3629 - acc: 0.93 - ETA: 10:40 - loss: 0.3588 - acc: 0.93 - ETA: 10:37 - loss: 0.3549 - acc: 0.93 - ETA: 10:35 - loss: 0.3554 - acc: 0.93 - ETA: 10:33 - loss: 0.3565 - acc: 0.93 - ETA: 10:30 - loss: 0.3526 - acc: 0.93 - ETA: 10:28 - loss: 0.3526 - acc: 0.93 - ETA: 10:25 - loss: 0.3485 - acc: 0.93 - ETA: 10:23 - loss: 0.3478 - acc: 0.93 - ETA: 10:20 - loss: 0.3477 - acc: 0.93 - ETA: 10:18 - loss: 0.3466 - acc: 0.93 - ETA: 10:16 - loss: 0.3594 - acc: 0.93 - ETA: 10:13 - loss: 0.3803 - acc: 0.93 - ETA: 10:11 - loss: 0.3768 - acc: 0.93 - ETA: 10:09 - loss: 0.3743 - acc: 0.93 - ETA: 10:06 - loss: 0.3718 - acc: 0.93 - ETA: 10:04 - loss: 0.3695 - acc: 0.93 - ETA: 10:01 - loss: 0.3679 - acc: 0.93 - ETA: 9:59 - loss: 0.3644 - acc: 0.9354 - ETA: 9:56 - loss: 0.3624 - acc: 0.935 - ETA: 9:54 - loss: 0.3605 - acc: 0.935 - ETA: 9:51 - loss: 0.3572 - acc: 0.936 - ETA: 9:49 - loss: 0.3539 - acc: 0.937 - ETA: 9:46 - loss: 0.3716 - acc: 0.934 - ETA: 9:44 - loss: 0.3742 - acc: 0.934 - ETA: 9:41 - loss: 0.3711 - acc: 0.934 - ETA: 9:39 - loss: 0.3692 - acc: 0.935 - ETA: 9:37 - loss: 0.3655 - acc: 0.935 - ETA: 9:34 - loss: 0.3647 - acc: 0.935 - ETA: 9:32 - loss: 0.3630 - acc: 0.936 - ETA: 9:29 - loss: 0.3598 - acc: 0.936 - ETA: 9:27 - loss: 0.3565 - acc: 0.937 - ETA: 9:24 - loss: 0.3555 - acc: 0.937 - ETA: 9:22 - loss: 0.3569 - acc: 0.937 - ETA: 9:20 - loss: 0.3543 - acc: 0.938 - ETA: 9:17 - loss: 0.3526 - acc: 0.938 - ETA: 9:15 - loss: 0.3496 - acc: 0.938 - ETA: 9:12 - loss: 0.3506 - acc: 0.937 - ETA: 9:10 - loss: 0.3482 - acc: 0.938 - ETA: 9:07 - loss: 0.3466 - acc: 0.938 - ETA: 9:05 - loss: 0.3446 - acc: 0.938 - ETA: 9:02 - loss: 0.3415 - acc: 0.939 - ETA: 9:00 - loss: 0.3414 - acc: 0.939 - ETA: 8:57 - loss: 0.3406 - acc: 0.939 - ETA: 8:55 - loss: 0.3384 - acc: 0.939 - ETA: 8:53 - loss: 0.3377 - acc: 0.939 - ETA: 8:50 - loss: 0.3366 - acc: 0.938 - ETA: 8:48 - loss: 0.3350 - acc: 0.939 - ETA: 8:45 - loss: 0.3333 - acc: 0.939 - ETA: 8:43 - loss: 0.3307 - acc: 0.940 - ETA: 8:40 - loss: 0.3294 - acc: 0.940 - ETA: 8:38 - loss: 0.3278 - acc: 0.940 - ETA: 8:36 - loss: 0.3263 - acc: 0.940 - ETA: 8:33 - loss: 0.3253 - acc: 0.940 - ETA: 8:31 - loss: 0.3246 - acc: 0.940 - ETA: 8:28 - loss: 0.3221 - acc: 0.940 - ETA: 8:26 - loss: 0.3215 - acc: 0.940 - ETA: 8:23 - loss: 0.3195 - acc: 0.940 - ETA: 8:21 - loss: 0.3172 - acc: 0.941 - ETA: 8:18 - loss: 0.3148 - acc: 0.941 - ETA: 8:16 - loss: 0.3216 - acc: 0.940 - ETA: 8:14 - loss: 0.3193 - acc: 0.940 - ETA: 8:11 - loss: 0.3174 - acc: 0.941 - ETA: 8:09 - loss: 0.3186 - acc: 0.941 - ETA: 8:06 - loss: 0.3173 - acc: 0.941 - ETA: 8:04 - loss: 0.3161 - acc: 0.941 - ETA: 8:01 - loss: 0.3187 - acc: 0.940 - ETA: 7:59 - loss: 0.3201 - acc: 0.939 - ETA: 7:57 - loss: 0.3187 - acc: 0.939 - ETA: 7:54 - loss: 0.3167 - acc: 0.940 - ETA: 7:52 - loss: 0.3149 - acc: 0.940 - ETA: 7:49 - loss: 0.3148 - acc: 0.940 - ETA: 7:47 - loss: 0.3144 - acc: 0.940 - ETA: 7:44 - loss: 0.3279 - acc: 0.937 - ETA: 7:42 - loss: 0.3273 - acc: 0.937 - ETA: 7:39 - loss: 0.3264 - acc: 0.937 - ETA: 7:37 - loss: 0.3264 - acc: 0.937 - ETA: 7:35 - loss: 0.3271 - acc: 0.936 - ETA: 7:32 - loss: 0.3275 - acc: 0.936 - ETA: 7:30 - loss: 0.3284 - acc: 0.936 - ETA: 7:27 - loss: 0.3327 - acc: 0.934 - ETA: 7:25 - loss: 0.3391 - acc: 0.933 - ETA: 7:22 - loss: 0.3387 - acc: 0.933 - ETA: 7:20 - loss: 0.3366 - acc: 0.933 - ETA: 7:18 - loss: 0.3357 - acc: 0.934 - ETA: 7:15 - loss: 0.3389 - acc: 0.934 - ETA: 7:13 - loss: 0.3391 - acc: 0.934 - ETA: 7:10 - loss: 0.3408 - acc: 0.933 - ETA: 7:08 - loss: 0.3397 - acc: 0.934 - ETA: 7:05 - loss: 0.3396 - acc: 0.934 - ETA: 7:03 - loss: 0.3383 - acc: 0.934 - ETA: 7:00 - loss: 0.3370 - acc: 0.934 - ETA: 6:58 - loss: 0.3350 - acc: 0.935 - ETA: 6:55 - loss: 0.3341 - acc: 0.934 - ETA: 6:53 - loss: 0.3325 - acc: 0.935 - ETA: 6:50 - loss: 0.3322 - acc: 0.935 - ETA: 6:48 - loss: 0.3315 - acc: 0.935 - ETA: 6:46 - loss: 0.3303 - acc: 0.935 - ETA: 6:43 - loss: 0.3299 - acc: 0.935 - ETA: 6:41 - loss: 0.3281 - acc: 0.935 - ETA: 6:38 - loss: 0.3270 - acc: 0.936 - ETA: 6:36 - loss: 0.3287 - acc: 0.935 - ETA: 6:33 - loss: 0.3270 - acc: 0.936 - ETA: 6:31 - loss: 0.3275 - acc: 0.936 - ETA: 6:28 - loss: 0.3275 - acc: 0.936 - ETA: 6:26 - loss: 0.3258 - acc: 0.936 - ETA: 6:23 - loss: 0.3254 - acc: 0.936 - ETA: 6:21 - loss: 0.3238 - acc: 0.936 - ETA: 6:19 - loss: 0.3225 - acc: 0.936 - ETA: 6:16 - loss: 0.3243 - acc: 0.936 - ETA: 6:14 - loss: 0.3250 - acc: 0.936 - ETA: 6:11 - loss: 0.3261 - acc: 0.935 - ETA: 6:09 - loss: 0.3244 - acc: 0.936 - ETA: 6:06 - loss: 0.3232 - acc: 0.936 - ETA: 6:04 - loss: 0.3231 - acc: 0.935 - ETA: 6:01 - loss: 0.3224 - acc: 0.936 - ETA: 5:59 - loss: 0.3222 - acc: 0.935 - ETA: 5:56 - loss: 0.3215 - acc: 0.935 - ETA: 5:54 - loss: 0.3207 - acc: 0.936 - ETA: 5:52 - loss: 0.3196 - acc: 0.936 - ETA: 5:49 - loss: 0.3185 - acc: 0.936 - ETA: 5:47 - loss: 0.3171 - acc: 0.936 - ETA: 5:44 - loss: 0.3156 - acc: 0.936 - ETA: 5:42 - loss: 0.3143 - acc: 0.937 - ETA: 5:39 - loss: 0.3135 - acc: 0.937 - ETA: 5:37 - loss: 0.3123 - acc: 0.937 - ETA: 5:35 - loss: 0.3112 - acc: 0.937 - ETA: 5:32 - loss: 0.3100 - acc: 0.937 - ETA: 5:30 - loss: 0.3086 - acc: 0.938 - ETA: 5:27 - loss: 0.3074 - acc: 0.938 - ETA: 5:25 - loss: 0.3078 - acc: 0.938 - ETA: 5:22 - loss: 0.3063 - acc: 0.938 - ETA: 5:20 - loss: 0.3057 - acc: 0.938 - ETA: 5:17 - loss: 0.3045 - acc: 0.93906680/6680 [==============================] - ETA: 5:15 - loss: 0.3040 - acc: 0.939 - ETA: 5:12 - loss: 0.3040 - acc: 0.938 - ETA: 5:10 - loss: 0.3031 - acc: 0.938 - ETA: 5:08 - loss: 0.3067 - acc: 0.938 - ETA: 5:05 - loss: 0.3109 - acc: 0.937 - ETA: 5:03 - loss: 0.3109 - acc: 0.936 - ETA: 5:00 - loss: 0.3097 - acc: 0.937 - ETA: 4:58 - loss: 0.3108 - acc: 0.936 - ETA: 4:55 - loss: 0.3107 - acc: 0.936 - ETA: 4:53 - loss: 0.3098 - acc: 0.936 - ETA: 4:51 - loss: 0.3107 - acc: 0.937 - ETA: 4:48 - loss: 0.3099 - acc: 0.937 - ETA: 4:46 - loss: 0.3101 - acc: 0.937 - ETA: 4:43 - loss: 0.3103 - acc: 0.937 - ETA: 4:41 - loss: 0.3101 - acc: 0.937 - ETA: 4:38 - loss: 0.3105 - acc: 0.936 - ETA: 4:36 - loss: 0.3100 - acc: 0.936 - ETA: 4:33 - loss: 0.3091 - acc: 0.936 - ETA: 4:31 - loss: 0.3081 - acc: 0.937 - ETA: 4:28 - loss: 0.3072 - acc: 0.937 - ETA: 4:26 - loss: 0.3063 - acc: 0.937 - ETA: 4:24 - loss: 0.3058 - acc: 0.936 - ETA: 4:21 - loss: 0.3045 - acc: 0.937 - ETA: 4:19 - loss: 0.3062 - acc: 0.937 - ETA: 4:16 - loss: 0.3070 - acc: 0.936 - ETA: 4:14 - loss: 0.3108 - acc: 0.936 - ETA: 4:11 - loss: 0.3119 - acc: 0.935 - ETA: 4:09 - loss: 0.3126 - acc: 0.935 - ETA: 4:07 - loss: 0.3127 - acc: 0.935 - ETA: 4:04 - loss: 0.3125 - acc: 0.935 - ETA: 4:02 - loss: 0.3133 - acc: 0.935 - ETA: 3:59 - loss: 0.3122 - acc: 0.936 - ETA: 3:57 - loss: 0.3115 - acc: 0.936 - ETA: 3:54 - loss: 0.3106 - acc: 0.936 - ETA: 3:52 - loss: 0.3106 - acc: 0.936 - ETA: 3:49 - loss: 0.3095 - acc: 0.936 - ETA: 3:47 - loss: 0.3108 - acc: 0.935 - ETA: 3:45 - loss: 0.3097 - acc: 0.936 - ETA: 3:42 - loss: 0.3087 - acc: 0.936 - ETA: 3:40 - loss: 0.3098 - acc: 0.936 - ETA: 3:37 - loss: 0.3088 - acc: 0.936 - ETA: 3:35 - loss: 0.3083 - acc: 0.936 - ETA: 3:32 - loss: 0.3082 - acc: 0.936 - ETA: 3:30 - loss: 0.3112 - acc: 0.935 - ETA: 3:27 - loss: 0.3120 - acc: 0.935 - ETA: 3:25 - loss: 0.3110 - acc: 0.935 - ETA: 3:23 - loss: 0.3135 - acc: 0.935 - ETA: 3:20 - loss: 0.3190 - acc: 0.934 - ETA: 3:18 - loss: 0.3209 - acc: 0.934 - ETA: 3:15 - loss: 0.3249 - acc: 0.934 - ETA: 3:13 - loss: 0.3281 - acc: 0.933 - ETA: 3:10 - loss: 0.3304 - acc: 0.933 - ETA: 3:08 - loss: 0.3298 - acc: 0.933 - ETA: 3:05 - loss: 0.3476 - acc: 0.931 - ETA: 3:03 - loss: 0.3527 - acc: 0.930 - ETA: 3:01 - loss: 0.3538 - acc: 0.930 - ETA: 2:58 - loss: 0.3553 - acc: 0.929 - ETA: 2:56 - loss: 0.3552 - acc: 0.929 - ETA: 2:53 - loss: 0.3543 - acc: 0.929 - ETA: 2:51 - loss: 0.3545 - acc: 0.929 - ETA: 2:48 - loss: 0.3532 - acc: 0.929 - ETA: 2:46 - loss: 0.3524 - acc: 0.929 - ETA: 2:43 - loss: 0.3519 - acc: 0.929 - ETA: 2:41 - loss: 0.3512 - acc: 0.929 - ETA: 2:38 - loss: 0.3507 - acc: 0.929 - ETA: 2:36 - loss: 0.3495 - acc: 0.929 - ETA: 2:34 - loss: 0.3482 - acc: 0.929 - ETA: 2:31 - loss: 0.3491 - acc: 0.929 - ETA: 2:29 - loss: 0.3490 - acc: 0.929 - ETA: 2:26 - loss: 0.3479 - acc: 0.929 - ETA: 2:24 - loss: 0.3475 - acc: 0.930 - ETA: 2:21 - loss: 0.3496 - acc: 0.930 - ETA: 2:19 - loss: 0.3495 - acc: 0.929 - ETA: 2:16 - loss: 0.3493 - acc: 0.929 - ETA: 2:14 - loss: 0.3487 - acc: 0.929 - ETA: 2:12 - loss: 0.3476 - acc: 0.930 - ETA: 2:09 - loss: 0.3474 - acc: 0.930 - ETA: 2:07 - loss: 0.3493 - acc: 0.929 - ETA: 2:04 - loss: 0.3485 - acc: 0.929 - ETA: 2:02 - loss: 0.3494 - acc: 0.929 - ETA: 1:59 - loss: 0.3490 - acc: 0.929 - ETA: 1:57 - loss: 0.3480 - acc: 0.929 - ETA: 1:54 - loss: 0.3474 - acc: 0.929 - ETA: 1:52 - loss: 0.3463 - acc: 0.930 - ETA: 1:50 - loss: 0.3455 - acc: 0.930 - ETA: 1:47 - loss: 0.3449 - acc: 0.930 - ETA: 1:45 - loss: 0.3462 - acc: 0.930 - ETA: 1:42 - loss: 0.3457 - acc: 0.930 - ETA: 1:40 - loss: 0.3468 - acc: 0.930 - ETA: 1:37 - loss: 0.3461 - acc: 0.930 - ETA: 1:35 - loss: 0.3459 - acc: 0.930 - ETA: 1:32 - loss: 0.3459 - acc: 0.930 - ETA: 1:30 - loss: 0.3462 - acc: 0.930 - ETA: 1:28 - loss: 0.3464 - acc: 0.930 - ETA: 1:25 - loss: 0.3472 - acc: 0.929 - ETA: 1:23 - loss: 0.3490 - acc: 0.929 - ETA: 1:20 - loss: 0.3517 - acc: 0.929 - ETA: 1:18 - loss: 0.3529 - acc: 0.928 - ETA: 1:15 - loss: 0.3524 - acc: 0.928 - ETA: 1:13 - loss: 0.3525 - acc: 0.928 - ETA: 1:10 - loss: 0.3526 - acc: 0.928 - ETA: 1:08 - loss: 0.3523 - acc: 0.928 - ETA: 1:06 - loss: 0.3525 - acc: 0.928 - ETA: 1:03 - loss: 0.3516 - acc: 0.928 - ETA: 1:01 - loss: 0.3511 - acc: 0.928 - ETA: 58s - loss: 0.3522 - acc: 0.928 - ETA: 56s - loss: 0.3519 - acc: 0.92 - ETA: 53s - loss: 0.3515 - acc: 0.92 - ETA: 51s - loss: 0.3509 - acc: 0.92 - ETA: 48s - loss: 0.3501 - acc: 0.92 - ETA: 46s - loss: 0.3496 - acc: 0.92 - ETA: 44s - loss: 0.3486 - acc: 0.92 - ETA: 41s - loss: 0.3511 - acc: 0.92 - ETA: 39s - loss: 0.3504 - acc: 0.92 - ETA: 36s - loss: 0.3500 - acc: 0.92 - ETA: 34s - loss: 0.3496 - acc: 0.92 - ETA: 31s - loss: 0.3508 - acc: 0.92 - ETA: 29s - loss: 0.3506 - acc: 0.92 - ETA: 26s - loss: 0.3513 - acc: 0.92 - ETA: 24s - loss: 0.3508 - acc: 0.92 - ETA: 22s - loss: 0.3498 - acc: 0.92 - ETA: 19s - loss: 0.3527 - acc: 0.92 - ETA: 17s - loss: 0.3535 - acc: 0.92 - ETA: 14s - loss: 0.3572 - acc: 0.92 - ETA: 12s - loss: 0.3598 - acc: 0.92 - ETA: 9s - loss: 0.3593 - acc: 0.9268 - ETA: 7s - loss: 0.3583 - acc: 0.927 - ETA: 4s - loss: 0.3574 - acc: 0.927 - ETA: 2s - loss: 0.3576 - acc: 0.927 - 843s 126ms/step - loss: 0.3566 - acc: 0.9274 - val_loss: 9.2680 - val_acc: 0.0479\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 13:37 - loss: 0.0086 - acc: 1.00 - ETA: 13:30 - loss: 0.0979 - acc: 0.97 - ETA: 13:31 - loss: 0.0930 - acc: 0.98 - ETA: 13:27 - loss: 0.0745 - acc: 0.98 - ETA: 13:24 - loss: 0.0697 - acc: 0.98 - ETA: 13:23 - loss: 0.0588 - acc: 0.98 - ETA: 13:21 - loss: 0.0529 - acc: 0.98 - ETA: 13:19 - loss: 0.0495 - acc: 0.98 - ETA: 13:15 - loss: 0.0505 - acc: 0.98 - ETA: 13:13 - loss: 0.0457 - acc: 0.98 - ETA: 13:11 - loss: 0.0519 - acc: 0.98 - ETA: 13:08 - loss: 0.0788 - acc: 0.97 - ETA: 13:06 - loss: 0.1243 - acc: 0.96 - ETA: 13:03 - loss: 0.1289 - acc: 0.96 - ETA: 13:00 - loss: 0.1834 - acc: 0.96 - ETA: 12:58 - loss: 0.1937 - acc: 0.96 - ETA: 12:55 - loss: 0.1936 - acc: 0.96 - ETA: 12:53 - loss: 0.2012 - acc: 0.95 - ETA: 12:51 - loss: 0.1917 - acc: 0.96 - ETA: 12:48 - loss: 0.2505 - acc: 0.95 - ETA: 12:46 - loss: 0.2524 - acc: 0.95 - ETA: 12:44 - loss: 0.2511 - acc: 0.95 - ETA: 12:41 - loss: 0.2461 - acc: 0.95 - ETA: 12:38 - loss: 0.2469 - acc: 0.95 - ETA: 12:35 - loss: 0.2445 - acc: 0.96 - ETA: 12:33 - loss: 0.2363 - acc: 0.96 - ETA: 12:31 - loss: 0.2287 - acc: 0.96 - ETA: 12:29 - loss: 0.2248 - acc: 0.96 - ETA: 12:27 - loss: 0.2213 - acc: 0.96 - ETA: 12:24 - loss: 0.2157 - acc: 0.96 - ETA: 12:22 - loss: 0.2142 - acc: 0.96 - ETA: 12:20 - loss: 0.2281 - acc: 0.95 - ETA: 12:17 - loss: 0.2696 - acc: 0.95 - ETA: 12:14 - loss: 0.2642 - acc: 0.95 - ETA: 12:12 - loss: 0.2570 - acc: 0.95 - ETA: 12:09 - loss: 0.2500 - acc: 0.95 - ETA: 12:06 - loss: 0.2434 - acc: 0.95 - ETA: 12:04 - loss: 0.2378 - acc: 0.95 - ETA: 12:01 - loss: 0.2415 - acc: 0.95 - ETA: 11:59 - loss: 0.2444 - acc: 0.95 - ETA: 11:57 - loss: 0.2387 - acc: 0.95 - ETA: 11:54 - loss: 0.2351 - acc: 0.95 - ETA: 11:52 - loss: 0.2331 - acc: 0.95 - ETA: 11:50 - loss: 0.2283 - acc: 0.95 - ETA: 11:47 - loss: 0.2266 - acc: 0.95 - ETA: 11:45 - loss: 0.2223 - acc: 0.95 - ETA: 11:42 - loss: 0.2176 - acc: 0.95 - ETA: 11:39 - loss: 0.2164 - acc: 0.95 - ETA: 11:37 - loss: 0.2124 - acc: 0.96 - ETA: 11:34 - loss: 0.2186 - acc: 0.96 - ETA: 11:32 - loss: 0.2332 - acc: 0.95 - ETA: 11:29 - loss: 0.2302 - acc: 0.95 - ETA: 11:27 - loss: 0.2261 - acc: 0.95 - ETA: 11:25 - loss: 0.2246 - acc: 0.95 - ETA: 11:22 - loss: 0.2325 - acc: 0.95 - ETA: 11:19 - loss: 0.2288 - acc: 0.95 - ETA: 11:17 - loss: 0.2250 - acc: 0.95 - ETA: 11:14 - loss: 0.2212 - acc: 0.96 - ETA: 11:12 - loss: 0.2181 - acc: 0.96 - ETA: 11:09 - loss: 0.2218 - acc: 0.96 - ETA: 11:07 - loss: 0.2187 - acc: 0.96 - ETA: 11:04 - loss: 0.2172 - acc: 0.96 - ETA: 11:02 - loss: 0.2225 - acc: 0.96 - ETA: 10:59 - loss: 0.2194 - acc: 0.96 - ETA: 10:57 - loss: 0.2164 - acc: 0.96 - ETA: 10:54 - loss: 0.2187 - acc: 0.96 - ETA: 10:52 - loss: 0.2163 - acc: 0.96 - ETA: 10:49 - loss: 0.2134 - acc: 0.96 - ETA: 10:47 - loss: 0.2134 - acc: 0.96 - ETA: 10:45 - loss: 0.2116 - acc: 0.96 - ETA: 10:42 - loss: 0.2096 - acc: 0.96 - ETA: 10:40 - loss: 0.2118 - acc: 0.96 - ETA: 10:37 - loss: 0.2138 - acc: 0.96 - ETA: 10:35 - loss: 0.2132 - acc: 0.96 - ETA: 10:32 - loss: 0.2105 - acc: 0.96 - ETA: 10:30 - loss: 0.2080 - acc: 0.96 - ETA: 10:27 - loss: 0.2054 - acc: 0.96 - ETA: 10:25 - loss: 0.2056 - acc: 0.96 - ETA: 10:23 - loss: 0.2071 - acc: 0.96 - ETA: 10:20 - loss: 0.2050 - acc: 0.96 - ETA: 10:18 - loss: 0.2078 - acc: 0.96 - ETA: 10:15 - loss: 0.2053 - acc: 0.96 - ETA: 10:13 - loss: 0.2064 - acc: 0.96 - ETA: 10:10 - loss: 0.2047 - acc: 0.96 - ETA: 10:08 - loss: 0.2026 - acc: 0.96 - ETA: 10:05 - loss: 0.2003 - acc: 0.96 - ETA: 10:03 - loss: 0.1997 - acc: 0.96 - ETA: 10:00 - loss: 0.1977 - acc: 0.96 - ETA: 9:58 - loss: 0.1972 - acc: 0.9612 - ETA: 9:56 - loss: 0.1965 - acc: 0.961 - ETA: 9:53 - loss: 0.1974 - acc: 0.961 - ETA: 9:51 - loss: 0.1956 - acc: 0.961 - ETA: 9:48 - loss: 0.1968 - acc: 0.960 - ETA: 9:46 - loss: 0.1954 - acc: 0.961 - ETA: 9:43 - loss: 0.1954 - acc: 0.960 - ETA: 9:41 - loss: 0.1950 - acc: 0.960 - ETA: 9:38 - loss: 0.1932 - acc: 0.960 - ETA: 9:36 - loss: 0.1913 - acc: 0.961 - ETA: 9:34 - loss: 0.1894 - acc: 0.961 - ETA: 9:31 - loss: 0.1877 - acc: 0.962 - ETA: 9:29 - loss: 0.1860 - acc: 0.962 - ETA: 9:26 - loss: 0.1865 - acc: 0.962 - ETA: 9:24 - loss: 0.1856 - acc: 0.962 - ETA: 9:21 - loss: 0.1913 - acc: 0.961 - ETA: 9:19 - loss: 0.1902 - acc: 0.961 - ETA: 9:17 - loss: 0.1885 - acc: 0.962 - ETA: 9:14 - loss: 0.1900 - acc: 0.961 - ETA: 9:12 - loss: 0.1888 - acc: 0.962 - ETA: 9:09 - loss: 0.1882 - acc: 0.961 - ETA: 9:07 - loss: 0.1903 - acc: 0.961 - ETA: 9:04 - loss: 0.2002 - acc: 0.960 - ETA: 9:02 - loss: 0.1987 - acc: 0.960 - ETA: 8:59 - loss: 0.1982 - acc: 0.960 - ETA: 8:57 - loss: 0.1972 - acc: 0.960 - ETA: 8:55 - loss: 0.1955 - acc: 0.960 - ETA: 8:52 - loss: 0.1950 - acc: 0.960 - ETA: 8:50 - loss: 0.1935 - acc: 0.961 - ETA: 8:47 - loss: 0.1919 - acc: 0.961 - ETA: 8:45 - loss: 0.1932 - acc: 0.961 - ETA: 8:42 - loss: 0.1918 - acc: 0.961 - ETA: 8:40 - loss: 0.1922 - acc: 0.961 - ETA: 8:37 - loss: 0.1952 - acc: 0.960 - ETA: 8:35 - loss: 0.1948 - acc: 0.960 - ETA: 8:33 - loss: 0.1934 - acc: 0.960 - ETA: 8:30 - loss: 0.1920 - acc: 0.961 - ETA: 8:28 - loss: 0.1918 - acc: 0.961 - ETA: 8:25 - loss: 0.1907 - acc: 0.961 - ETA: 8:23 - loss: 0.1896 - acc: 0.961 - ETA: 8:20 - loss: 0.1906 - acc: 0.961 - ETA: 8:18 - loss: 0.1906 - acc: 0.961 - ETA: 8:16 - loss: 0.1982 - acc: 0.960 - ETA: 8:13 - loss: 0.1973 - acc: 0.960 - ETA: 8:11 - loss: 0.1978 - acc: 0.960 - ETA: 8:08 - loss: 0.1970 - acc: 0.960 - ETA: 8:06 - loss: 0.1960 - acc: 0.961 - ETA: 8:03 - loss: 0.1969 - acc: 0.961 - ETA: 8:01 - loss: 0.1956 - acc: 0.961 - ETA: 7:58 - loss: 0.1947 - acc: 0.961 - ETA: 7:56 - loss: 0.1990 - acc: 0.960 - ETA: 7:54 - loss: 0.1978 - acc: 0.961 - ETA: 7:51 - loss: 0.1970 - acc: 0.961 - ETA: 7:49 - loss: 0.1979 - acc: 0.960 - ETA: 7:46 - loss: 0.1966 - acc: 0.961 - ETA: 7:44 - loss: 0.1973 - acc: 0.960 - ETA: 7:41 - loss: 0.1976 - acc: 0.960 - ETA: 7:39 - loss: 0.1970 - acc: 0.961 - ETA: 7:36 - loss: 0.1963 - acc: 0.961 - ETA: 7:34 - loss: 0.1951 - acc: 0.961 - ETA: 7:31 - loss: 0.1948 - acc: 0.961 - ETA: 7:29 - loss: 0.1937 - acc: 0.962 - ETA: 7:26 - loss: 0.1925 - acc: 0.962 - ETA: 7:24 - loss: 0.1916 - acc: 0.962 - ETA: 7:22 - loss: 0.1924 - acc: 0.962 - ETA: 7:19 - loss: 0.1922 - acc: 0.962 - ETA: 7:17 - loss: 0.1931 - acc: 0.961 - ETA: 7:14 - loss: 0.1944 - acc: 0.961 - ETA: 7:12 - loss: 0.1935 - acc: 0.962 - ETA: 7:09 - loss: 0.1932 - acc: 0.962 - ETA: 7:07 - loss: 0.1926 - acc: 0.961 - ETA: 7:04 - loss: 0.1933 - acc: 0.961 - ETA: 7:02 - loss: 0.1942 - acc: 0.960 - ETA: 7:00 - loss: 0.1965 - acc: 0.960 - ETA: 6:57 - loss: 0.1955 - acc: 0.961 - ETA: 6:55 - loss: 0.1964 - acc: 0.961 - ETA: 6:52 - loss: 0.2053 - acc: 0.959 - ETA: 6:50 - loss: 0.2062 - acc: 0.959 - ETA: 6:47 - loss: 0.2057 - acc: 0.959 - ETA: 6:45 - loss: 0.2055 - acc: 0.959 - ETA: 6:43 - loss: 0.2074 - acc: 0.959 - ETA: 6:40 - loss: 0.2133 - acc: 0.958 - ETA: 6:38 - loss: 0.2142 - acc: 0.958 - ETA: 6:35 - loss: 0.2131 - acc: 0.959 - ETA: 6:33 - loss: 0.2123 - acc: 0.959 - ETA: 6:30 - loss: 0.2128 - acc: 0.958 - ETA: 6:28 - loss: 0.2118 - acc: 0.958 - ETA: 6:25 - loss: 0.2132 - acc: 0.958 - ETA: 6:23 - loss: 0.2124 - acc: 0.959 - ETA: 6:21 - loss: 0.2131 - acc: 0.958 - ETA: 6:18 - loss: 0.2130 - acc: 0.958 - ETA: 6:16 - loss: 0.2120 - acc: 0.958 - ETA: 6:13 - loss: 0.2113 - acc: 0.959 - ETA: 6:11 - loss: 0.2125 - acc: 0.959 - ETA: 6:08 - loss: 0.2132 - acc: 0.958 - ETA: 6:06 - loss: 0.2189 - acc: 0.958 - ETA: 6:04 - loss: 0.2190 - acc: 0.958 - ETA: 6:01 - loss: 0.2191 - acc: 0.958 - ETA: 5:59 - loss: 0.2180 - acc: 0.958 - ETA: 5:56 - loss: 0.2173 - acc: 0.958 - ETA: 5:54 - loss: 0.2162 - acc: 0.958 - ETA: 5:51 - loss: 0.2154 - acc: 0.958 - ETA: 5:49 - loss: 0.2175 - acc: 0.957 - ETA: 5:46 - loss: 0.2180 - acc: 0.957 - ETA: 5:44 - loss: 0.2180 - acc: 0.957 - ETA: 5:41 - loss: 0.2169 - acc: 0.958 - ETA: 5:39 - loss: 0.2160 - acc: 0.958 - ETA: 5:37 - loss: 0.2153 - acc: 0.958 - ETA: 5:34 - loss: 0.2149 - acc: 0.958 - ETA: 5:32 - loss: 0.2146 - acc: 0.957 - ETA: 5:29 - loss: 0.2152 - acc: 0.957 - ETA: 5:27 - loss: 0.2173 - acc: 0.957 - ETA: 5:24 - loss: 0.2170 - acc: 0.957 - ETA: 5:22 - loss: 0.2164 - acc: 0.957 - ETA: 5:19 - loss: 0.2218 - acc: 0.957 - ETA: 5:17 - loss: 0.2213 - acc: 0.9574"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5:15 - loss: 0.2207 - acc: 0.957 - ETA: 5:12 - loss: 0.2197 - acc: 0.957 - ETA: 5:10 - loss: 0.2190 - acc: 0.957 - ETA: 5:07 - loss: 0.2181 - acc: 0.957 - ETA: 5:05 - loss: 0.2172 - acc: 0.958 - ETA: 5:02 - loss: 0.2171 - acc: 0.958 - ETA: 5:00 - loss: 0.2166 - acc: 0.958 - ETA: 4:58 - loss: 0.2187 - acc: 0.957 - ETA: 4:55 - loss: 0.2180 - acc: 0.958 - ETA: 4:53 - loss: 0.2172 - acc: 0.958 - ETA: 4:50 - loss: 0.2165 - acc: 0.958 - ETA: 4:48 - loss: 0.2155 - acc: 0.958 - ETA: 4:45 - loss: 0.2173 - acc: 0.958 - ETA: 4:43 - loss: 0.2211 - acc: 0.957 - ETA: 4:40 - loss: 0.2206 - acc: 0.957 - ETA: 4:38 - loss: 0.2225 - acc: 0.957 - ETA: 4:35 - loss: 0.2226 - acc: 0.957 - ETA: 4:33 - loss: 0.2234 - acc: 0.956 - ETA: 4:31 - loss: 0.2241 - acc: 0.956 - ETA: 4:28 - loss: 0.2245 - acc: 0.956 - ETA: 4:26 - loss: 0.2235 - acc: 0.956 - ETA: 4:23 - loss: 0.2232 - acc: 0.956 - ETA: 4:21 - loss: 0.2223 - acc: 0.956 - ETA: 4:18 - loss: 0.2214 - acc: 0.956 - ETA: 4:16 - loss: 0.2205 - acc: 0.956 - ETA: 4:14 - loss: 0.2220 - acc: 0.956 - ETA: 4:11 - loss: 0.2213 - acc: 0.956 - ETA: 4:09 - loss: 0.2205 - acc: 0.956 - ETA: 4:06 - loss: 0.2213 - acc: 0.956 - ETA: 4:04 - loss: 0.2215 - acc: 0.956 - ETA: 4:01 - loss: 0.2209 - acc: 0.956 - ETA: 3:59 - loss: 0.2211 - acc: 0.956 - ETA: 3:56 - loss: 0.2203 - acc: 0.956 - ETA: 3:54 - loss: 0.2197 - acc: 0.956 - ETA: 3:52 - loss: 0.2209 - acc: 0.956 - ETA: 3:49 - loss: 0.2200 - acc: 0.956 - ETA: 3:47 - loss: 0.2193 - acc: 0.956 - ETA: 3:44 - loss: 0.2185 - acc: 0.957 - ETA: 3:42 - loss: 0.2196 - acc: 0.956 - ETA: 3:39 - loss: 0.2191 - acc: 0.956 - ETA: 3:37 - loss: 0.2188 - acc: 0.956 - ETA: 3:34 - loss: 0.2231 - acc: 0.955 - ETA: 3:32 - loss: 0.2346 - acc: 0.954 - ETA: 3:30 - loss: 0.2380 - acc: 0.954 - ETA: 3:27 - loss: 0.2391 - acc: 0.954 - ETA: 3:25 - loss: 0.2389 - acc: 0.954 - ETA: 3:22 - loss: 0.2383 - acc: 0.954 - ETA: 3:20 - loss: 0.2374 - acc: 0.954 - ETA: 3:17 - loss: 0.2368 - acc: 0.954 - ETA: 3:15 - loss: 0.2391 - acc: 0.954 - ETA: 3:12 - loss: 0.2385 - acc: 0.954 - ETA: 3:10 - loss: 0.2384 - acc: 0.954 - ETA: 3:08 - loss: 0.2377 - acc: 0.954 - ETA: 3:05 - loss: 0.2406 - acc: 0.954 - ETA: 3:03 - loss: 0.2510 - acc: 0.952 - ETA: 3:00 - loss: 0.2510 - acc: 0.952 - ETA: 2:58 - loss: 0.2500 - acc: 0.952 - ETA: 2:55 - loss: 0.2491 - acc: 0.952 - ETA: 2:53 - loss: 0.2491 - acc: 0.952 - ETA: 2:51 - loss: 0.2491 - acc: 0.952 - ETA: 2:48 - loss: 0.2482 - acc: 0.952 - ETA: 2:46 - loss: 0.2478 - acc: 0.953 - ETA: 2:43 - loss: 0.2470 - acc: 0.953 - ETA: 2:41 - loss: 0.2480 - acc: 0.953 - ETA: 2:38 - loss: 0.2474 - acc: 0.953 - ETA: 2:36 - loss: 0.2467 - acc: 0.953 - ETA: 2:33 - loss: 0.2488 - acc: 0.953 - ETA: 2:31 - loss: 0.2480 - acc: 0.953 - ETA: 2:29 - loss: 0.2472 - acc: 0.953 - ETA: 2:26 - loss: 0.2465 - acc: 0.953 - ETA: 2:24 - loss: 0.2461 - acc: 0.953 - ETA: 2:21 - loss: 0.2458 - acc: 0.953 - ETA: 2:19 - loss: 0.2465 - acc: 0.953 - ETA: 2:16 - loss: 0.2469 - acc: 0.953 - ETA: 2:14 - loss: 0.2493 - acc: 0.952 - ETA: 2:11 - loss: 0.2490 - acc: 0.952 - ETA: 2:09 - loss: 0.2482 - acc: 0.953 - ETA: 2:07 - loss: 0.2482 - acc: 0.953 - ETA: 2:04 - loss: 0.2481 - acc: 0.953 - ETA: 2:02 - loss: 0.2473 - acc: 0.953 - ETA: 1:59 - loss: 0.2468 - acc: 0.953 - ETA: 1:57 - loss: 0.2459 - acc: 0.953 - ETA: 1:54 - loss: 0.2466 - acc: 0.953 - ETA: 1:52 - loss: 0.2459 - acc: 0.953 - ETA: 1:49 - loss: 0.2457 - acc: 0.953 - ETA: 1:47 - loss: 0.2469 - acc: 0.953 - ETA: 1:45 - loss: 0.2461 - acc: 0.953 - ETA: 1:42 - loss: 0.2465 - acc: 0.953 - ETA: 1:40 - loss: 0.2495 - acc: 0.952 - ETA: 1:37 - loss: 0.2516 - acc: 0.952 - ETA: 1:35 - loss: 0.2532 - acc: 0.952 - ETA: 1:32 - loss: 0.2526 - acc: 0.952 - ETA: 1:30 - loss: 0.2523 - acc: 0.952 - ETA: 1:27 - loss: 0.2514 - acc: 0.952 - ETA: 1:25 - loss: 0.2508 - acc: 0.952 - ETA: 1:23 - loss: 0.2518 - acc: 0.952 - ETA: 1:20 - loss: 0.2535 - acc: 0.952 - ETA: 1:18 - loss: 0.2532 - acc: 0.952 - ETA: 1:15 - loss: 0.2535 - acc: 0.952 - ETA: 1:13 - loss: 0.2562 - acc: 0.951 - ETA: 1:10 - loss: 0.2563 - acc: 0.951 - ETA: 1:08 - loss: 0.2566 - acc: 0.951 - ETA: 1:05 - loss: 0.2559 - acc: 0.951 - ETA: 1:03 - loss: 0.2554 - acc: 0.951 - ETA: 1:01 - loss: 0.2551 - acc: 0.951 - ETA: 58s - loss: 0.2549 - acc: 0.951 - ETA: 56s - loss: 0.2543 - acc: 0.95 - ETA: 53s - loss: 0.2553 - acc: 0.95 - ETA: 51s - loss: 0.2548 - acc: 0.95 - ETA: 48s - loss: 0.2544 - acc: 0.95 - ETA: 46s - loss: 0.2562 - acc: 0.95 - ETA: 43s - loss: 0.2557 - acc: 0.95 - ETA: 41s - loss: 0.2644 - acc: 0.94 - ETA: 39s - loss: 0.2639 - acc: 0.94 - ETA: 36s - loss: 0.2640 - acc: 0.94 - ETA: 34s - loss: 0.2635 - acc: 0.94 - ETA: 31s - loss: 0.2628 - acc: 0.95 - ETA: 29s - loss: 0.2622 - acc: 0.95 - ETA: 26s - loss: 0.2622 - acc: 0.95 - ETA: 24s - loss: 0.2623 - acc: 0.95 - ETA: 21s - loss: 0.2717 - acc: 0.94 - ETA: 19s - loss: 0.2718 - acc: 0.94 - ETA: 17s - loss: 0.2709 - acc: 0.94 - ETA: 14s - loss: 0.2707 - acc: 0.94 - ETA: 12s - loss: 0.2720 - acc: 0.94 - ETA: 9s - loss: 0.2727 - acc: 0.9483 - ETA: 7s - loss: 0.2722 - acc: 0.948 - ETA: 4s - loss: 0.2718 - acc: 0.948 - ETA: 2s - loss: 0.2710 - acc: 0.948 - 842s 126ms/step - loss: 0.2728 - acc: 0.9485 - val_loss: 10.1011 - val_acc: 0.0383\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/10\n",
      "4080/6680 [=================>............] - ETA: 13:30 - loss: 0.2696 - acc: 0.95 - ETA: 13:25 - loss: 0.2907 - acc: 0.95 - ETA: 13:28 - loss: 0.1944 - acc: 0.96 - ETA: 13:22 - loss: 0.1460 - acc: 0.97 - ETA: 13:20 - loss: 0.1563 - acc: 0.97 - ETA: 13:21 - loss: 0.1322 - acc: 0.97 - ETA: 13:18 - loss: 0.1144 - acc: 0.97 - ETA: 13:16 - loss: 0.1295 - acc: 0.96 - ETA: 13:14 - loss: 0.1155 - acc: 0.97 - ETA: 13:11 - loss: 0.1215 - acc: 0.97 - ETA: 13:09 - loss: 0.1148 - acc: 0.97 - ETA: 13:08 - loss: 0.1119 - acc: 0.97 - ETA: 13:07 - loss: 0.1046 - acc: 0.97 - ETA: 13:03 - loss: 0.1161 - acc: 0.97 - ETA: 13:01 - loss: 0.1533 - acc: 0.97 - ETA: 12:58 - loss: 0.1447 - acc: 0.97 - ETA: 12:56 - loss: 0.1408 - acc: 0.97 - ETA: 12:54 - loss: 0.1408 - acc: 0.97 - ETA: 12:52 - loss: 0.1336 - acc: 0.97 - ETA: 12:49 - loss: 0.1274 - acc: 0.97 - ETA: 12:46 - loss: 0.1225 - acc: 0.97 - ETA: 12:43 - loss: 0.1453 - acc: 0.97 - ETA: 12:41 - loss: 0.1428 - acc: 0.97 - ETA: 12:38 - loss: 0.1371 - acc: 0.97 - ETA: 12:36 - loss: 0.1491 - acc: 0.97 - ETA: 12:33 - loss: 0.1449 - acc: 0.97 - ETA: 12:31 - loss: 0.1404 - acc: 0.97 - ETA: 12:28 - loss: 0.1378 - acc: 0.97 - ETA: 12:26 - loss: 0.1354 - acc: 0.97 - ETA: 12:23 - loss: 0.1310 - acc: 0.97 - ETA: 12:22 - loss: 0.1278 - acc: 0.97 - ETA: 12:20 - loss: 0.1243 - acc: 0.97 - ETA: 12:17 - loss: 0.1208 - acc: 0.97 - ETA: 12:15 - loss: 0.1173 - acc: 0.97 - ETA: 12:13 - loss: 0.1140 - acc: 0.98 - ETA: 12:10 - loss: 0.1117 - acc: 0.98 - ETA: 12:08 - loss: 0.1108 - acc: 0.97 - ETA: 12:05 - loss: 0.1089 - acc: 0.98 - ETA: 12:03 - loss: 0.1064 - acc: 0.98 - ETA: 12:00 - loss: 0.1041 - acc: 0.98 - ETA: 11:57 - loss: 0.1024 - acc: 0.98 - ETA: 11:55 - loss: 0.1002 - acc: 0.98 - ETA: 11:52 - loss: 0.0979 - acc: 0.98 - ETA: 11:49 - loss: 0.0959 - acc: 0.98 - ETA: 11:47 - loss: 0.0939 - acc: 0.98 - ETA: 11:45 - loss: 0.0919 - acc: 0.98 - ETA: 11:42 - loss: 0.0907 - acc: 0.98 - ETA: 11:40 - loss: 0.0889 - acc: 0.98 - ETA: 11:37 - loss: 0.0895 - acc: 0.98 - ETA: 11:34 - loss: 0.0953 - acc: 0.98 - ETA: 11:32 - loss: 0.1005 - acc: 0.98 - ETA: 11:29 - loss: 0.0987 - acc: 0.98 - ETA: 11:26 - loss: 0.1036 - acc: 0.98 - ETA: 11:24 - loss: 0.1224 - acc: 0.97 - ETA: 11:21 - loss: 0.1205 - acc: 0.97 - ETA: 11:19 - loss: 0.1184 - acc: 0.97 - ETA: 11:17 - loss: 0.1163 - acc: 0.97 - ETA: 11:14 - loss: 0.1143 - acc: 0.97 - ETA: 11:12 - loss: 0.1230 - acc: 0.97 - ETA: 11:10 - loss: 0.1210 - acc: 0.97 - ETA: 11:07 - loss: 0.1253 - acc: 0.97 - ETA: 11:05 - loss: 0.1248 - acc: 0.97 - ETA: 11:02 - loss: 0.1254 - acc: 0.97 - ETA: 11:00 - loss: 0.1235 - acc: 0.97 - ETA: 10:57 - loss: 0.1250 - acc: 0.97 - ETA: 10:55 - loss: 0.1320 - acc: 0.97 - ETA: 10:53 - loss: 0.1507 - acc: 0.97 - ETA: 10:50 - loss: 0.1599 - acc: 0.97 - ETA: 10:48 - loss: 0.1588 - acc: 0.97 - ETA: 10:45 - loss: 0.1573 - acc: 0.97 - ETA: 10:43 - loss: 0.1553 - acc: 0.97 - ETA: 10:40 - loss: 0.1533 - acc: 0.97 - ETA: 10:38 - loss: 0.1537 - acc: 0.97 - ETA: 10:35 - loss: 0.1577 - acc: 0.97 - ETA: 10:33 - loss: 0.1557 - acc: 0.97 - ETA: 10:30 - loss: 0.1543 - acc: 0.97 - ETA: 10:28 - loss: 0.1545 - acc: 0.97 - ETA: 10:25 - loss: 0.1560 - acc: 0.97 - ETA: 10:23 - loss: 0.1571 - acc: 0.97 - ETA: 10:21 - loss: 0.1553 - acc: 0.97 - ETA: 10:18 - loss: 0.1534 - acc: 0.97 - ETA: 10:16 - loss: 0.1522 - acc: 0.97 - ETA: 10:13 - loss: 0.1607 - acc: 0.97 - ETA: 10:11 - loss: 0.1614 - acc: 0.97 - ETA: 10:09 - loss: 0.1600 - acc: 0.97 - ETA: 10:06 - loss: 0.1607 - acc: 0.97 - ETA: 10:04 - loss: 0.1589 - acc: 0.97 - ETA: 10:01 - loss: 0.1571 - acc: 0.97 - ETA: 9:59 - loss: 0.1563 - acc: 0.9708 - ETA: 9:56 - loss: 0.1566 - acc: 0.970 - ETA: 9:54 - loss: 0.1599 - acc: 0.969 - ETA: 9:51 - loss: 0.1582 - acc: 0.970 - ETA: 9:49 - loss: 0.1654 - acc: 0.969 - ETA: 9:46 - loss: 0.1638 - acc: 0.970 - ETA: 9:44 - loss: 0.1624 - acc: 0.970 - ETA: 9:42 - loss: 0.1610 - acc: 0.970 - ETA: 9:39 - loss: 0.1594 - acc: 0.971 - ETA: 9:37 - loss: 0.1642 - acc: 0.970 - ETA: 9:34 - loss: 0.1652 - acc: 0.970 - ETA: 9:32 - loss: 0.1683 - acc: 0.969 - ETA: 9:29 - loss: 0.1667 - acc: 0.969 - ETA: 9:27 - loss: 0.1651 - acc: 0.970 - ETA: 9:24 - loss: 0.1656 - acc: 0.969 - ETA: 9:22 - loss: 0.1646 - acc: 0.970 - ETA: 9:19 - loss: 0.1630 - acc: 0.970 - ETA: 9:17 - loss: 0.1616 - acc: 0.970 - ETA: 9:14 - loss: 0.1602 - acc: 0.971 - ETA: 9:12 - loss: 0.1610 - acc: 0.969 - ETA: 9:10 - loss: 0.1615 - acc: 0.969 - ETA: 9:07 - loss: 0.1610 - acc: 0.969 - ETA: 9:05 - loss: 0.1596 - acc: 0.969 - ETA: 9:02 - loss: 0.1584 - acc: 0.970 - ETA: 9:00 - loss: 0.1570 - acc: 0.970 - ETA: 8:58 - loss: 0.1568 - acc: 0.970 - ETA: 8:55 - loss: 0.1561 - acc: 0.970 - ETA: 8:53 - loss: 0.1558 - acc: 0.969 - ETA: 8:50 - loss: 0.1908 - acc: 0.965 - ETA: 8:48 - loss: 0.1997 - acc: 0.964 - ETA: 8:45 - loss: 0.2002 - acc: 0.964 - ETA: 8:43 - loss: 0.1993 - acc: 0.964 - ETA: 8:40 - loss: 0.2149 - acc: 0.962 - ETA: 8:38 - loss: 0.2159 - acc: 0.961 - ETA: 8:36 - loss: 0.2201 - acc: 0.961 - ETA: 8:33 - loss: 0.2458 - acc: 0.958 - ETA: 8:31 - loss: 0.2441 - acc: 0.959 - ETA: 8:28 - loss: 0.2428 - acc: 0.959 - ETA: 8:26 - loss: 0.2415 - acc: 0.959 - ETA: 8:23 - loss: 0.2400 - acc: 0.959 - ETA: 8:21 - loss: 0.2397 - acc: 0.958 - ETA: 8:18 - loss: 0.2394 - acc: 0.958 - ETA: 8:16 - loss: 0.2437 - acc: 0.958 - ETA: 8:14 - loss: 0.2455 - acc: 0.958 - ETA: 8:11 - loss: 0.2844 - acc: 0.954 - ETA: 8:09 - loss: 0.2869 - acc: 0.953 - ETA: 8:07 - loss: 0.2945 - acc: 0.953 - ETA: 8:04 - loss: 0.2924 - acc: 0.953 - ETA: 8:02 - loss: 0.2985 - acc: 0.952 - ETA: 7:59 - loss: 0.2964 - acc: 0.952 - ETA: 7:57 - loss: 0.2943 - acc: 0.952 - ETA: 7:54 - loss: 0.2943 - acc: 0.952 - ETA: 7:52 - loss: 0.2938 - acc: 0.952 - ETA: 7:50 - loss: 0.2929 - acc: 0.952 - ETA: 7:47 - loss: 0.2938 - acc: 0.952 - ETA: 7:45 - loss: 0.2940 - acc: 0.952 - ETA: 7:42 - loss: 0.2922 - acc: 0.952 - ETA: 7:40 - loss: 0.2916 - acc: 0.952 - ETA: 7:37 - loss: 0.2897 - acc: 0.953 - ETA: 7:35 - loss: 0.2880 - acc: 0.953 - ETA: 7:32 - loss: 0.2862 - acc: 0.953 - ETA: 7:30 - loss: 0.2844 - acc: 0.954 - ETA: 7:27 - loss: 0.2830 - acc: 0.954 - ETA: 7:25 - loss: 0.2812 - acc: 0.954 - ETA: 7:23 - loss: 0.2800 - acc: 0.954 - ETA: 7:20 - loss: 0.2805 - acc: 0.954 - ETA: 7:18 - loss: 0.2795 - acc: 0.954 - ETA: 7:15 - loss: 0.2793 - acc: 0.954 - ETA: 7:13 - loss: 0.2775 - acc: 0.954 - ETA: 7:10 - loss: 0.2758 - acc: 0.955 - ETA: 7:08 - loss: 0.2749 - acc: 0.955 - ETA: 7:05 - loss: 0.2761 - acc: 0.955 - ETA: 7:03 - loss: 0.2752 - acc: 0.955 - ETA: 7:00 - loss: 0.2735 - acc: 0.955 - ETA: 6:58 - loss: 0.2758 - acc: 0.954 - ETA: 6:55 - loss: 0.2741 - acc: 0.954 - ETA: 6:53 - loss: 0.2730 - acc: 0.955 - ETA: 6:51 - loss: 0.2717 - acc: 0.955 - ETA: 6:48 - loss: 0.2705 - acc: 0.955 - ETA: 6:46 - loss: 0.2699 - acc: 0.955 - ETA: 6:43 - loss: 0.2686 - acc: 0.955 - ETA: 6:41 - loss: 0.2673 - acc: 0.955 - ETA: 6:38 - loss: 0.2670 - acc: 0.955 - ETA: 6:36 - loss: 0.2658 - acc: 0.955 - ETA: 6:33 - loss: 0.2656 - acc: 0.955 - ETA: 6:31 - loss: 0.2652 - acc: 0.955 - ETA: 6:29 - loss: 0.2643 - acc: 0.955 - ETA: 6:26 - loss: 0.2639 - acc: 0.955 - ETA: 6:24 - loss: 0.2629 - acc: 0.955 - ETA: 6:21 - loss: 0.2615 - acc: 0.955 - ETA: 6:19 - loss: 0.2606 - acc: 0.955 - ETA: 6:16 - loss: 0.2592 - acc: 0.955 - ETA: 6:14 - loss: 0.2586 - acc: 0.955 - ETA: 6:11 - loss: 0.2574 - acc: 0.956 - ETA: 6:09 - loss: 0.2562 - acc: 0.956 - ETA: 6:07 - loss: 0.2554 - acc: 0.956 - ETA: 6:04 - loss: 0.2547 - acc: 0.956 - ETA: 6:02 - loss: 0.2548 - acc: 0.956 - ETA: 5:59 - loss: 0.2535 - acc: 0.956 - ETA: 5:57 - loss: 0.2523 - acc: 0.956 - ETA: 5:54 - loss: 0.2528 - acc: 0.956 - ETA: 5:52 - loss: 0.2516 - acc: 0.956 - ETA: 5:49 - loss: 0.2522 - acc: 0.956 - ETA: 5:47 - loss: 0.2525 - acc: 0.956 - ETA: 5:44 - loss: 0.2516 - acc: 0.956 - ETA: 5:42 - loss: 0.2504 - acc: 0.956 - ETA: 5:40 - loss: 0.2500 - acc: 0.956 - ETA: 5:37 - loss: 0.2538 - acc: 0.955 - ETA: 5:35 - loss: 0.2597 - acc: 0.953 - ETA: 5:32 - loss: 0.2606 - acc: 0.953 - ETA: 5:30 - loss: 0.2599 - acc: 0.953 - ETA: 5:27 - loss: 0.2592 - acc: 0.953 - ETA: 5:25 - loss: 0.2582 - acc: 0.953 - ETA: 5:22 - loss: 0.2571 - acc: 0.953 - ETA: 5:20 - loss: 0.2560 - acc: 0.953 - ETA: 5:18 - loss: 0.2563 - acc: 0.95396680/6680 [==============================] - ETA: 5:15 - loss: 0.2551 - acc: 0.954 - ETA: 5:13 - loss: 0.2547 - acc: 0.953 - ETA: 5:10 - loss: 0.2550 - acc: 0.953 - ETA: 5:08 - loss: 0.2544 - acc: 0.953 - ETA: 5:05 - loss: 0.2533 - acc: 0.953 - ETA: 5:03 - loss: 0.2522 - acc: 0.954 - ETA: 5:00 - loss: 0.2546 - acc: 0.953 - ETA: 4:58 - loss: 0.2534 - acc: 0.953 - ETA: 4:55 - loss: 0.2525 - acc: 0.953 - ETA: 4:53 - loss: 0.2517 - acc: 0.954 - ETA: 4:51 - loss: 0.2505 - acc: 0.954 - ETA: 4:48 - loss: 0.2495 - acc: 0.954 - ETA: 4:46 - loss: 0.2510 - acc: 0.954 - ETA: 4:43 - loss: 0.2499 - acc: 0.954 - ETA: 4:41 - loss: 0.2488 - acc: 0.954 - ETA: 4:38 - loss: 0.2477 - acc: 0.954 - ETA: 4:36 - loss: 0.2466 - acc: 0.955 - ETA: 4:33 - loss: 0.2459 - acc: 0.955 - ETA: 4:31 - loss: 0.2449 - acc: 0.955 - ETA: 4:29 - loss: 0.2439 - acc: 0.955 - ETA: 4:26 - loss: 0.2433 - acc: 0.955 - ETA: 4:24 - loss: 0.2428 - acc: 0.955 - ETA: 4:21 - loss: 0.2420 - acc: 0.955 - ETA: 4:19 - loss: 0.2436 - acc: 0.955 - ETA: 4:16 - loss: 0.2430 - acc: 0.955 - ETA: 4:14 - loss: 0.2435 - acc: 0.955 - ETA: 4:11 - loss: 0.2430 - acc: 0.955 - ETA: 4:09 - loss: 0.2457 - acc: 0.954 - ETA: 4:07 - loss: 0.2447 - acc: 0.954 - ETA: 4:04 - loss: 0.2441 - acc: 0.954 - ETA: 4:02 - loss: 0.2434 - acc: 0.955 - ETA: 3:59 - loss: 0.2425 - acc: 0.955 - ETA: 3:57 - loss: 0.2429 - acc: 0.955 - ETA: 3:54 - loss: 0.2475 - acc: 0.954 - ETA: 3:52 - loss: 0.2465 - acc: 0.954 - ETA: 3:49 - loss: 0.2513 - acc: 0.954 - ETA: 3:47 - loss: 0.2544 - acc: 0.953 - ETA: 3:45 - loss: 0.2535 - acc: 0.953 - ETA: 3:42 - loss: 0.2525 - acc: 0.953 - ETA: 3:40 - loss: 0.2521 - acc: 0.953 - ETA: 3:37 - loss: 0.2527 - acc: 0.953 - ETA: 3:35 - loss: 0.2533 - acc: 0.953 - ETA: 3:32 - loss: 0.2524 - acc: 0.953 - ETA: 3:30 - loss: 0.2515 - acc: 0.954 - ETA: 3:27 - loss: 0.2538 - acc: 0.953 - ETA: 3:25 - loss: 0.2542 - acc: 0.953 - ETA: 3:23 - loss: 0.2534 - acc: 0.953 - ETA: 3:20 - loss: 0.2535 - acc: 0.953 - ETA: 3:18 - loss: 0.2532 - acc: 0.953 - ETA: 3:15 - loss: 0.2540 - acc: 0.953 - ETA: 3:13 - loss: 0.2530 - acc: 0.953 - ETA: 3:10 - loss: 0.2521 - acc: 0.953 - ETA: 3:08 - loss: 0.2513 - acc: 0.953 - ETA: 3:05 - loss: 0.2505 - acc: 0.953 - ETA: 3:03 - loss: 0.2500 - acc: 0.953 - ETA: 3:01 - loss: 0.2499 - acc: 0.953 - ETA: 2:58 - loss: 0.2496 - acc: 0.953 - ETA: 2:56 - loss: 0.2487 - acc: 0.953 - ETA: 2:53 - loss: 0.2478 - acc: 0.954 - ETA: 2:51 - loss: 0.2469 - acc: 0.954 - ETA: 2:48 - loss: 0.2463 - acc: 0.954 - ETA: 2:46 - loss: 0.2454 - acc: 0.954 - ETA: 2:43 - loss: 0.2445 - acc: 0.954 - ETA: 2:41 - loss: 0.2460 - acc: 0.954 - ETA: 2:39 - loss: 0.2472 - acc: 0.953 - ETA: 2:36 - loss: 0.2468 - acc: 0.953 - ETA: 2:34 - loss: 0.2459 - acc: 0.954 - ETA: 2:31 - loss: 0.2451 - acc: 0.954 - ETA: 2:29 - loss: 0.2442 - acc: 0.954 - ETA: 2:26 - loss: 0.2438 - acc: 0.954 - ETA: 2:24 - loss: 0.2431 - acc: 0.954 - ETA: 2:21 - loss: 0.2425 - acc: 0.954 - ETA: 2:19 - loss: 0.2417 - acc: 0.954 - ETA: 2:16 - loss: 0.2409 - acc: 0.954 - ETA: 2:14 - loss: 0.2416 - acc: 0.954 - ETA: 2:12 - loss: 0.2469 - acc: 0.953 - ETA: 2:09 - loss: 0.2466 - acc: 0.953 - ETA: 2:07 - loss: 0.2459 - acc: 0.953 - ETA: 2:04 - loss: 0.2450 - acc: 0.953 - ETA: 2:02 - loss: 0.2444 - acc: 0.953 - ETA: 1:59 - loss: 0.2440 - acc: 0.953 - ETA: 1:57 - loss: 0.2433 - acc: 0.954 - ETA: 1:54 - loss: 0.2443 - acc: 0.953 - ETA: 1:52 - loss: 0.2435 - acc: 0.954 - ETA: 1:50 - loss: 0.2428 - acc: 0.954 - ETA: 1:47 - loss: 0.2423 - acc: 0.954 - ETA: 1:45 - loss: 0.2426 - acc: 0.954 - ETA: 1:42 - loss: 0.2439 - acc: 0.953 - ETA: 1:40 - loss: 0.2435 - acc: 0.953 - ETA: 1:37 - loss: 0.2428 - acc: 0.953 - ETA: 1:35 - loss: 0.2420 - acc: 0.954 - ETA: 1:32 - loss: 0.2427 - acc: 0.953 - ETA: 1:30 - loss: 0.2421 - acc: 0.954 - ETA: 1:28 - loss: 0.2413 - acc: 0.954 - ETA: 1:25 - loss: 0.2405 - acc: 0.954 - ETA: 1:23 - loss: 0.2403 - acc: 0.954 - ETA: 1:20 - loss: 0.2395 - acc: 0.954 - ETA: 1:18 - loss: 0.2392 - acc: 0.954 - ETA: 1:15 - loss: 0.2397 - acc: 0.954 - ETA: 1:13 - loss: 0.2401 - acc: 0.954 - ETA: 1:10 - loss: 0.2421 - acc: 0.954 - ETA: 1:08 - loss: 0.2413 - acc: 0.954 - ETA: 1:06 - loss: 0.2407 - acc: 0.954 - ETA: 1:03 - loss: 0.2417 - acc: 0.954 - ETA: 1:01 - loss: 0.2413 - acc: 0.954 - ETA: 58s - loss: 0.2415 - acc: 0.954 - ETA: 56s - loss: 0.2414 - acc: 0.95 - ETA: 53s - loss: 0.2408 - acc: 0.95 - ETA: 51s - loss: 0.2401 - acc: 0.95 - ETA: 48s - loss: 0.2394 - acc: 0.95 - ETA: 46s - loss: 0.2406 - acc: 0.95 - ETA: 44s - loss: 0.2401 - acc: 0.95 - ETA: 41s - loss: 0.2394 - acc: 0.95 - ETA: 39s - loss: 0.2389 - acc: 0.95 - ETA: 36s - loss: 0.2385 - acc: 0.95 - ETA: 34s - loss: 0.2380 - acc: 0.95 - ETA: 31s - loss: 0.2388 - acc: 0.95 - ETA: 29s - loss: 0.2382 - acc: 0.95 - ETA: 26s - loss: 0.2377 - acc: 0.95 - ETA: 24s - loss: 0.2378 - acc: 0.95 - ETA: 22s - loss: 0.2374 - acc: 0.95 - ETA: 19s - loss: 0.2369 - acc: 0.95 - ETA: 17s - loss: 0.2362 - acc: 0.95 - ETA: 14s - loss: 0.2371 - acc: 0.95 - ETA: 12s - loss: 0.2366 - acc: 0.95 - ETA: 9s - loss: 0.2360 - acc: 0.9555 - ETA: 7s - loss: 0.2353 - acc: 0.955 - ETA: 4s - loss: 0.2347 - acc: 0.955 - ETA: 2s - loss: 0.2356 - acc: 0.955 - 843s 126ms/step - loss: 0.2360 - acc: 0.9549 - val_loss: 11.2877 - val_acc: 0.0407\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 13:45 - loss: 0.1670 - acc: 0.95 - ETA: 13:37 - loss: 0.1247 - acc: 0.97 - ETA: 13:32 - loss: 0.0836 - acc: 0.98 - ETA: 13:29 - loss: 0.0631 - acc: 0.98 - ETA: 13:28 - loss: 0.0506 - acc: 0.99 - ETA: 13:23 - loss: 0.0448 - acc: 0.99 - ETA: 13:17 - loss: 0.0384 - acc: 0.99 - ETA: 13:15 - loss: 0.0337 - acc: 0.99 - ETA: 13:12 - loss: 0.0312 - acc: 0.99 - ETA: 13:11 - loss: 0.0894 - acc: 0.98 - ETA: 13:08 - loss: 0.1048 - acc: 0.97 - ETA: 13:06 - loss: 0.1135 - acc: 0.97 - ETA: 13:03 - loss: 0.1505 - acc: 0.96 - ETA: 13:02 - loss: 0.1614 - acc: 0.96 - ETA: 13:01 - loss: 0.1557 - acc: 0.96 - ETA: 12:58 - loss: 0.1725 - acc: 0.96 - ETA: 12:55 - loss: 0.1633 - acc: 0.96 - ETA: 12:52 - loss: 0.1545 - acc: 0.96 - ETA: 12:50 - loss: 0.1464 - acc: 0.96 - ETA: 12:47 - loss: 0.1467 - acc: 0.96 - ETA: 12:45 - loss: 0.1475 - acc: 0.96 - ETA: 12:42 - loss: 0.1472 - acc: 0.96 - ETA: 12:40 - loss: 0.1432 - acc: 0.96 - ETA: 12:37 - loss: 0.1380 - acc: 0.96 - ETA: 12:34 - loss: 0.1325 - acc: 0.96 - ETA: 12:32 - loss: 0.1288 - acc: 0.96 - ETA: 12:29 - loss: 0.1241 - acc: 0.97 - ETA: 12:27 - loss: 0.1226 - acc: 0.96 - ETA: 12:25 - loss: 0.1229 - acc: 0.96 - ETA: 12:23 - loss: 0.1199 - acc: 0.97 - ETA: 12:21 - loss: 0.1160 - acc: 0.97 - ETA: 12:18 - loss: 0.1133 - acc: 0.97 - ETA: 12:15 - loss: 0.1100 - acc: 0.97 - ETA: 12:13 - loss: 0.1072 - acc: 0.97 - ETA: 12:11 - loss: 0.1048 - acc: 0.97 - ETA: 12:08 - loss: 0.1056 - acc: 0.97 - ETA: 12:06 - loss: 0.1027 - acc: 0.97 - ETA: 12:05 - loss: 0.1000 - acc: 0.97 - ETA: 12:03 - loss: 0.1004 - acc: 0.97 - ETA: 12:02 - loss: 0.0979 - acc: 0.97 - ETA: 11:59 - loss: 0.0961 - acc: 0.97 - ETA: 11:57 - loss: 0.0979 - acc: 0.97 - ETA: 11:54 - loss: 0.0959 - acc: 0.97 - ETA: 11:52 - loss: 0.1000 - acc: 0.97 - ETA: 11:49 - loss: 0.1306 - acc: 0.97 - ETA: 11:47 - loss: 0.1282 - acc: 0.97 - ETA: 11:44 - loss: 0.1256 - acc: 0.97 - ETA: 11:41 - loss: 0.1230 - acc: 0.97 - ETA: 11:39 - loss: 0.1205 - acc: 0.97 - ETA: 11:36 - loss: 0.1182 - acc: 0.97 - ETA: 11:34 - loss: 0.1178 - acc: 0.97 - ETA: 11:31 - loss: 0.1156 - acc: 0.97 - ETA: 11:29 - loss: 0.1135 - acc: 0.97 - ETA: 11:26 - loss: 0.1115 - acc: 0.97 - ETA: 11:24 - loss: 0.1103 - acc: 0.97 - ETA: 11:21 - loss: 0.1091 - acc: 0.97 - ETA: 11:19 - loss: 0.1079 - acc: 0.97 - ETA: 11:17 - loss: 0.1091 - acc: 0.97 - ETA: 11:14 - loss: 0.1111 - acc: 0.97 - ETA: 11:12 - loss: 0.1172 - acc: 0.97 - ETA: 11:09 - loss: 0.1158 - acc: 0.97 - ETA: 11:07 - loss: 0.1161 - acc: 0.97 - ETA: 11:04 - loss: 0.1144 - acc: 0.97 - ETA: 11:02 - loss: 0.1127 - acc: 0.97 - ETA: 11:00 - loss: 0.1110 - acc: 0.97 - ETA: 10:57 - loss: 0.1097 - acc: 0.97 - ETA: 10:54 - loss: 0.1082 - acc: 0.97 - ETA: 10:52 - loss: 0.1183 - acc: 0.97 - ETA: 10:49 - loss: 0.1178 - acc: 0.97 - ETA: 10:47 - loss: 0.1161 - acc: 0.97 - ETA: 10:44 - loss: 0.1145 - acc: 0.97 - ETA: 10:42 - loss: 0.1129 - acc: 0.97 - ETA: 10:39 - loss: 0.1115 - acc: 0.97 - ETA: 10:37 - loss: 0.1104 - acc: 0.97 - ETA: 10:34 - loss: 0.1092 - acc: 0.97 - ETA: 10:32 - loss: 0.1085 - acc: 0.97 - ETA: 10:29 - loss: 0.1071 - acc: 0.97 - ETA: 10:27 - loss: 0.1058 - acc: 0.97 - ETA: 10:25 - loss: 0.1044 - acc: 0.97 - ETA: 10:22 - loss: 0.1069 - acc: 0.97 - ETA: 10:20 - loss: 0.1158 - acc: 0.97 - ETA: 10:17 - loss: 0.1177 - acc: 0.97 - ETA: 10:15 - loss: 0.1163 - acc: 0.97 - ETA: 10:12 - loss: 0.1164 - acc: 0.97 - ETA: 10:10 - loss: 0.1150 - acc: 0.97 - ETA: 10:07 - loss: 0.1171 - acc: 0.97 - ETA: 10:05 - loss: 0.1158 - acc: 0.97 - ETA: 10:02 - loss: 0.1149 - acc: 0.97 - ETA: 10:00 - loss: 0.1136 - acc: 0.97 - ETA: 9:57 - loss: 0.1132 - acc: 0.9767 - ETA: 9:55 - loss: 0.1132 - acc: 0.976 - ETA: 9:53 - loss: 0.1121 - acc: 0.976 - ETA: 9:50 - loss: 0.1109 - acc: 0.976 - ETA: 9:48 - loss: 0.1104 - acc: 0.976 - ETA: 9:45 - loss: 0.1117 - acc: 0.976 - ETA: 9:43 - loss: 0.1111 - acc: 0.976 - ETA: 9:40 - loss: 0.1120 - acc: 0.976 - ETA: 9:38 - loss: 0.1192 - acc: 0.976 - ETA: 9:35 - loss: 0.1180 - acc: 0.976 - ETA: 9:33 - loss: 0.1189 - acc: 0.976 - ETA: 9:30 - loss: 0.1257 - acc: 0.975 - ETA: 9:28 - loss: 0.1254 - acc: 0.976 - ETA: 9:25 - loss: 0.1254 - acc: 0.975 - ETA: 9:23 - loss: 0.1242 - acc: 0.976 - ETA: 9:21 - loss: 0.1251 - acc: 0.975 - ETA: 9:18 - loss: 0.1317 - acc: 0.974 - ETA: 9:16 - loss: 0.1306 - acc: 0.974 - ETA: 9:13 - loss: 0.1322 - acc: 0.974 - ETA: 9:11 - loss: 0.1310 - acc: 0.974 - ETA: 9:08 - loss: 0.1301 - acc: 0.975 - ETA: 9:06 - loss: 0.1289 - acc: 0.975 - ETA: 9:03 - loss: 0.1283 - acc: 0.975 - ETA: 9:01 - loss: 0.1272 - acc: 0.975 - ETA: 8:58 - loss: 0.1262 - acc: 0.975 - ETA: 8:56 - loss: 0.1257 - acc: 0.975 - ETA: 8:53 - loss: 0.1247 - acc: 0.975 - ETA: 8:51 - loss: 0.1239 - acc: 0.976 - ETA: 8:48 - loss: 0.1228 - acc: 0.976 - ETA: 8:46 - loss: 0.1218 - acc: 0.976 - ETA: 8:43 - loss: 0.1216 - acc: 0.975 - ETA: 8:41 - loss: 0.1229 - acc: 0.975 - ETA: 8:39 - loss: 0.1278 - acc: 0.975 - ETA: 8:36 - loss: 0.1292 - acc: 0.974 - ETA: 8:34 - loss: 0.1282 - acc: 0.975 - ETA: 8:31 - loss: 0.1313 - acc: 0.974 - ETA: 8:29 - loss: 0.1306 - acc: 0.974 - ETA: 8:26 - loss: 0.1359 - acc: 0.974 - ETA: 8:24 - loss: 0.1349 - acc: 0.974 - ETA: 8:21 - loss: 0.1344 - acc: 0.974 - ETA: 8:19 - loss: 0.1334 - acc: 0.974 - ETA: 8:16 - loss: 0.1325 - acc: 0.974 - ETA: 8:14 - loss: 0.1321 - acc: 0.974 - ETA: 8:11 - loss: 0.1312 - acc: 0.974 - ETA: 8:09 - loss: 0.1302 - acc: 0.975 - ETA: 8:06 - loss: 0.1352 - acc: 0.974 - ETA: 8:04 - loss: 0.1359 - acc: 0.974 - ETA: 8:02 - loss: 0.1349 - acc: 0.974 - ETA: 7:59 - loss: 0.1339 - acc: 0.975 - ETA: 7:57 - loss: 0.1331 - acc: 0.975 - ETA: 7:54 - loss: 0.1322 - acc: 0.975 - ETA: 7:52 - loss: 0.1354 - acc: 0.975 - ETA: 7:49 - loss: 0.1344 - acc: 0.975 - ETA: 7:47 - loss: 0.1341 - acc: 0.975 - ETA: 7:44 - loss: 0.1332 - acc: 0.975 - ETA: 7:42 - loss: 0.1359 - acc: 0.974 - ETA: 7:40 - loss: 0.1357 - acc: 0.974 - ETA: 7:37 - loss: 0.1375 - acc: 0.974 - ETA: 7:35 - loss: 0.1367 - acc: 0.974 - ETA: 7:32 - loss: 0.1359 - acc: 0.974 - ETA: 7:30 - loss: 0.1365 - acc: 0.974 - ETA: 7:27 - loss: 0.1356 - acc: 0.974 - ETA: 7:25 - loss: 0.1347 - acc: 0.974 - ETA: 7:22 - loss: 0.1362 - acc: 0.974 - ETA: 7:20 - loss: 0.1386 - acc: 0.974 - ETA: 7:18 - loss: 0.1396 - acc: 0.973 - ETA: 7:15 - loss: 0.1390 - acc: 0.973 - ETA: 7:13 - loss: 0.1399 - acc: 0.973 - ETA: 7:10 - loss: 0.1479 - acc: 0.972 - ETA: 7:08 - loss: 0.1469 - acc: 0.973 - ETA: 7:05 - loss: 0.1461 - acc: 0.973 - ETA: 7:03 - loss: 0.1452 - acc: 0.973 - ETA: 7:00 - loss: 0.1447 - acc: 0.973 - ETA: 6:58 - loss: 0.1443 - acc: 0.973 - ETA: 6:55 - loss: 0.1434 - acc: 0.973 - ETA: 6:53 - loss: 0.1425 - acc: 0.973 - ETA: 6:51 - loss: 0.1418 - acc: 0.973 - ETA: 6:48 - loss: 0.1421 - acc: 0.973 - ETA: 6:46 - loss: 0.1413 - acc: 0.973 - ETA: 6:43 - loss: 0.1406 - acc: 0.973 - ETA: 6:41 - loss: 0.1398 - acc: 0.973 - ETA: 6:38 - loss: 0.1390 - acc: 0.973 - ETA: 6:36 - loss: 0.1382 - acc: 0.973 - ETA: 6:33 - loss: 0.1374 - acc: 0.974 - ETA: 6:31 - loss: 0.1403 - acc: 0.973 - ETA: 6:28 - loss: 0.1396 - acc: 0.973 - ETA: 6:26 - loss: 0.1402 - acc: 0.973 - ETA: 6:24 - loss: 0.1429 - acc: 0.973 - ETA: 6:21 - loss: 0.1422 - acc: 0.973 - ETA: 6:19 - loss: 0.1418 - acc: 0.973 - ETA: 6:16 - loss: 0.1411 - acc: 0.973 - ETA: 6:14 - loss: 0.1415 - acc: 0.973 - ETA: 6:11 - loss: 0.1416 - acc: 0.973 - ETA: 6:09 - loss: 0.1949 - acc: 0.969 - ETA: 6:07 - loss: 0.1964 - acc: 0.969 - ETA: 6:04 - loss: 0.1954 - acc: 0.969 - ETA: 6:02 - loss: 0.1956 - acc: 0.969 - ETA: 5:59 - loss: 0.1959 - acc: 0.969 - ETA: 5:57 - loss: 0.1996 - acc: 0.968 - ETA: 5:54 - loss: 0.2021 - acc: 0.968 - ETA: 5:52 - loss: 0.2013 - acc: 0.968 - ETA: 5:49 - loss: 0.2003 - acc: 0.968 - ETA: 5:47 - loss: 0.1994 - acc: 0.968 - ETA: 5:44 - loss: 0.1984 - acc: 0.968 - ETA: 5:42 - loss: 0.1976 - acc: 0.969 - ETA: 5:40 - loss: 0.1994 - acc: 0.969 - ETA: 5:37 - loss: 0.2026 - acc: 0.968 - ETA: 5:35 - loss: 0.2017 - acc: 0.969 - ETA: 5:32 - loss: 0.2007 - acc: 0.969 - ETA: 5:30 - loss: 0.2037 - acc: 0.969 - ETA: 5:27 - loss: 0.2028 - acc: 0.969 - ETA: 5:25 - loss: 0.2034 - acc: 0.968 - ETA: 5:22 - loss: 0.2039 - acc: 0.968 - ETA: 5:20 - loss: 0.2030 - acc: 0.969 - ETA: 5:17 - loss: 0.2020 - acc: 0.9691"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5:15 - loss: 0.2018 - acc: 0.969 - ETA: 5:13 - loss: 0.2013 - acc: 0.968 - ETA: 5:10 - loss: 0.2003 - acc: 0.969 - ETA: 5:08 - loss: 0.2013 - acc: 0.969 - ETA: 5:05 - loss: 0.2003 - acc: 0.969 - ETA: 5:03 - loss: 0.2006 - acc: 0.969 - ETA: 5:00 - loss: 0.2022 - acc: 0.968 - ETA: 4:58 - loss: 0.2020 - acc: 0.968 - ETA: 4:55 - loss: 0.2051 - acc: 0.968 - ETA: 4:53 - loss: 0.2054 - acc: 0.968 - ETA: 4:51 - loss: 0.2083 - acc: 0.968 - ETA: 4:48 - loss: 0.2078 - acc: 0.968 - ETA: 4:46 - loss: 0.2069 - acc: 0.968 - ETA: 4:43 - loss: 0.2061 - acc: 0.968 - ETA: 4:41 - loss: 0.2052 - acc: 0.968 - ETA: 4:38 - loss: 0.2044 - acc: 0.968 - ETA: 4:36 - loss: 0.2041 - acc: 0.968 - ETA: 4:33 - loss: 0.2031 - acc: 0.968 - ETA: 4:31 - loss: 0.2035 - acc: 0.968 - ETA: 4:29 - loss: 0.2027 - acc: 0.968 - ETA: 4:26 - loss: 0.2018 - acc: 0.968 - ETA: 4:24 - loss: 0.2009 - acc: 0.968 - ETA: 4:21 - loss: 0.2014 - acc: 0.968 - ETA: 4:19 - loss: 0.2021 - acc: 0.968 - ETA: 4:16 - loss: 0.2013 - acc: 0.968 - ETA: 4:14 - loss: 0.2004 - acc: 0.968 - ETA: 4:11 - loss: 0.2009 - acc: 0.968 - ETA: 4:09 - loss: 0.2009 - acc: 0.968 - ETA: 4:06 - loss: 0.2016 - acc: 0.967 - ETA: 4:04 - loss: 0.2075 - acc: 0.967 - ETA: 4:02 - loss: 0.2067 - acc: 0.967 - ETA: 3:59 - loss: 0.2085 - acc: 0.967 - ETA: 3:57 - loss: 0.2077 - acc: 0.967 - ETA: 3:54 - loss: 0.2080 - acc: 0.967 - ETA: 3:52 - loss: 0.2076 - acc: 0.966 - ETA: 3:49 - loss: 0.2068 - acc: 0.966 - ETA: 3:47 - loss: 0.2067 - acc: 0.966 - ETA: 3:44 - loss: 0.2064 - acc: 0.966 - ETA: 3:42 - loss: 0.2058 - acc: 0.966 - ETA: 3:40 - loss: 0.2052 - acc: 0.967 - ETA: 3:37 - loss: 0.2068 - acc: 0.966 - ETA: 3:35 - loss: 0.2060 - acc: 0.967 - ETA: 3:32 - loss: 0.2058 - acc: 0.967 - ETA: 3:30 - loss: 0.2059 - acc: 0.966 - ETA: 3:27 - loss: 0.2053 - acc: 0.967 - ETA: 3:25 - loss: 0.2045 - acc: 0.967 - ETA: 3:22 - loss: 0.2038 - acc: 0.967 - ETA: 3:20 - loss: 0.2030 - acc: 0.967 - ETA: 3:18 - loss: 0.2025 - acc: 0.967 - ETA: 3:15 - loss: 0.2022 - acc: 0.967 - ETA: 3:13 - loss: 0.2063 - acc: 0.967 - ETA: 3:10 - loss: 0.2065 - acc: 0.967 - ETA: 3:08 - loss: 0.2058 - acc: 0.967 - ETA: 3:05 - loss: 0.2050 - acc: 0.967 - ETA: 3:03 - loss: 0.2043 - acc: 0.967 - ETA: 3:00 - loss: 0.2041 - acc: 0.967 - ETA: 2:58 - loss: 0.2034 - acc: 0.967 - ETA: 2:56 - loss: 0.2031 - acc: 0.967 - ETA: 2:53 - loss: 0.2027 - acc: 0.967 - ETA: 2:51 - loss: 0.2019 - acc: 0.967 - ETA: 2:48 - loss: 0.2023 - acc: 0.967 - ETA: 2:46 - loss: 0.2017 - acc: 0.967 - ETA: 2:43 - loss: 0.2012 - acc: 0.967 - ETA: 2:41 - loss: 0.2005 - acc: 0.967 - ETA: 2:38 - loss: 0.2035 - acc: 0.967 - ETA: 2:36 - loss: 0.2034 - acc: 0.967 - ETA: 2:34 - loss: 0.2033 - acc: 0.967 - ETA: 2:31 - loss: 0.2028 - acc: 0.967 - ETA: 2:29 - loss: 0.2033 - acc: 0.966 - ETA: 2:26 - loss: 0.2029 - acc: 0.966 - ETA: 2:24 - loss: 0.2024 - acc: 0.966 - ETA: 2:21 - loss: 0.2020 - acc: 0.966 - ETA: 2:19 - loss: 0.2014 - acc: 0.966 - ETA: 2:16 - loss: 0.2007 - acc: 0.966 - ETA: 2:14 - loss: 0.2002 - acc: 0.966 - ETA: 2:12 - loss: 0.1995 - acc: 0.967 - ETA: 2:09 - loss: 0.2004 - acc: 0.966 - ETA: 2:07 - loss: 0.1997 - acc: 0.966 - ETA: 2:04 - loss: 0.1990 - acc: 0.967 - ETA: 2:02 - loss: 0.1983 - acc: 0.967 - ETA: 1:59 - loss: 0.1981 - acc: 0.967 - ETA: 1:57 - loss: 0.1977 - acc: 0.967 - ETA: 1:54 - loss: 0.1972 - acc: 0.967 - ETA: 1:52 - loss: 0.1967 - acc: 0.967 - ETA: 1:50 - loss: 0.1966 - acc: 0.967 - ETA: 1:47 - loss: 0.2064 - acc: 0.966 - ETA: 1:45 - loss: 0.2058 - acc: 0.966 - ETA: 1:42 - loss: 0.2079 - acc: 0.966 - ETA: 1:40 - loss: 0.2072 - acc: 0.966 - ETA: 1:37 - loss: 0.2078 - acc: 0.966 - ETA: 1:35 - loss: 0.2084 - acc: 0.966 - ETA: 1:32 - loss: 0.2077 - acc: 0.966 - ETA: 1:30 - loss: 0.2072 - acc: 0.966 - ETA: 1:28 - loss: 0.2065 - acc: 0.966 - ETA: 1:25 - loss: 0.2059 - acc: 0.966 - ETA: 1:23 - loss: 0.2052 - acc: 0.966 - ETA: 1:20 - loss: 0.2046 - acc: 0.966 - ETA: 1:18 - loss: 0.2039 - acc: 0.967 - ETA: 1:15 - loss: 0.2032 - acc: 0.967 - ETA: 1:13 - loss: 0.2031 - acc: 0.967 - ETA: 1:10 - loss: 0.2048 - acc: 0.966 - ETA: 1:08 - loss: 0.2047 - acc: 0.966 - ETA: 1:06 - loss: 0.2071 - acc: 0.966 - ETA: 1:03 - loss: 0.2174 - acc: 0.964 - ETA: 1:01 - loss: 0.2174 - acc: 0.964 - ETA: 58s - loss: 0.2167 - acc: 0.964 - ETA: 56s - loss: 0.2163 - acc: 0.96 - ETA: 53s - loss: 0.2156 - acc: 0.96 - ETA: 51s - loss: 0.2176 - acc: 0.96 - ETA: 48s - loss: 0.2173 - acc: 0.96 - ETA: 46s - loss: 0.2167 - acc: 0.96 - ETA: 44s - loss: 0.2160 - acc: 0.96 - ETA: 41s - loss: 0.2154 - acc: 0.96 - ETA: 39s - loss: 0.2153 - acc: 0.96 - ETA: 36s - loss: 0.2163 - acc: 0.96 - ETA: 34s - loss: 0.2162 - acc: 0.96 - ETA: 31s - loss: 0.2160 - acc: 0.96 - ETA: 29s - loss: 0.2157 - acc: 0.96 - ETA: 26s - loss: 0.2151 - acc: 0.96 - ETA: 24s - loss: 0.2145 - acc: 0.96 - ETA: 22s - loss: 0.2139 - acc: 0.96 - ETA: 19s - loss: 0.2134 - acc: 0.96 - ETA: 17s - loss: 0.2128 - acc: 0.96 - ETA: 14s - loss: 0.2122 - acc: 0.96 - ETA: 12s - loss: 0.2119 - acc: 0.96 - ETA: 9s - loss: 0.2113 - acc: 0.9653 - ETA: 7s - loss: 0.2114 - acc: 0.965 - ETA: 4s - loss: 0.2108 - acc: 0.965 - ETA: 2s - loss: 0.2102 - acc: 0.965 - 842s 126ms/step - loss: 0.2096 - acc: 0.9656 - val_loss: 11.2584 - val_acc: 0.0395\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/10\n",
      "4080/6680 [=================>............] - ETA: 13:33 - loss: 0.0012 - acc: 1.00 - ETA: 13:33 - loss: 0.0831 - acc: 0.97 - ETA: 13:26 - loss: 0.0578 - acc: 0.98 - ETA: 13:24 - loss: 0.0452 - acc: 0.98 - ETA: 13:25 - loss: 0.0363 - acc: 0.99 - ETA: 13:22 - loss: 0.0305 - acc: 0.99 - ETA: 13:17 - loss: 0.0263 - acc: 0.99 - ETA: 13:15 - loss: 0.0231 - acc: 0.99 - ETA: 13:13 - loss: 0.0208 - acc: 0.99 - ETA: 13:12 - loss: 0.0189 - acc: 0.99 - ETA: 13:09 - loss: 0.0244 - acc: 0.99 - ETA: 13:06 - loss: 0.0234 - acc: 0.99 - ETA: 13:03 - loss: 0.0219 - acc: 0.99 - ETA: 13:01 - loss: 0.0254 - acc: 0.98 - ETA: 12:58 - loss: 0.0380 - acc: 0.98 - ETA: 12:56 - loss: 0.0358 - acc: 0.98 - ETA: 12:53 - loss: 0.0588 - acc: 0.98 - ETA: 12:51 - loss: 0.0560 - acc: 0.98 - ETA: 12:48 - loss: 0.0531 - acc: 0.98 - ETA: 12:46 - loss: 0.0505 - acc: 0.98 - ETA: 12:43 - loss: 0.0482 - acc: 0.98 - ETA: 12:41 - loss: 0.0549 - acc: 0.98 - ETA: 12:38 - loss: 0.0530 - acc: 0.98 - ETA: 12:36 - loss: 0.0510 - acc: 0.98 - ETA: 12:33 - loss: 0.0508 - acc: 0.98 - ETA: 12:31 - loss: 0.0546 - acc: 0.98 - ETA: 12:28 - loss: 0.0726 - acc: 0.97 - ETA: 12:26 - loss: 0.0758 - acc: 0.97 - ETA: 12:24 - loss: 0.2847 - acc: 0.96 - ETA: 12:22 - loss: 0.3136 - acc: 0.95 - ETA: 12:19 - loss: 0.3269 - acc: 0.95 - ETA: 12:17 - loss: 0.3168 - acc: 0.95 - ETA: 12:15 - loss: 0.3160 - acc: 0.95 - ETA: 12:12 - loss: 0.3068 - acc: 0.95 - ETA: 12:09 - loss: 0.2993 - acc: 0.95 - ETA: 12:07 - loss: 0.2911 - acc: 0.95 - ETA: 12:04 - loss: 0.2832 - acc: 0.95 - ETA: 12:02 - loss: 0.2758 - acc: 0.96 - ETA: 11:59 - loss: 0.2872 - acc: 0.96 - ETA: 11:57 - loss: 0.2801 - acc: 0.96 - ETA: 11:55 - loss: 0.2733 - acc: 0.96 - ETA: 11:52 - loss: 0.2669 - acc: 0.96 - ETA: 11:50 - loss: 0.2607 - acc: 0.96 - ETA: 11:47 - loss: 0.2634 - acc: 0.96 - ETA: 11:45 - loss: 0.2576 - acc: 0.96 - ETA: 11:43 - loss: 0.2521 - acc: 0.96 - ETA: 11:40 - loss: 0.2723 - acc: 0.96 - ETA: 11:38 - loss: 0.2835 - acc: 0.96 - ETA: 11:36 - loss: 0.2777 - acc: 0.96 - ETA: 11:33 - loss: 0.2803 - acc: 0.96 - ETA: 11:31 - loss: 0.2804 - acc: 0.96 - ETA: 11:29 - loss: 0.2752 - acc: 0.96 - ETA: 11:26 - loss: 0.2700 - acc: 0.96 - ETA: 11:24 - loss: 0.2651 - acc: 0.96 - ETA: 11:21 - loss: 0.2652 - acc: 0.96 - ETA: 11:19 - loss: 0.2605 - acc: 0.96 - ETA: 11:16 - loss: 0.2561 - acc: 0.96 - ETA: 11:14 - loss: 0.2656 - acc: 0.96 - ETA: 11:11 - loss: 0.2611 - acc: 0.96 - ETA: 11:09 - loss: 0.2595 - acc: 0.96 - ETA: 11:07 - loss: 0.2553 - acc: 0.96 - ETA: 11:04 - loss: 0.2513 - acc: 0.96 - ETA: 11:02 - loss: 0.2476 - acc: 0.96 - ETA: 10:59 - loss: 0.2448 - acc: 0.96 - ETA: 10:57 - loss: 0.2413 - acc: 0.96 - ETA: 10:54 - loss: 0.2377 - acc: 0.96 - ETA: 10:52 - loss: 0.2342 - acc: 0.97 - ETA: 10:49 - loss: 0.2311 - acc: 0.97 - ETA: 10:47 - loss: 0.2285 - acc: 0.97 - ETA: 10:45 - loss: 0.2263 - acc: 0.97 - ETA: 10:42 - loss: 0.2233 - acc: 0.97 - ETA: 10:40 - loss: 0.2316 - acc: 0.97 - ETA: 10:37 - loss: 0.2303 - acc: 0.97 - ETA: 10:35 - loss: 0.2478 - acc: 0.96 - ETA: 10:32 - loss: 0.2526 - acc: 0.96 - ETA: 10:30 - loss: 0.2493 - acc: 0.96 - ETA: 10:27 - loss: 0.2483 - acc: 0.96 - ETA: 10:25 - loss: 0.2506 - acc: 0.96 - ETA: 10:23 - loss: 0.2559 - acc: 0.96 - ETA: 10:20 - loss: 0.2564 - acc: 0.96 - ETA: 10:18 - loss: 0.2532 - acc: 0.96 - ETA: 10:15 - loss: 0.2502 - acc: 0.96 - ETA: 10:13 - loss: 0.2504 - acc: 0.96 - ETA: 10:10 - loss: 0.2474 - acc: 0.96 - ETA: 10:08 - loss: 0.2448 - acc: 0.96 - ETA: 10:05 - loss: 0.2420 - acc: 0.96 - ETA: 10:03 - loss: 0.2394 - acc: 0.96 - ETA: 10:00 - loss: 0.2443 - acc: 0.96 - ETA: 9:58 - loss: 0.2423 - acc: 0.9663 - ETA: 9:56 - loss: 0.2546 - acc: 0.965 - ETA: 9:53 - loss: 0.2518 - acc: 0.965 - ETA: 9:51 - loss: 0.2519 - acc: 0.965 - ETA: 9:48 - loss: 0.2509 - acc: 0.965 - ETA: 9:46 - loss: 0.2484 - acc: 0.966 - ETA: 9:43 - loss: 0.2459 - acc: 0.966 - ETA: 9:41 - loss: 0.2434 - acc: 0.966 - ETA: 9:38 - loss: 0.2424 - acc: 0.967 - ETA: 9:36 - loss: 0.2400 - acc: 0.967 - ETA: 9:34 - loss: 0.2376 - acc: 0.967 - ETA: 9:31 - loss: 0.2352 - acc: 0.968 - ETA: 9:29 - loss: 0.2329 - acc: 0.968 - ETA: 9:26 - loss: 0.2323 - acc: 0.968 - ETA: 9:24 - loss: 0.2378 - acc: 0.968 - ETA: 9:21 - loss: 0.2356 - acc: 0.968 - ETA: 9:19 - loss: 0.2333 - acc: 0.969 - ETA: 9:16 - loss: 0.2324 - acc: 0.968 - ETA: 9:14 - loss: 0.2303 - acc: 0.969 - ETA: 9:12 - loss: 0.2282 - acc: 0.969 - ETA: 9:09 - loss: 0.2270 - acc: 0.969 - ETA: 9:07 - loss: 0.2250 - acc: 0.969 - ETA: 9:04 - loss: 0.2230 - acc: 0.969 - ETA: 9:02 - loss: 0.2255 - acc: 0.969 - ETA: 8:59 - loss: 0.2259 - acc: 0.969 - ETA: 8:57 - loss: 0.2280 - acc: 0.968 - ETA: 8:54 - loss: 0.2261 - acc: 0.969 - ETA: 8:52 - loss: 0.2259 - acc: 0.969 - ETA: 8:50 - loss: 0.2245 - acc: 0.968 - ETA: 8:47 - loss: 0.2226 - acc: 0.969 - ETA: 8:45 - loss: 0.2209 - acc: 0.969 - ETA: 8:42 - loss: 0.2192 - acc: 0.969 - ETA: 8:40 - loss: 0.2191 - acc: 0.969 - ETA: 8:37 - loss: 0.2174 - acc: 0.969 - ETA: 8:35 - loss: 0.2156 - acc: 0.969 - ETA: 8:32 - loss: 0.2140 - acc: 0.969 - ETA: 8:30 - loss: 0.2134 - acc: 0.969 - ETA: 8:27 - loss: 0.2117 - acc: 0.969 - ETA: 8:25 - loss: 0.2101 - acc: 0.970 - ETA: 8:23 - loss: 0.2109 - acc: 0.969 - ETA: 8:20 - loss: 0.2093 - acc: 0.970 - ETA: 8:18 - loss: 0.2077 - acc: 0.970 - ETA: 8:15 - loss: 0.2063 - acc: 0.970 - ETA: 8:13 - loss: 0.2074 - acc: 0.970 - ETA: 8:11 - loss: 0.2065 - acc: 0.970 - ETA: 8:08 - loss: 0.2063 - acc: 0.970 - ETA: 8:06 - loss: 0.2048 - acc: 0.970 - ETA: 8:03 - loss: 0.2070 - acc: 0.970 - ETA: 8:01 - loss: 0.2077 - acc: 0.969 - ETA: 7:58 - loss: 0.2064 - acc: 0.969 - ETA: 7:56 - loss: 0.2060 - acc: 0.969 - ETA: 7:53 - loss: 0.2052 - acc: 0.969 - ETA: 7:51 - loss: 0.2038 - acc: 0.969 - ETA: 7:48 - loss: 0.2056 - acc: 0.969 - ETA: 7:46 - loss: 0.2050 - acc: 0.969 - ETA: 7:44 - loss: 0.2057 - acc: 0.969 - ETA: 7:41 - loss: 0.2061 - acc: 0.969 - ETA: 7:39 - loss: 0.2073 - acc: 0.968 - ETA: 7:36 - loss: 0.2062 - acc: 0.968 - ETA: 7:34 - loss: 0.2055 - acc: 0.968 - ETA: 7:31 - loss: 0.2042 - acc: 0.968 - ETA: 7:29 - loss: 0.2028 - acc: 0.969 - ETA: 7:27 - loss: 0.2016 - acc: 0.969 - ETA: 7:24 - loss: 0.2006 - acc: 0.969 - ETA: 7:22 - loss: 0.2046 - acc: 0.969 - ETA: 7:19 - loss: 0.2038 - acc: 0.969 - ETA: 7:17 - loss: 0.2026 - acc: 0.969 - ETA: 7:14 - loss: 0.2013 - acc: 0.969 - ETA: 7:12 - loss: 0.2005 - acc: 0.969 - ETA: 7:09 - loss: 0.1992 - acc: 0.969 - ETA: 7:07 - loss: 0.1981 - acc: 0.969 - ETA: 7:05 - loss: 0.2022 - acc: 0.969 - ETA: 7:02 - loss: 0.2029 - acc: 0.969 - ETA: 7:00 - loss: 0.2016 - acc: 0.969 - ETA: 6:57 - loss: 0.2006 - acc: 0.969 - ETA: 6:55 - loss: 0.2010 - acc: 0.969 - ETA: 6:52 - loss: 0.2014 - acc: 0.968 - ETA: 6:50 - loss: 0.2013 - acc: 0.968 - ETA: 6:47 - loss: 0.2025 - acc: 0.968 - ETA: 6:45 - loss: 0.2024 - acc: 0.968 - ETA: 6:42 - loss: 0.2012 - acc: 0.968 - ETA: 6:40 - loss: 0.2007 - acc: 0.968 - ETA: 6:38 - loss: 0.2011 - acc: 0.968 - ETA: 6:35 - loss: 0.1999 - acc: 0.968 - ETA: 6:33 - loss: 0.1989 - acc: 0.968 - ETA: 6:30 - loss: 0.1990 - acc: 0.968 - ETA: 6:28 - loss: 0.1979 - acc: 0.968 - ETA: 6:25 - loss: 0.1969 - acc: 0.969 - ETA: 6:23 - loss: 0.1958 - acc: 0.969 - ETA: 6:21 - loss: 0.1947 - acc: 0.969 - ETA: 6:18 - loss: 0.1937 - acc: 0.969 - ETA: 6:16 - loss: 0.1932 - acc: 0.969 - ETA: 6:13 - loss: 0.1929 - acc: 0.969 - ETA: 6:11 - loss: 0.1925 - acc: 0.969 - ETA: 6:08 - loss: 0.1923 - acc: 0.969 - ETA: 6:06 - loss: 0.1914 - acc: 0.969 - ETA: 6:03 - loss: 0.1921 - acc: 0.969 - ETA: 6:01 - loss: 0.1918 - acc: 0.969 - ETA: 5:59 - loss: 0.1927 - acc: 0.969 - ETA: 5:56 - loss: 0.1921 - acc: 0.968 - ETA: 5:54 - loss: 0.1912 - acc: 0.969 - ETA: 5:51 - loss: 0.1903 - acc: 0.969 - ETA: 5:49 - loss: 0.1898 - acc: 0.969 - ETA: 5:46 - loss: 0.1888 - acc: 0.969 - ETA: 5:44 - loss: 0.1882 - acc: 0.969 - ETA: 5:41 - loss: 0.1874 - acc: 0.969 - ETA: 5:39 - loss: 0.1864 - acc: 0.969 - ETA: 5:37 - loss: 0.1871 - acc: 0.969 - ETA: 5:34 - loss: 0.1888 - acc: 0.969 - ETA: 5:32 - loss: 0.1879 - acc: 0.969 - ETA: 5:29 - loss: 0.1879 - acc: 0.968 - ETA: 5:27 - loss: 0.1888 - acc: 0.968 - ETA: 5:24 - loss: 0.1879 - acc: 0.968 - ETA: 5:22 - loss: 0.1870 - acc: 0.969 - ETA: 5:19 - loss: 0.1865 - acc: 0.969 - ETA: 5:17 - loss: 0.1861 - acc: 0.96896680/6680 [==============================] - ETA: 5:15 - loss: 0.1852 - acc: 0.969 - ETA: 5:12 - loss: 0.1846 - acc: 0.968 - ETA: 5:10 - loss: 0.1840 - acc: 0.968 - ETA: 5:07 - loss: 0.1841 - acc: 0.968 - ETA: 5:05 - loss: 0.1834 - acc: 0.968 - ETA: 5:02 - loss: 0.1830 - acc: 0.968 - ETA: 5:00 - loss: 0.1821 - acc: 0.969 - ETA: 4:57 - loss: 0.1813 - acc: 0.969 - ETA: 4:55 - loss: 0.1804 - acc: 0.969 - ETA: 4:53 - loss: 0.1802 - acc: 0.969 - ETA: 4:50 - loss: 0.1846 - acc: 0.968 - ETA: 4:48 - loss: 0.1838 - acc: 0.968 - ETA: 4:45 - loss: 0.1830 - acc: 0.968 - ETA: 4:43 - loss: 0.1822 - acc: 0.969 - ETA: 4:40 - loss: 0.1814 - acc: 0.969 - ETA: 4:38 - loss: 0.1806 - acc: 0.969 - ETA: 4:36 - loss: 0.1799 - acc: 0.969 - ETA: 4:33 - loss: 0.1792 - acc: 0.969 - ETA: 4:31 - loss: 0.1785 - acc: 0.969 - ETA: 4:28 - loss: 0.1777 - acc: 0.969 - ETA: 4:26 - loss: 0.1769 - acc: 0.970 - ETA: 4:23 - loss: 0.1761 - acc: 0.970 - ETA: 4:21 - loss: 0.1754 - acc: 0.970 - ETA: 4:18 - loss: 0.1747 - acc: 0.970 - ETA: 4:16 - loss: 0.1740 - acc: 0.970 - ETA: 4:14 - loss: 0.1733 - acc: 0.970 - ETA: 4:11 - loss: 0.1729 - acc: 0.970 - ETA: 4:09 - loss: 0.1722 - acc: 0.970 - ETA: 4:06 - loss: 0.1715 - acc: 0.971 - ETA: 4:04 - loss: 0.1707 - acc: 0.971 - ETA: 4:01 - loss: 0.1709 - acc: 0.971 - ETA: 3:59 - loss: 0.1704 - acc: 0.971 - ETA: 3:56 - loss: 0.1716 - acc: 0.971 - ETA: 3:54 - loss: 0.1725 - acc: 0.970 - ETA: 3:52 - loss: 0.1832 - acc: 0.969 - ETA: 3:49 - loss: 0.1861 - acc: 0.969 - ETA: 3:47 - loss: 0.1905 - acc: 0.968 - ETA: 3:44 - loss: 0.1899 - acc: 0.968 - ETA: 3:42 - loss: 0.1906 - acc: 0.968 - ETA: 3:39 - loss: 0.1898 - acc: 0.968 - ETA: 3:37 - loss: 0.1891 - acc: 0.968 - ETA: 3:34 - loss: 0.1892 - acc: 0.968 - ETA: 3:32 - loss: 0.1884 - acc: 0.968 - ETA: 3:30 - loss: 0.1884 - acc: 0.968 - ETA: 3:27 - loss: 0.1877 - acc: 0.968 - ETA: 3:25 - loss: 0.1869 - acc: 0.968 - ETA: 3:22 - loss: 0.1862 - acc: 0.968 - ETA: 3:20 - loss: 0.1854 - acc: 0.968 - ETA: 3:17 - loss: 0.1847 - acc: 0.969 - ETA: 3:15 - loss: 0.1840 - acc: 0.969 - ETA: 3:12 - loss: 0.1833 - acc: 0.969 - ETA: 3:10 - loss: 0.1825 - acc: 0.969 - ETA: 3:08 - loss: 0.1824 - acc: 0.969 - ETA: 3:05 - loss: 0.1861 - acc: 0.969 - ETA: 3:03 - loss: 0.1854 - acc: 0.969 - ETA: 3:00 - loss: 0.1863 - acc: 0.969 - ETA: 2:58 - loss: 0.1864 - acc: 0.969 - ETA: 2:55 - loss: 0.1859 - acc: 0.969 - ETA: 2:53 - loss: 0.1852 - acc: 0.969 - ETA: 2:50 - loss: 0.1845 - acc: 0.969 - ETA: 2:48 - loss: 0.1838 - acc: 0.969 - ETA: 2:46 - loss: 0.1831 - acc: 0.969 - ETA: 2:43 - loss: 0.1825 - acc: 0.969 - ETA: 2:41 - loss: 0.1818 - acc: 0.969 - ETA: 2:38 - loss: 0.1842 - acc: 0.969 - ETA: 2:36 - loss: 0.1835 - acc: 0.969 - ETA: 2:33 - loss: 0.1835 - acc: 0.969 - ETA: 2:31 - loss: 0.1829 - acc: 0.969 - ETA: 2:29 - loss: 0.1822 - acc: 0.970 - ETA: 2:26 - loss: 0.1819 - acc: 0.970 - ETA: 2:24 - loss: 0.1814 - acc: 0.970 - ETA: 2:21 - loss: 0.1810 - acc: 0.970 - ETA: 2:19 - loss: 0.1819 - acc: 0.970 - ETA: 2:16 - loss: 0.1815 - acc: 0.970 - ETA: 2:14 - loss: 0.1809 - acc: 0.970 - ETA: 2:11 - loss: 0.1803 - acc: 0.970 - ETA: 2:09 - loss: 0.1797 - acc: 0.970 - ETA: 2:06 - loss: 0.1792 - acc: 0.970 - ETA: 2:04 - loss: 0.1785 - acc: 0.970 - ETA: 2:02 - loss: 0.1779 - acc: 0.970 - ETA: 1:59 - loss: 0.1801 - acc: 0.970 - ETA: 1:57 - loss: 0.1795 - acc: 0.970 - ETA: 1:54 - loss: 0.1814 - acc: 0.970 - ETA: 1:52 - loss: 0.1856 - acc: 0.970 - ETA: 1:49 - loss: 0.1855 - acc: 0.969 - ETA: 1:47 - loss: 0.1850 - acc: 0.970 - ETA: 1:45 - loss: 0.1852 - acc: 0.969 - ETA: 1:42 - loss: 0.1845 - acc: 0.970 - ETA: 1:40 - loss: 0.1839 - acc: 0.970 - ETA: 1:37 - loss: 0.1833 - acc: 0.970 - ETA: 1:35 - loss: 0.1827 - acc: 0.970 - ETA: 1:32 - loss: 0.1827 - acc: 0.970 - ETA: 1:30 - loss: 0.1826 - acc: 0.970 - ETA: 1:27 - loss: 0.1820 - acc: 0.970 - ETA: 1:25 - loss: 0.1815 - acc: 0.970 - ETA: 1:23 - loss: 0.1825 - acc: 0.970 - ETA: 1:20 - loss: 0.1848 - acc: 0.970 - ETA: 1:18 - loss: 0.1852 - acc: 0.970 - ETA: 1:15 - loss: 0.1849 - acc: 0.969 - ETA: 1:13 - loss: 0.1843 - acc: 0.969 - ETA: 1:10 - loss: 0.1843 - acc: 0.969 - ETA: 1:08 - loss: 0.1838 - acc: 0.969 - ETA: 1:05 - loss: 0.1852 - acc: 0.969 - ETA: 1:03 - loss: 0.1847 - acc: 0.969 - ETA: 1:01 - loss: 0.1847 - acc: 0.969 - ETA: 58s - loss: 0.1847 - acc: 0.969 - ETA: 56s - loss: 0.1841 - acc: 0.96 - ETA: 53s - loss: 0.1835 - acc: 0.96 - ETA: 51s - loss: 0.1848 - acc: 0.96 - ETA: 48s - loss: 0.1890 - acc: 0.96 - ETA: 46s - loss: 0.1886 - acc: 0.96 - ETA: 43s - loss: 0.1882 - acc: 0.96 - ETA: 41s - loss: 0.1876 - acc: 0.96 - ETA: 39s - loss: 0.1887 - acc: 0.96 - ETA: 36s - loss: 0.1887 - acc: 0.96 - ETA: 34s - loss: 0.1883 - acc: 0.96 - ETA: 31s - loss: 0.1877 - acc: 0.96 - ETA: 29s - loss: 0.1872 - acc: 0.96 - ETA: 26s - loss: 0.1866 - acc: 0.96 - ETA: 24s - loss: 0.1861 - acc: 0.96 - ETA: 21s - loss: 0.1857 - acc: 0.96 - ETA: 19s - loss: 0.1855 - acc: 0.96 - ETA: 17s - loss: 0.1849 - acc: 0.96 - ETA: 14s - loss: 0.1845 - acc: 0.96 - ETA: 12s - loss: 0.1840 - acc: 0.96 - ETA: 9s - loss: 0.1859 - acc: 0.9691 - ETA: 7s - loss: 0.1853 - acc: 0.969 - ETA: 4s - loss: 0.1861 - acc: 0.969 - ETA: 2s - loss: 0.1915 - acc: 0.968 - 841s 126ms/step - loss: 0.1937 - acc: 0.9681 - val_loss: 11.5806 - val_acc: 0.0407\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 13:36 - loss: 4.2048e-05 - acc: 1.00 - ETA: 13:37 - loss: 0.0022 - acc: 1.0000   - ETA: 13:35 - loss: 0.0020 - acc: 1.00 - ETA: 13:29 - loss: 0.0021 - acc: 1.00 - ETA: 13:26 - loss: 0.0024 - acc: 1.00 - ETA: 13:22 - loss: 0.0058 - acc: 1.00 - ETA: 13:19 - loss: 0.0068 - acc: 1.00 - ETA: 13:15 - loss: 0.0155 - acc: 0.99 - ETA: 13:12 - loss: 0.0138 - acc: 0.99 - ETA: 13:10 - loss: 0.0553 - acc: 0.98 - ETA: 13:07 - loss: 0.0503 - acc: 0.98 - ETA: 13:05 - loss: 0.0463 - acc: 0.98 - ETA: 13:02 - loss: 0.0503 - acc: 0.98 - ETA: 13:00 - loss: 0.0479 - acc: 0.98 - ETA: 12:57 - loss: 0.0450 - acc: 0.99 - ETA: 12:55 - loss: 0.0577 - acc: 0.98 - ETA: 12:53 - loss: 0.0573 - acc: 0.98 - ETA: 12:51 - loss: 0.0554 - acc: 0.98 - ETA: 12:48 - loss: 0.0526 - acc: 0.98 - ETA: 12:46 - loss: 0.0500 - acc: 0.98 - ETA: 12:43 - loss: 0.0523 - acc: 0.98 - ETA: 12:41 - loss: 0.0507 - acc: 0.98 - ETA: 12:38 - loss: 0.0485 - acc: 0.98 - ETA: 12:35 - loss: 0.0473 - acc: 0.98 - ETA: 12:32 - loss: 0.0454 - acc: 0.98 - ETA: 12:30 - loss: 0.0437 - acc: 0.98 - ETA: 12:29 - loss: 0.0421 - acc: 0.98 - ETA: 12:27 - loss: 0.0406 - acc: 0.98 - ETA: 12:25 - loss: 0.0398 - acc: 0.98 - ETA: 12:22 - loss: 0.0385 - acc: 0.99 - ETA: 12:20 - loss: 0.0375 - acc: 0.99 - ETA: 12:17 - loss: 0.0363 - acc: 0.99 - ETA: 12:15 - loss: 0.0352 - acc: 0.99 - ETA: 12:13 - loss: 0.0342 - acc: 0.99 - ETA: 12:10 - loss: 0.0333 - acc: 0.99 - ETA: 12:08 - loss: 0.0325 - acc: 0.99 - ETA: 12:05 - loss: 0.0316 - acc: 0.99 - ETA: 12:03 - loss: 0.0308 - acc: 0.99 - ETA: 12:00 - loss: 0.0383 - acc: 0.99 - ETA: 11:58 - loss: 0.0447 - acc: 0.98 - ETA: 11:56 - loss: 0.0519 - acc: 0.98 - ETA: 11:53 - loss: 0.0798 - acc: 0.98 - ETA: 11:51 - loss: 0.0985 - acc: 0.98 - ETA: 11:48 - loss: 0.1073 - acc: 0.97 - ETA: 11:46 - loss: 0.1053 - acc: 0.97 - ETA: 11:43 - loss: 0.1030 - acc: 0.97 - ETA: 11:41 - loss: 0.1021 - acc: 0.97 - ETA: 11:38 - loss: 0.1000 - acc: 0.97 - ETA: 11:36 - loss: 0.0980 - acc: 0.97 - ETA: 11:34 - loss: 0.0964 - acc: 0.98 - ETA: 11:31 - loss: 0.0950 - acc: 0.98 - ETA: 11:29 - loss: 0.0933 - acc: 0.98 - ETA: 11:27 - loss: 0.0916 - acc: 0.98 - ETA: 11:24 - loss: 0.1035 - acc: 0.97 - ETA: 11:22 - loss: 0.1017 - acc: 0.98 - ETA: 11:19 - loss: 0.1000 - acc: 0.98 - ETA: 11:17 - loss: 0.0982 - acc: 0.98 - ETA: 11:14 - loss: 0.1151 - acc: 0.97 - ETA: 11:12 - loss: 0.1131 - acc: 0.97 - ETA: 11:09 - loss: 0.1152 - acc: 0.97 - ETA: 11:07 - loss: 0.1162 - acc: 0.97 - ETA: 11:04 - loss: 0.1278 - acc: 0.97 - ETA: 11:02 - loss: 0.1258 - acc: 0.97 - ETA: 10:59 - loss: 0.1239 - acc: 0.97 - ETA: 10:57 - loss: 0.1220 - acc: 0.97 - ETA: 10:55 - loss: 0.1202 - acc: 0.97 - ETA: 10:52 - loss: 0.1203 - acc: 0.97 - ETA: 10:50 - loss: 0.1196 - acc: 0.97 - ETA: 10:47 - loss: 0.1181 - acc: 0.97 - ETA: 10:45 - loss: 0.1164 - acc: 0.98 - ETA: 10:42 - loss: 0.1152 - acc: 0.98 - ETA: 10:40 - loss: 0.1137 - acc: 0.98 - ETA: 10:38 - loss: 0.1121 - acc: 0.98 - ETA: 10:35 - loss: 0.1120 - acc: 0.98 - ETA: 10:33 - loss: 0.1115 - acc: 0.98 - ETA: 10:30 - loss: 0.1107 - acc: 0.97 - ETA: 10:28 - loss: 0.1101 - acc: 0.97 - ETA: 10:26 - loss: 0.1168 - acc: 0.97 - ETA: 10:23 - loss: 0.1164 - acc: 0.97 - ETA: 10:21 - loss: 0.1150 - acc: 0.97 - ETA: 10:19 - loss: 0.1136 - acc: 0.97 - ETA: 10:16 - loss: 0.1122 - acc: 0.97 - ETA: 10:14 - loss: 0.1116 - acc: 0.97 - ETA: 10:11 - loss: 0.1103 - acc: 0.97 - ETA: 10:09 - loss: 0.1112 - acc: 0.97 - ETA: 10:06 - loss: 0.1278 - acc: 0.97 - ETA: 10:04 - loss: 0.1263 - acc: 0.97 - ETA: 10:02 - loss: 0.1255 - acc: 0.97 - ETA: 9:59 - loss: 0.1241 - acc: 0.9787 - ETA: 9:57 - loss: 0.1228 - acc: 0.978 - ETA: 9:54 - loss: 0.1215 - acc: 0.979 - ETA: 9:51 - loss: 0.1209 - acc: 0.978 - ETA: 9:49 - loss: 0.1197 - acc: 0.979 - ETA: 9:47 - loss: 0.1184 - acc: 0.979 - ETA: 9:44 - loss: 0.1172 - acc: 0.979 - ETA: 9:42 - loss: 0.1159 - acc: 0.979 - ETA: 9:39 - loss: 0.1149 - acc: 0.979 - ETA: 9:37 - loss: 0.1153 - acc: 0.979 - ETA: 9:34 - loss: 0.1171 - acc: 0.979 - ETA: 9:32 - loss: 0.1244 - acc: 0.978 - ETA: 9:30 - loss: 0.1255 - acc: 0.977 - ETA: 9:27 - loss: 0.1243 - acc: 0.977 - ETA: 9:25 - loss: 0.1233 - acc: 0.978 - ETA: 9:22 - loss: 0.1236 - acc: 0.977 - ETA: 9:20 - loss: 0.1225 - acc: 0.978 - ETA: 9:17 - loss: 0.1214 - acc: 0.978 - ETA: 9:15 - loss: 0.1223 - acc: 0.978 - ETA: 9:12 - loss: 0.1287 - acc: 0.977 - ETA: 9:10 - loss: 0.1381 - acc: 0.976 - ETA: 9:07 - loss: 0.1411 - acc: 0.975 - ETA: 9:05 - loss: 0.1425 - acc: 0.975 - ETA: 9:03 - loss: 0.1420 - acc: 0.975 - ETA: 9:00 - loss: 0.1407 - acc: 0.975 - ETA: 8:58 - loss: 0.1395 - acc: 0.975 - ETA: 8:55 - loss: 0.1383 - acc: 0.975 - ETA: 8:53 - loss: 0.1381 - acc: 0.975 - ETA: 8:50 - loss: 0.1370 - acc: 0.975 - ETA: 8:48 - loss: 0.1368 - acc: 0.975 - ETA: 8:45 - loss: 0.1359 - acc: 0.975 - ETA: 8:43 - loss: 0.1348 - acc: 0.975 - ETA: 8:40 - loss: 0.1337 - acc: 0.976 - ETA: 8:38 - loss: 0.1415 - acc: 0.975 - ETA: 8:36 - loss: 0.1404 - acc: 0.975 - ETA: 8:33 - loss: 0.1393 - acc: 0.975 - ETA: 8:31 - loss: 0.1383 - acc: 0.976 - ETA: 8:28 - loss: 0.1382 - acc: 0.975 - ETA: 8:26 - loss: 0.1386 - acc: 0.975 - ETA: 8:23 - loss: 0.1413 - acc: 0.974 - ETA: 8:21 - loss: 0.1412 - acc: 0.974 - ETA: 8:18 - loss: 0.1402 - acc: 0.974 - ETA: 8:16 - loss: 0.1434 - acc: 0.974 - ETA: 8:14 - loss: 0.1452 - acc: 0.974 - ETA: 8:11 - loss: 0.1442 - acc: 0.974 - ETA: 8:09 - loss: 0.1454 - acc: 0.974 - ETA: 8:06 - loss: 0.1494 - acc: 0.974 - ETA: 8:04 - loss: 0.1488 - acc: 0.974 - ETA: 8:01 - loss: 0.1478 - acc: 0.974 - ETA: 7:59 - loss: 0.1530 - acc: 0.973 - ETA: 7:56 - loss: 0.1525 - acc: 0.973 - ETA: 7:54 - loss: 0.1516 - acc: 0.973 - ETA: 7:51 - loss: 0.1520 - acc: 0.973 - ETA: 7:49 - loss: 0.1509 - acc: 0.973 - ETA: 7:47 - loss: 0.1499 - acc: 0.973 - ETA: 7:44 - loss: 0.1489 - acc: 0.974 - ETA: 7:42 - loss: 0.1479 - acc: 0.974 - ETA: 7:39 - loss: 0.1469 - acc: 0.974 - ETA: 7:37 - loss: 0.1460 - acc: 0.974 - ETA: 7:34 - loss: 0.1450 - acc: 0.974 - ETA: 7:32 - loss: 0.1441 - acc: 0.974 - ETA: 7:29 - loss: 0.1432 - acc: 0.975 - ETA: 7:27 - loss: 0.1422 - acc: 0.975 - ETA: 7:25 - loss: 0.1413 - acc: 0.975 - ETA: 7:22 - loss: 0.1407 - acc: 0.975 - ETA: 7:20 - loss: 0.1398 - acc: 0.975 - ETA: 7:17 - loss: 0.1391 - acc: 0.975 - ETA: 7:15 - loss: 0.1384 - acc: 0.976 - ETA: 7:12 - loss: 0.1430 - acc: 0.975 - ETA: 7:10 - loss: 0.1422 - acc: 0.975 - ETA: 7:07 - loss: 0.1413 - acc: 0.975 - ETA: 7:05 - loss: 0.1408 - acc: 0.975 - ETA: 7:02 - loss: 0.1400 - acc: 0.975 - ETA: 7:00 - loss: 0.1391 - acc: 0.975 - ETA: 6:58 - loss: 0.1383 - acc: 0.976 - ETA: 6:55 - loss: 0.1375 - acc: 0.976 - ETA: 6:53 - loss: 0.1370 - acc: 0.976 - ETA: 6:50 - loss: 0.1362 - acc: 0.976 - ETA: 6:48 - loss: 0.1361 - acc: 0.976 - ETA: 6:45 - loss: 0.1449 - acc: 0.975 - ETA: 6:43 - loss: 0.1493 - acc: 0.974 - ETA: 6:41 - loss: 0.1497 - acc: 0.974 - ETA: 6:38 - loss: 0.1494 - acc: 0.974 - ETA: 6:36 - loss: 0.1496 - acc: 0.974 - ETA: 6:33 - loss: 0.1487 - acc: 0.974 - ETA: 6:31 - loss: 0.1479 - acc: 0.974 - ETA: 6:28 - loss: 0.1471 - acc: 0.974 - ETA: 6:26 - loss: 0.1462 - acc: 0.974 - ETA: 6:23 - loss: 0.1455 - acc: 0.974 - ETA: 6:21 - loss: 0.1450 - acc: 0.974 - ETA: 6:19 - loss: 0.1443 - acc: 0.974 - ETA: 6:16 - loss: 0.1435 - acc: 0.975 - ETA: 6:14 - loss: 0.1429 - acc: 0.975 - ETA: 6:11 - loss: 0.1433 - acc: 0.975 - ETA: 6:09 - loss: 0.1425 - acc: 0.975 - ETA: 6:06 - loss: 0.1419 - acc: 0.975 - ETA: 6:04 - loss: 0.1411 - acc: 0.975 - ETA: 6:01 - loss: 0.1451 - acc: 0.974 - ETA: 5:59 - loss: 0.1445 - acc: 0.974 - ETA: 5:56 - loss: 0.1444 - acc: 0.974 - ETA: 5:54 - loss: 0.1440 - acc: 0.974 - ETA: 5:52 - loss: 0.1435 - acc: 0.974 - ETA: 5:49 - loss: 0.1428 - acc: 0.974 - ETA: 5:47 - loss: 0.1502 - acc: 0.974 - ETA: 5:44 - loss: 0.1528 - acc: 0.973 - ETA: 5:42 - loss: 0.1556 - acc: 0.973 - ETA: 5:39 - loss: 0.1556 - acc: 0.973 - ETA: 5:37 - loss: 0.1552 - acc: 0.973 - ETA: 5:34 - loss: 0.1544 - acc: 0.973 - ETA: 5:32 - loss: 0.1547 - acc: 0.973 - ETA: 5:29 - loss: 0.1540 - acc: 0.973 - ETA: 5:27 - loss: 0.1539 - acc: 0.973 - ETA: 5:25 - loss: 0.1533 - acc: 0.973 - ETA: 5:22 - loss: 0.1525 - acc: 0.973 - ETA: 5:20 - loss: 0.1519 - acc: 0.973 - ETA: 5:17 - loss: 0.1522 - acc: 0.9738"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5:15 - loss: 0.1534 - acc: 0.973 - ETA: 5:12 - loss: 0.1527 - acc: 0.973 - ETA: 5:10 - loss: 0.1520 - acc: 0.973 - ETA: 5:08 - loss: 0.1523 - acc: 0.973 - ETA: 5:05 - loss: 0.1576 - acc: 0.973 - ETA: 5:03 - loss: 0.1569 - acc: 0.973 - ETA: 5:00 - loss: 0.1565 - acc: 0.973 - ETA: 4:58 - loss: 0.1575 - acc: 0.972 - ETA: 4:55 - loss: 0.1569 - acc: 0.973 - ETA: 4:53 - loss: 0.1564 - acc: 0.973 - ETA: 4:50 - loss: 0.1568 - acc: 0.972 - ETA: 4:48 - loss: 0.1572 - acc: 0.972 - ETA: 4:45 - loss: 0.1565 - acc: 0.972 - ETA: 4:43 - loss: 0.1558 - acc: 0.972 - ETA: 4:41 - loss: 0.1554 - acc: 0.972 - ETA: 4:38 - loss: 0.1566 - acc: 0.972 - ETA: 4:36 - loss: 0.1565 - acc: 0.972 - ETA: 4:33 - loss: 0.1568 - acc: 0.972 - ETA: 4:31 - loss: 0.1573 - acc: 0.972 - ETA: 4:28 - loss: 0.1615 - acc: 0.971 - ETA: 4:26 - loss: 0.1611 - acc: 0.971 - ETA: 4:23 - loss: 0.1604 - acc: 0.971 - ETA: 4:21 - loss: 0.1599 - acc: 0.972 - ETA: 4:19 - loss: 0.1592 - acc: 0.972 - ETA: 4:16 - loss: 0.1585 - acc: 0.972 - ETA: 4:14 - loss: 0.1579 - acc: 0.972 - ETA: 4:11 - loss: 0.1575 - acc: 0.972 - ETA: 4:09 - loss: 0.1574 - acc: 0.972 - ETA: 4:06 - loss: 0.1572 - acc: 0.972 - ETA: 4:04 - loss: 0.1566 - acc: 0.972 - ETA: 4:02 - loss: 0.1561 - acc: 0.972 - ETA: 3:59 - loss: 0.1554 - acc: 0.972 - ETA: 3:57 - loss: 0.1554 - acc: 0.972 - ETA: 3:54 - loss: 0.1550 - acc: 0.972 - ETA: 3:52 - loss: 0.1551 - acc: 0.972 - ETA: 3:49 - loss: 0.1548 - acc: 0.972 - ETA: 3:47 - loss: 0.1542 - acc: 0.972 - ETA: 3:44 - loss: 0.1538 - acc: 0.972 - ETA: 3:42 - loss: 0.1532 - acc: 0.972 - ETA: 3:39 - loss: 0.1526 - acc: 0.972 - ETA: 3:37 - loss: 0.1521 - acc: 0.972 - ETA: 3:35 - loss: 0.1515 - acc: 0.973 - ETA: 3:32 - loss: 0.1509 - acc: 0.973 - ETA: 3:30 - loss: 0.1503 - acc: 0.973 - ETA: 3:27 - loss: 0.1500 - acc: 0.973 - ETA: 3:25 - loss: 0.1515 - acc: 0.972 - ETA: 3:22 - loss: 0.1580 - acc: 0.971 - ETA: 3:20 - loss: 0.1574 - acc: 0.971 - ETA: 3:18 - loss: 0.1567 - acc: 0.971 - ETA: 3:15 - loss: 0.1561 - acc: 0.971 - ETA: 3:13 - loss: 0.1555 - acc: 0.972 - ETA: 3:10 - loss: 0.1550 - acc: 0.972 - ETA: 3:08 - loss: 0.1547 - acc: 0.972 - ETA: 3:05 - loss: 0.1542 - acc: 0.972 - ETA: 3:03 - loss: 0.1542 - acc: 0.972 - ETA: 3:00 - loss: 0.1558 - acc: 0.971 - ETA: 2:58 - loss: 0.1552 - acc: 0.972 - ETA: 2:56 - loss: 0.1547 - acc: 0.972 - ETA: 2:53 - loss: 0.1562 - acc: 0.971 - ETA: 2:51 - loss: 0.1558 - acc: 0.972 - ETA: 2:48 - loss: 0.1581 - acc: 0.971 - ETA: 2:46 - loss: 0.1575 - acc: 0.971 - ETA: 2:43 - loss: 0.1570 - acc: 0.971 - ETA: 2:41 - loss: 0.1564 - acc: 0.971 - ETA: 2:38 - loss: 0.1558 - acc: 0.971 - ETA: 2:36 - loss: 0.1562 - acc: 0.971 - ETA: 2:34 - loss: 0.1558 - acc: 0.972 - ETA: 2:31 - loss: 0.1569 - acc: 0.971 - ETA: 2:29 - loss: 0.1570 - acc: 0.971 - ETA: 2:26 - loss: 0.1564 - acc: 0.971 - ETA: 2:24 - loss: 0.1559 - acc: 0.971 - ETA: 2:21 - loss: 0.1554 - acc: 0.971 - ETA: 2:19 - loss: 0.1566 - acc: 0.971 - ETA: 2:16 - loss: 0.1596 - acc: 0.971 - ETA: 2:14 - loss: 0.1600 - acc: 0.971 - ETA: 2:12 - loss: 0.1597 - acc: 0.971 - ETA: 2:09 - loss: 0.1597 - acc: 0.971 - ETA: 2:07 - loss: 0.1596 - acc: 0.970 - ETA: 2:04 - loss: 0.1594 - acc: 0.970 - ETA: 2:02 - loss: 0.1589 - acc: 0.971 - ETA: 1:59 - loss: 0.1586 - acc: 0.971 - ETA: 1:57 - loss: 0.1586 - acc: 0.971 - ETA: 1:54 - loss: 0.1581 - acc: 0.971 - ETA: 1:52 - loss: 0.1576 - acc: 0.971 - ETA: 1:50 - loss: 0.1581 - acc: 0.970 - ETA: 1:47 - loss: 0.1575 - acc: 0.971 - ETA: 1:45 - loss: 0.1573 - acc: 0.971 - ETA: 1:42 - loss: 0.1700 - acc: 0.969 - ETA: 1:40 - loss: 0.1697 - acc: 0.969 - ETA: 1:37 - loss: 0.1697 - acc: 0.969 - ETA: 1:35 - loss: 0.1696 - acc: 0.969 - ETA: 1:33 - loss: 0.1718 - acc: 0.969 - ETA: 1:30 - loss: 0.1712 - acc: 0.969 - ETA: 1:28 - loss: 0.1708 - acc: 0.969 - ETA: 1:25 - loss: 0.1702 - acc: 0.969 - ETA: 1:23 - loss: 0.1696 - acc: 0.969 - ETA: 1:20 - loss: 0.1693 - acc: 0.969 - ETA: 1:18 - loss: 0.1687 - acc: 0.969 - ETA: 1:16 - loss: 0.1683 - acc: 0.969 - ETA: 1:13 - loss: 0.1679 - acc: 0.969 - ETA: 1:11 - loss: 0.1674 - acc: 0.969 - ETA: 1:08 - loss: 0.1671 - acc: 0.969 - ETA: 1:06 - loss: 0.1692 - acc: 0.968 - ETA: 1:03 - loss: 0.1702 - acc: 0.968 - ETA: 1:01 - loss: 0.1697 - acc: 0.968 - ETA: 58s - loss: 0.1691 - acc: 0.968 - ETA: 56s - loss: 0.1686 - acc: 0.96 - ETA: 54s - loss: 0.1681 - acc: 0.96 - ETA: 51s - loss: 0.1695 - acc: 0.96 - ETA: 49s - loss: 0.1697 - acc: 0.96 - ETA: 46s - loss: 0.1693 - acc: 0.96 - ETA: 44s - loss: 0.1690 - acc: 0.96 - ETA: 41s - loss: 0.1688 - acc: 0.96 - ETA: 39s - loss: 0.1683 - acc: 0.96 - ETA: 36s - loss: 0.1679 - acc: 0.96 - ETA: 34s - loss: 0.1727 - acc: 0.96 - ETA: 31s - loss: 0.1732 - acc: 0.96 - ETA: 29s - loss: 0.1743 - acc: 0.96 - ETA: 27s - loss: 0.1766 - acc: 0.96 - ETA: 24s - loss: 0.1760 - acc: 0.96 - ETA: 22s - loss: 0.1755 - acc: 0.96 - ETA: 19s - loss: 0.1750 - acc: 0.96 - ETA: 17s - loss: 0.1745 - acc: 0.96 - ETA: 14s - loss: 0.1740 - acc: 0.96 - ETA: 12s - loss: 0.1757 - acc: 0.96 - ETA: 9s - loss: 0.1752 - acc: 0.9680 - ETA: 7s - loss: 0.1771 - acc: 0.968 - ETA: 4s - loss: 0.1766 - acc: 0.968 - ETA: 2s - loss: 0.1761 - acc: 0.968 - 853s 128ms/step - loss: 0.1766 - acc: 0.9681 - val_loss: 12.5644 - val_acc: 0.0431\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x384ba9b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3.8278%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 2s 264us/step - loss: 12.2529 - acc: 0.1249 - val_loss: 10.8711 - val_acc: 0.2204\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.87112, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 2s 237us/step - loss: 10.1526 - acc: 0.2766 - val_loss: 9.9821 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.87112 to 9.98212, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 2s 240us/step - loss: 9.5110 - acc: 0.3427 - val_loss: 9.7973 - val_acc: 0.3126\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.98212 to 9.79730, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 2s 235us/step - loss: 9.1193 - acc: 0.3786 - val_loss: 9.3019 - val_acc: 0.3317\n",
      "\n",
      "Epoch 00004: val_loss improved from 9.79730 to 9.30188, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 2s 252us/step - loss: 8.4110 - acc: 0.4220 - val_loss: 8.7171 - val_acc: 0.3760\n",
      "\n",
      "Epoch 00005: val_loss improved from 9.30188 to 8.71713, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 2s 247us/step - loss: 8.0751 - acc: 0.4518 - val_loss: 8.5122 - val_acc: 0.3808\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.71713 to 8.51223, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 2s 242us/step - loss: 7.8203 - acc: 0.4760 - val_loss: 8.3862 - val_acc: 0.3916\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.51223 to 8.38616, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 2s 248us/step - loss: 7.6928 - acc: 0.4919 - val_loss: 8.2482 - val_acc: 0.4024\n",
      "\n",
      "Epoch 00008: val_loss improved from 8.38616 to 8.24824, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 2s 231us/step - loss: 7.4188 - acc: 0.5033 - val_loss: 7.8381 - val_acc: 0.4096\n",
      "\n",
      "Epoch 00009: val_loss improved from 8.24824 to 7.83810, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 2s 245us/step - loss: 7.1074 - acc: 0.5298 - val_loss: 7.8092 - val_acc: 0.4180\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.83810 to 7.80916, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 2s 254us/step - loss: 7.0254 - acc: 0.5449 - val_loss: 7.7817 - val_acc: 0.4275\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.80916 to 7.78171, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 2s 227us/step - loss: 7.0018 - acc: 0.5522 - val_loss: 7.7136 - val_acc: 0.4383\n",
      "\n",
      "Epoch 00012: val_loss improved from 7.78171 to 7.71363, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 2s 243us/step - loss: 6.9296 - acc: 0.5504 - val_loss: 7.8155 - val_acc: 0.4251\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 2s 229us/step - loss: 6.7347 - acc: 0.5644 - val_loss: 7.4365 - val_acc: 0.4563\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.71363 to 7.43646, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 2s 232us/step - loss: 6.6077 - acc: 0.5740 - val_loss: 7.4074 - val_acc: 0.4611\n",
      "\n",
      "Epoch 00015: val_loss improved from 7.43646 to 7.40745, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 2s 227us/step - loss: 6.5182 - acc: 0.5778 - val_loss: 7.3621 - val_acc: 0.4515\n",
      "\n",
      "Epoch 00016: val_loss improved from 7.40745 to 7.36210, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 2s 234us/step - loss: 6.3379 - acc: 0.5892 - val_loss: 7.1915 - val_acc: 0.4611\n",
      "\n",
      "Epoch 00017: val_loss improved from 7.36210 to 7.19151, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 2s 226us/step - loss: 6.1719 - acc: 0.6027 - val_loss: 7.0476 - val_acc: 0.4695\n",
      "\n",
      "Epoch 00018: val_loss improved from 7.19151 to 7.04758, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 2s 234us/step - loss: 6.0358 - acc: 0.6115 - val_loss: 6.9450 - val_acc: 0.4790\n",
      "\n",
      "Epoch 00019: val_loss improved from 7.04758 to 6.94503, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 2s 228us/step - loss: 5.9140 - acc: 0.6174 - val_loss: 7.0697 - val_acc: 0.4599\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c652e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 49.6411%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "pretrained_model_name = \"Resnet50\"\n",
    "bottleneck_features = np.load('bottleneck_features/Dog'+ pretrained_model_name + 'Data.npz')\n",
    "train_new_model = bottleneck_features['train']\n",
    "valid_new_model = bottleneck_features['valid']\n",
    "test_new_model = bottleneck_features['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(GlobalAveragePooling2D(input_shape=train_new_model.shape[1:]))\n",
    "new_model.add(Dropout(.2))\n",
    "new_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "new_model.summary()\n",
    "print(train_new_model.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 7:09 - loss: 5.5551 - acc: 0.0000e+0 - ETA: 1:48 - loss: 5.5665 - acc: 0.0000e+0 - ETA: 48s - loss: 5.4096 - acc: 0.0000e+0 - ETA: 29s - loss: 5.3278 - acc: 0.0133   - ETA: 20s - loss: 5.2256 - acc: 0.01 - ETA: 16s - loss: 5.1647 - acc: 0.02 - ETA: 13s - loss: 5.0891 - acc: 0.02 - ETA: 10s - loss: 5.0249 - acc: 0.03 - ETA: 9s - loss: 4.9657 - acc: 0.0348 - ETA: 8s - loss: 4.9030 - acc: 0.039 - ETA: 7s - loss: 4.8553 - acc: 0.043 - ETA: 6s - loss: 4.7930 - acc: 0.049 - ETA: 5s - loss: 4.7372 - acc: 0.054 - ETA: 5s - loss: 4.6845 - acc: 0.061 - ETA: 4s - loss: 4.6247 - acc: 0.067 - ETA: 4s - loss: 4.5622 - acc: 0.076 - ETA: 3s - loss: 4.4980 - acc: 0.088 - ETA: 3s - loss: 4.4428 - acc: 0.098 - ETA: 3s - loss: 4.3867 - acc: 0.106 - ETA: 2s - loss: 4.3169 - acc: 0.119 - ETA: 2s - loss: 4.2604 - acc: 0.130 - ETA: 2s - loss: 4.2000 - acc: 0.143 - ETA: 2s - loss: 4.1420 - acc: 0.154 - ETA: 1s - loss: 4.0876 - acc: 0.166 - ETA: 1s - loss: 4.0338 - acc: 0.175 - ETA: 1s - loss: 3.9771 - acc: 0.186 - ETA: 1s - loss: 3.9084 - acc: 0.197 - ETA: 0s - loss: 3.8430 - acc: 0.209 - ETA: 0s - loss: 3.7783 - acc: 0.221 - ETA: 0s - loss: 3.7279 - acc: 0.231 - ETA: 0s - loss: 3.6845 - acc: 0.239 - ETA: 0s - loss: 3.6392 - acc: 0.249 - ETA: 0s - loss: 3.5928 - acc: 0.258 - 4s 583us/step - loss: 3.5675 - acc: 0.2624 - val_loss: 2.2049 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.20491, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 2.0266 - acc: 0.666 - ETA: 1s - loss: 2.0348 - acc: 0.587 - ETA: 1s - loss: 2.0860 - acc: 0.562 - ETA: 1s - loss: 2.0628 - acc: 0.578 - ETA: 1s - loss: 2.0417 - acc: 0.590 - ETA: 1s - loss: 2.0311 - acc: 0.592 - ETA: 1s - loss: 2.0150 - acc: 0.589 - ETA: 0s - loss: 2.0103 - acc: 0.586 - ETA: 0s - loss: 1.9824 - acc: 0.594 - ETA: 0s - loss: 1.9727 - acc: 0.592 - ETA: 0s - loss: 1.9560 - acc: 0.593 - ETA: 0s - loss: 1.9530 - acc: 0.590 - ETA: 0s - loss: 1.9394 - acc: 0.594 - ETA: 0s - loss: 1.9354 - acc: 0.591 - ETA: 0s - loss: 1.9230 - acc: 0.595 - ETA: 0s - loss: 1.9115 - acc: 0.598 - ETA: 0s - loss: 1.8981 - acc: 0.602 - ETA: 0s - loss: 1.8900 - acc: 0.602 - ETA: 0s - loss: 1.8751 - acc: 0.604 - ETA: 0s - loss: 1.8638 - acc: 0.604 - ETA: 0s - loss: 1.8505 - acc: 0.606 - ETA: 0s - loss: 1.8422 - acc: 0.608 - ETA: 0s - loss: 1.8267 - acc: 0.612 - ETA: 0s - loss: 1.8145 - acc: 0.613 - ETA: 0s - loss: 1.8035 - acc: 0.615 - ETA: 0s - loss: 1.7952 - acc: 0.616 - ETA: 0s - loss: 1.7844 - acc: 0.618 - ETA: 0s - loss: 1.7719 - acc: 0.618 - ETA: 0s - loss: 1.7527 - acc: 0.623 - 2s 236us/step - loss: 1.7437 - acc: 0.6243 - val_loss: 1.4011 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.20491 to 1.40112, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 1.1197 - acc: 0.766 - ETA: 1s - loss: 1.2358 - acc: 0.725 - ETA: 1s - loss: 1.2887 - acc: 0.711 - ETA: 1s - loss: 1.2889 - acc: 0.710 - ETA: 1s - loss: 1.3175 - acc: 0.698 - ETA: 1s - loss: 1.3130 - acc: 0.701 - ETA: 1s - loss: 1.3122 - acc: 0.709 - ETA: 1s - loss: 1.3260 - acc: 0.702 - ETA: 1s - loss: 1.3045 - acc: 0.709 - ETA: 1s - loss: 1.3019 - acc: 0.708 - ETA: 1s - loss: 1.2919 - acc: 0.710 - ETA: 1s - loss: 1.2812 - acc: 0.715 - ETA: 1s - loss: 1.2905 - acc: 0.707 - ETA: 1s - loss: 1.2775 - acc: 0.711 - ETA: 0s - loss: 1.2745 - acc: 0.711 - ETA: 0s - loss: 1.2686 - acc: 0.711 - ETA: 0s - loss: 1.2576 - acc: 0.713 - ETA: 0s - loss: 1.2510 - acc: 0.715 - ETA: 0s - loss: 1.2459 - acc: 0.716 - ETA: 0s - loss: 1.2362 - acc: 0.718 - ETA: 0s - loss: 1.2317 - acc: 0.716 - ETA: 0s - loss: 1.2214 - acc: 0.720 - ETA: 0s - loss: 1.2187 - acc: 0.719 - ETA: 0s - loss: 1.2114 - acc: 0.721 - ETA: 0s - loss: 1.2026 - acc: 0.724 - ETA: 0s - loss: 1.2036 - acc: 0.723 - ETA: 0s - loss: 1.1997 - acc: 0.724 - ETA: 0s - loss: 1.1929 - acc: 0.726 - ETA: 0s - loss: 1.1893 - acc: 0.726 - ETA: 0s - loss: 1.1849 - acc: 0.726 - ETA: 0s - loss: 1.1828 - acc: 0.726 - 2s 254us/step - loss: 1.1827 - acc: 0.7262 - val_loss: 1.0981 - val_acc: 0.7497\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.40112 to 1.09812, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 1.3288 - acc: 0.666 - ETA: 1s - loss: 0.9893 - acc: 0.762 - ETA: 1s - loss: 1.0550 - acc: 0.737 - ETA: 1s - loss: 1.0247 - acc: 0.753 - ETA: 1s - loss: 0.9918 - acc: 0.766 - ETA: 1s - loss: 0.9816 - acc: 0.772 - ETA: 1s - loss: 0.9687 - acc: 0.773 - ETA: 1s - loss: 0.9626 - acc: 0.777 - ETA: 1s - loss: 0.9435 - acc: 0.783 - ETA: 1s - loss: 0.9478 - acc: 0.779 - ETA: 1s - loss: 0.9406 - acc: 0.782 - ETA: 1s - loss: 0.9407 - acc: 0.781 - ETA: 0s - loss: 0.9528 - acc: 0.780 - ETA: 0s - loss: 0.9503 - acc: 0.782 - ETA: 0s - loss: 0.9434 - acc: 0.785 - ETA: 0s - loss: 0.9393 - acc: 0.788 - ETA: 0s - loss: 0.9373 - acc: 0.790 - ETA: 0s - loss: 0.9317 - acc: 0.791 - ETA: 0s - loss: 0.9299 - acc: 0.791 - ETA: 0s - loss: 0.9393 - acc: 0.788 - ETA: 0s - loss: 0.9378 - acc: 0.789 - ETA: 0s - loss: 0.9388 - acc: 0.788 - ETA: 0s - loss: 0.9372 - acc: 0.789 - ETA: 0s - loss: 0.9372 - acc: 0.788 - ETA: 0s - loss: 0.9332 - acc: 0.789 - ETA: 0s - loss: 0.9349 - acc: 0.788 - ETA: 0s - loss: 0.9320 - acc: 0.788 - ETA: 0s - loss: 0.9302 - acc: 0.788 - ETA: 0s - loss: 0.9252 - acc: 0.790 - 2s 244us/step - loss: 0.9242 - acc: 0.7898 - val_loss: 0.9514 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.09812 to 0.95136, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.9685 - acc: 0.766 - ETA: 1s - loss: 0.8465 - acc: 0.837 - ETA: 1s - loss: 0.8336 - acc: 0.828 - ETA: 1s - loss: 0.8197 - acc: 0.825 - ETA: 1s - loss: 0.8213 - acc: 0.819 - ETA: 1s - loss: 0.8154 - acc: 0.824 - ETA: 1s - loss: 0.7963 - acc: 0.833 - ETA: 1s - loss: 0.8027 - acc: 0.827 - ETA: 1s - loss: 0.7934 - acc: 0.829 - ETA: 0s - loss: 0.8013 - acc: 0.824 - ETA: 0s - loss: 0.7961 - acc: 0.824 - ETA: 0s - loss: 0.8024 - acc: 0.825 - ETA: 0s - loss: 0.8052 - acc: 0.822 - ETA: 0s - loss: 0.7947 - acc: 0.823 - ETA: 0s - loss: 0.7939 - acc: 0.823 - ETA: 0s - loss: 0.7904 - acc: 0.823 - ETA: 0s - loss: 0.7864 - acc: 0.823 - ETA: 0s - loss: 0.7906 - acc: 0.821 - ETA: 0s - loss: 0.7889 - acc: 0.820 - ETA: 0s - loss: 0.7840 - acc: 0.821 - ETA: 0s - loss: 0.7823 - acc: 0.821 - ETA: 0s - loss: 0.7806 - acc: 0.822 - ETA: 0s - loss: 0.7812 - acc: 0.821 - ETA: 0s - loss: 0.7799 - acc: 0.820 - ETA: 0s - loss: 0.7802 - acc: 0.819 - ETA: 0s - loss: 0.7787 - acc: 0.819 - ETA: 0s - loss: 0.7797 - acc: 0.819 - ETA: 0s - loss: 0.7744 - acc: 0.821 - ETA: 0s - loss: 0.7726 - acc: 0.819 - ETA: 0s - loss: 0.7716 - acc: 0.820 - 2s 246us/step - loss: 0.7713 - acc: 0.8201 - val_loss: 0.8582 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.95136 to 0.85818, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 0.4518 - acc: 0.866 - ETA: 1s - loss: 0.7256 - acc: 0.848 - ETA: 1s - loss: 0.6956 - acc: 0.835 - ETA: 1s - loss: 0.6793 - acc: 0.842 - ETA: 1s - loss: 0.6738 - acc: 0.842 - ETA: 1s - loss: 0.6711 - acc: 0.845 - ETA: 1s - loss: 0.6872 - acc: 0.838 - ETA: 1s - loss: 0.6883 - acc: 0.836 - ETA: 1s - loss: 0.6860 - acc: 0.841 - ETA: 1s - loss: 0.6870 - acc: 0.843 - ETA: 1s - loss: 0.6962 - acc: 0.840 - ETA: 1s - loss: 0.7006 - acc: 0.838 - ETA: 0s - loss: 0.6995 - acc: 0.840 - ETA: 0s - loss: 0.7054 - acc: 0.836 - ETA: 0s - loss: 0.7030 - acc: 0.838 - ETA: 0s - loss: 0.7018 - acc: 0.838 - ETA: 0s - loss: 0.6930 - acc: 0.840 - ETA: 0s - loss: 0.6949 - acc: 0.842 - ETA: 0s - loss: 0.6906 - acc: 0.842 - ETA: 0s - loss: 0.6902 - acc: 0.842 - ETA: 0s - loss: 0.6870 - acc: 0.843 - ETA: 0s - loss: 0.6836 - acc: 0.844 - ETA: 0s - loss: 0.6793 - acc: 0.846 - ETA: 0s - loss: 0.6803 - acc: 0.846 - ETA: 0s - loss: 0.6804 - acc: 0.846 - ETA: 0s - loss: 0.6789 - acc: 0.845 - ETA: 0s - loss: 0.6764 - acc: 0.846 - ETA: 0s - loss: 0.6715 - acc: 0.847 - ETA: 0s - loss: 0.6706 - acc: 0.847 - 2s 240us/step - loss: 0.6696 - acc: 0.8482 - val_loss: 0.7952 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.85818 to 0.79519, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.6879 - acc: 0.866 - ETA: 1s - loss: 0.6210 - acc: 0.854 - ETA: 1s - loss: 0.6081 - acc: 0.857 - ETA: 1s - loss: 0.5849 - acc: 0.866 - ETA: 1s - loss: 0.6038 - acc: 0.862 - ETA: 1s - loss: 0.6072 - acc: 0.861 - ETA: 1s - loss: 0.6034 - acc: 0.858 - ETA: 1s - loss: 0.6082 - acc: 0.854 - ETA: 1s - loss: 0.6136 - acc: 0.850 - ETA: 1s - loss: 0.6166 - acc: 0.848 - ETA: 1s - loss: 0.6078 - acc: 0.852 - ETA: 1s - loss: 0.6071 - acc: 0.854 - ETA: 1s - loss: 0.6060 - acc: 0.855 - ETA: 0s - loss: 0.6039 - acc: 0.856 - ETA: 0s - loss: 0.6019 - acc: 0.857 - ETA: 0s - loss: 0.6063 - acc: 0.856 - ETA: 0s - loss: 0.6053 - acc: 0.858 - ETA: 0s - loss: 0.6039 - acc: 0.858 - ETA: 0s - loss: 0.6023 - acc: 0.858 - ETA: 0s - loss: 0.6042 - acc: 0.858 - ETA: 0s - loss: 0.6006 - acc: 0.859 - ETA: 0s - loss: 0.6004 - acc: 0.858 - ETA: 0s - loss: 0.5996 - acc: 0.858 - ETA: 0s - loss: 0.6013 - acc: 0.857 - ETA: 0s - loss: 0.6010 - acc: 0.857 - ETA: 0s - loss: 0.6015 - acc: 0.856 - ETA: 0s - loss: 0.6000 - acc: 0.857 - ETA: 0s - loss: 0.5994 - acc: 0.857 - ETA: 0s - loss: 0.5991 - acc: 0.858 - ETA: 0s - loss: 0.5983 - acc: 0.859 - 2s 248us/step - loss: 0.5974 - acc: 0.8585 - val_loss: 0.7501 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.79519 to 0.75011, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.5030 - acc: 0.866 - ETA: 1s - loss: 0.4581 - acc: 0.904 - ETA: 1s - loss: 0.4856 - acc: 0.908 - ETA: 1s - loss: 0.5236 - acc: 0.887 - ETA: 1s - loss: 0.5468 - acc: 0.875 - ETA: 1s - loss: 0.5272 - acc: 0.883 - ETA: 1s - loss: 0.5390 - acc: 0.880 - ETA: 1s - loss: 0.5351 - acc: 0.879 - ETA: 1s - loss: 0.5456 - acc: 0.874 - ETA: 1s - loss: 0.5390 - acc: 0.876 - ETA: 1s - loss: 0.5440 - acc: 0.874 - ETA: 1s - loss: 0.5433 - acc: 0.876 - ETA: 0s - loss: 0.5428 - acc: 0.877 - ETA: 0s - loss: 0.5497 - acc: 0.873 - ETA: 0s - loss: 0.5473 - acc: 0.874 - ETA: 0s - loss: 0.5409 - acc: 0.875 - ETA: 0s - loss: 0.5396 - acc: 0.875 - ETA: 0s - loss: 0.5379 - acc: 0.876 - ETA: 0s - loss: 0.5433 - acc: 0.875 - ETA: 0s - loss: 0.5401 - acc: 0.876 - ETA: 0s - loss: 0.5385 - acc: 0.876 - ETA: 0s - loss: 0.5393 - acc: 0.874 - ETA: 0s - loss: 0.5387 - acc: 0.875 - ETA: 0s - loss: 0.5404 - acc: 0.875 - ETA: 0s - loss: 0.5439 - acc: 0.874 - ETA: 0s - loss: 0.5454 - acc: 0.873 - ETA: 0s - loss: 0.5468 - acc: 0.873 - ETA: 0s - loss: 0.5465 - acc: 0.874 - ETA: 0s - loss: 0.5444 - acc: 0.875 - ETA: 0s - loss: 0.5439 - acc: 0.875 - 2s 247us/step - loss: 0.5438 - acc: 0.8750 - val_loss: 0.7159 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.75011 to 0.71594, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3880 - acc: 1.000 - ETA: 1s - loss: 0.3901 - acc: 0.937 - ETA: 1s - loss: 0.4253 - acc: 0.904 - ETA: 1s - loss: 0.4576 - acc: 0.897 - ETA: 1s - loss: 0.4921 - acc: 0.885 - ETA: 1s - loss: 0.4923 - acc: 0.894 - ETA: 1s - loss: 0.4904 - acc: 0.893 - ETA: 1s - loss: 0.4942 - acc: 0.890 - ETA: 1s - loss: 0.4952 - acc: 0.888 - ETA: 1s - loss: 0.4947 - acc: 0.888 - ETA: 1s - loss: 0.4903 - acc: 0.891 - ETA: 0s - loss: 0.4873 - acc: 0.892 - ETA: 0s - loss: 0.4891 - acc: 0.892 - ETA: 0s - loss: 0.4957 - acc: 0.888 - ETA: 0s - loss: 0.4894 - acc: 0.891 - ETA: 0s - loss: 0.4862 - acc: 0.892 - ETA: 0s - loss: 0.4826 - acc: 0.893 - ETA: 0s - loss: 0.4813 - acc: 0.895 - ETA: 0s - loss: 0.4814 - acc: 0.895 - ETA: 0s - loss: 0.4817 - acc: 0.895 - ETA: 0s - loss: 0.4820 - acc: 0.895 - ETA: 0s - loss: 0.4802 - acc: 0.896 - ETA: 0s - loss: 0.4811 - acc: 0.895 - ETA: 0s - loss: 0.4820 - acc: 0.895 - ETA: 0s - loss: 0.4835 - acc: 0.893 - ETA: 0s - loss: 0.4864 - acc: 0.892 - ETA: 0s - loss: 0.4904 - acc: 0.891 - ETA: 0s - loss: 0.4896 - acc: 0.892 - 2s 232us/step - loss: 0.4910 - acc: 0.8909 - val_loss: 0.6809 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.71594 to 0.68087, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3985 - acc: 0.966 - ETA: 1s - loss: 0.4532 - acc: 0.911 - ETA: 1s - loss: 0.4508 - acc: 0.910 - ETA: 1s - loss: 0.4494 - acc: 0.902 - ETA: 1s - loss: 0.4524 - acc: 0.893 - ETA: 1s - loss: 0.4717 - acc: 0.883 - ETA: 0s - loss: 0.4618 - acc: 0.886 - ETA: 0s - loss: 0.4600 - acc: 0.890 - ETA: 0s - loss: 0.4584 - acc: 0.893 - ETA: 0s - loss: 0.4608 - acc: 0.893 - ETA: 0s - loss: 0.4591 - acc: 0.894 - ETA: 0s - loss: 0.4582 - acc: 0.894 - ETA: 0s - loss: 0.4607 - acc: 0.893 - ETA: 0s - loss: 0.4585 - acc: 0.892 - ETA: 0s - loss: 0.4575 - acc: 0.894 - ETA: 0s - loss: 0.4567 - acc: 0.894 - ETA: 0s - loss: 0.4587 - acc: 0.894 - ETA: 0s - loss: 0.4547 - acc: 0.895 - ETA: 0s - loss: 0.4544 - acc: 0.895 - ETA: 0s - loss: 0.4552 - acc: 0.895 - ETA: 0s - loss: 0.4532 - acc: 0.895 - ETA: 0s - loss: 0.4531 - acc: 0.896 - ETA: 0s - loss: 0.4548 - acc: 0.896 - ETA: 0s - loss: 0.4576 - acc: 0.895 - ETA: 0s - loss: 0.4570 - acc: 0.895 - ETA: 0s - loss: 0.4600 - acc: 0.895 - ETA: 0s - loss: 0.4599 - acc: 0.894 - ETA: 0s - loss: 0.4572 - acc: 0.896 - 2s 228us/step - loss: 0.4572 - acc: 0.8963 - val_loss: 0.6614 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68087 to 0.66136, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.3066 - acc: 0.900 - ETA: 1s - loss: 0.4083 - acc: 0.908 - ETA: 1s - loss: 0.4296 - acc: 0.900 - ETA: 1s - loss: 0.4290 - acc: 0.897 - ETA: 1s - loss: 0.4251 - acc: 0.900 - ETA: 1s - loss: 0.4237 - acc: 0.900 - ETA: 1s - loss: 0.4251 - acc: 0.896 - ETA: 1s - loss: 0.4212 - acc: 0.901 - ETA: 1s - loss: 0.4299 - acc: 0.897 - ETA: 1s - loss: 0.4366 - acc: 0.896 - ETA: 1s - loss: 0.4400 - acc: 0.897 - ETA: 1s - loss: 0.4290 - acc: 0.901 - ETA: 1s - loss: 0.4273 - acc: 0.901 - ETA: 0s - loss: 0.4265 - acc: 0.902 - ETA: 0s - loss: 0.4281 - acc: 0.902 - ETA: 0s - loss: 0.4299 - acc: 0.901 - ETA: 0s - loss: 0.4293 - acc: 0.902 - ETA: 0s - loss: 0.4305 - acc: 0.901 - ETA: 0s - loss: 0.4268 - acc: 0.903 - ETA: 0s - loss: 0.4246 - acc: 0.904 - ETA: 0s - loss: 0.4248 - acc: 0.905 - ETA: 0s - loss: 0.4222 - acc: 0.906 - ETA: 0s - loss: 0.4249 - acc: 0.904 - ETA: 0s - loss: 0.4258 - acc: 0.905 - ETA: 0s - loss: 0.4235 - acc: 0.905 - ETA: 0s - loss: 0.4245 - acc: 0.904 - ETA: 0s - loss: 0.4235 - acc: 0.905 - ETA: 0s - loss: 0.4231 - acc: 0.905 - ETA: 0s - loss: 0.4215 - acc: 0.906 - ETA: 0s - loss: 0.4216 - acc: 0.906 - 2s 249us/step - loss: 0.4196 - acc: 0.9072 - val_loss: 0.6456 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.66136 to 0.64556, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.4068 - acc: 0.866 - ETA: 1s - loss: 0.4093 - acc: 0.895 - ETA: 1s - loss: 0.4298 - acc: 0.897 - ETA: 1s - loss: 0.4115 - acc: 0.909 - ETA: 1s - loss: 0.4223 - acc: 0.907 - ETA: 1s - loss: 0.4098 - acc: 0.911 - ETA: 1s - loss: 0.4074 - acc: 0.914 - ETA: 1s - loss: 0.4126 - acc: 0.913 - ETA: 1s - loss: 0.4129 - acc: 0.913 - ETA: 1s - loss: 0.4067 - acc: 0.915 - ETA: 1s - loss: 0.4040 - acc: 0.914 - ETA: 0s - loss: 0.4029 - acc: 0.913 - ETA: 0s - loss: 0.3973 - acc: 0.914 - ETA: 0s - loss: 0.4058 - acc: 0.911 - ETA: 0s - loss: 0.4096 - acc: 0.910 - ETA: 0s - loss: 0.4082 - acc: 0.911 - ETA: 0s - loss: 0.4076 - acc: 0.910 - ETA: 0s - loss: 0.4112 - acc: 0.908 - ETA: 0s - loss: 0.4106 - acc: 0.908 - ETA: 0s - loss: 0.4088 - acc: 0.909 - ETA: 0s - loss: 0.4075 - acc: 0.909 - ETA: 0s - loss: 0.4066 - acc: 0.909 - ETA: 0s - loss: 0.4024 - acc: 0.911 - ETA: 0s - loss: 0.4025 - acc: 0.911 - ETA: 0s - loss: 0.3997 - acc: 0.912 - ETA: 0s - loss: 0.3983 - acc: 0.913 - ETA: 0s - loss: 0.3984 - acc: 0.913 - ETA: 0s - loss: 0.3981 - acc: 0.913 - ETA: 0s - loss: 0.3966 - acc: 0.914 - ETA: 0s - loss: 0.3963 - acc: 0.913 - ETA: 0s - loss: 0.3943 - acc: 0.914 - 2s 258us/step - loss: 0.3944 - acc: 0.9147 - val_loss: 0.6313 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.64556 to 0.63134, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3259 - acc: 1.000 - ETA: 1s - loss: 0.3596 - acc: 0.933 - ETA: 1s - loss: 0.3926 - acc: 0.914 - ETA: 0s - loss: 0.4013 - acc: 0.911 - ETA: 0s - loss: 0.3866 - acc: 0.918 - ETA: 1s - loss: 0.3901 - acc: 0.916 - ETA: 1s - loss: 0.3909 - acc: 0.913 - ETA: 1s - loss: 0.3853 - acc: 0.913 - ETA: 0s - loss: 0.3936 - acc: 0.911 - ETA: 0s - loss: 0.3905 - acc: 0.914 - ETA: 0s - loss: 0.3868 - acc: 0.916 - ETA: 0s - loss: 0.3884 - acc: 0.914 - ETA: 0s - loss: 0.3817 - acc: 0.916 - ETA: 0s - loss: 0.3818 - acc: 0.917 - ETA: 0s - loss: 0.3771 - acc: 0.919 - ETA: 0s - loss: 0.3779 - acc: 0.918 - ETA: 0s - loss: 0.3775 - acc: 0.918 - ETA: 0s - loss: 0.3756 - acc: 0.919 - ETA: 0s - loss: 0.3744 - acc: 0.919 - ETA: 0s - loss: 0.3744 - acc: 0.919 - ETA: 0s - loss: 0.3766 - acc: 0.919 - ETA: 0s - loss: 0.3771 - acc: 0.917 - ETA: 0s - loss: 0.3766 - acc: 0.917 - ETA: 0s - loss: 0.3760 - acc: 0.916 - ETA: 0s - loss: 0.3783 - acc: 0.914 - ETA: 0s - loss: 0.3772 - acc: 0.914 - ETA: 0s - loss: 0.3767 - acc: 0.916 - ETA: 0s - loss: 0.3751 - acc: 0.916 - ETA: 0s - loss: 0.3777 - acc: 0.915 - 2s 239us/step - loss: 0.3766 - acc: 0.9156 - val_loss: 0.6187 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.63134 to 0.61872, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.3298 - acc: 0.966 - ETA: 2s - loss: 0.3669 - acc: 0.927 - ETA: 1s - loss: 0.3598 - acc: 0.938 - ETA: 1s - loss: 0.3519 - acc: 0.938 - ETA: 1s - loss: 0.3523 - acc: 0.937 - ETA: 1s - loss: 0.3676 - acc: 0.933 - ETA: 1s - loss: 0.3631 - acc: 0.935 - ETA: 1s - loss: 0.3606 - acc: 0.933 - ETA: 1s - loss: 0.3537 - acc: 0.934 - ETA: 1s - loss: 0.3643 - acc: 0.927 - ETA: 1s - loss: 0.3593 - acc: 0.930 - ETA: 1s - loss: 0.3547 - acc: 0.930 - ETA: 1s - loss: 0.3498 - acc: 0.931 - ETA: 1s - loss: 0.3553 - acc: 0.927 - ETA: 1s - loss: 0.3576 - acc: 0.926 - ETA: 0s - loss: 0.3576 - acc: 0.926 - ETA: 0s - loss: 0.3593 - acc: 0.925 - ETA: 0s - loss: 0.3582 - acc: 0.926 - ETA: 0s - loss: 0.3579 - acc: 0.925 - ETA: 0s - loss: 0.3568 - acc: 0.925 - ETA: 0s - loss: 0.3572 - acc: 0.925 - ETA: 0s - loss: 0.3555 - acc: 0.924 - ETA: 0s - loss: 0.3569 - acc: 0.923 - ETA: 0s - loss: 0.3543 - acc: 0.925 - ETA: 0s - loss: 0.3524 - acc: 0.925 - ETA: 0s - loss: 0.3525 - acc: 0.926 - ETA: 0s - loss: 0.3526 - acc: 0.926 - ETA: 0s - loss: 0.3521 - acc: 0.927 - ETA: 0s - loss: 0.3506 - acc: 0.927 - ETA: 0s - loss: 0.3499 - acc: 0.928 - ETA: 0s - loss: 0.3511 - acc: 0.928 - ETA: 0s - loss: 0.3510 - acc: 0.928 - 2s 265us/step - loss: 0.3509 - acc: 0.9284 - val_loss: 0.6044 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.61872 to 0.60436, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.2637 - acc: 0.966 - ETA: 1s - loss: 0.3338 - acc: 0.929 - ETA: 1s - loss: 0.3580 - acc: 0.919 - ETA: 1s - loss: 0.3322 - acc: 0.925 - ETA: 1s - loss: 0.3534 - acc: 0.916 - ETA: 1s - loss: 0.3472 - acc: 0.922 - ETA: 1s - loss: 0.3387 - acc: 0.926 - ETA: 1s - loss: 0.3279 - acc: 0.931 - ETA: 1s - loss: 0.3284 - acc: 0.932 - ETA: 1s - loss: 0.3320 - acc: 0.930 - ETA: 1s - loss: 0.3268 - acc: 0.932 - ETA: 1s - loss: 0.3272 - acc: 0.934 - ETA: 1s - loss: 0.3269 - acc: 0.935 - ETA: 1s - loss: 0.3280 - acc: 0.933 - ETA: 1s - loss: 0.3289 - acc: 0.933 - ETA: 0s - loss: 0.3281 - acc: 0.933 - ETA: 0s - loss: 0.3301 - acc: 0.932 - ETA: 0s - loss: 0.3313 - acc: 0.932 - ETA: 0s - loss: 0.3317 - acc: 0.932 - ETA: 0s - loss: 0.3295 - acc: 0.932 - ETA: 0s - loss: 0.3311 - acc: 0.932 - ETA: 0s - loss: 0.3307 - acc: 0.931 - ETA: 0s - loss: 0.3307 - acc: 0.931 - ETA: 0s - loss: 0.3331 - acc: 0.930 - ETA: 0s - loss: 0.3333 - acc: 0.930 - ETA: 0s - loss: 0.3331 - acc: 0.930 - ETA: 0s - loss: 0.3339 - acc: 0.930 - ETA: 0s - loss: 0.3331 - acc: 0.930 - ETA: 0s - loss: 0.3314 - acc: 0.931 - ETA: 0s - loss: 0.3322 - acc: 0.930 - ETA: 0s - loss: 0.3311 - acc: 0.931 - ETA: 0s - loss: 0.3301 - acc: 0.931 - 2s 262us/step - loss: 0.3303 - acc: 0.9316 - val_loss: 0.5983 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60436 to 0.59832, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3056 - acc: 0.966 - ETA: 1s - loss: 0.3242 - acc: 0.945 - ETA: 1s - loss: 0.3014 - acc: 0.951 - ETA: 1s - loss: 0.2851 - acc: 0.954 - ETA: 1s - loss: 0.2825 - acc: 0.954 - ETA: 1s - loss: 0.2822 - acc: 0.954 - ETA: 1s - loss: 0.2875 - acc: 0.952 - ETA: 1s - loss: 0.2843 - acc: 0.952 - ETA: 1s - loss: 0.2901 - acc: 0.949 - ETA: 1s - loss: 0.2888 - acc: 0.949 - ETA: 1s - loss: 0.2855 - acc: 0.950 - ETA: 1s - loss: 0.2939 - acc: 0.946 - ETA: 0s - loss: 0.2961 - acc: 0.944 - ETA: 0s - loss: 0.2999 - acc: 0.943 - ETA: 0s - loss: 0.3008 - acc: 0.944 - ETA: 0s - loss: 0.2998 - acc: 0.944 - ETA: 0s - loss: 0.3006 - acc: 0.943 - ETA: 0s - loss: 0.3017 - acc: 0.941 - ETA: 0s - loss: 0.3024 - acc: 0.941 - ETA: 0s - loss: 0.3035 - acc: 0.941 - ETA: 0s - loss: 0.3042 - acc: 0.940 - ETA: 0s - loss: 0.3047 - acc: 0.941 - ETA: 0s - loss: 0.3066 - acc: 0.940 - ETA: 0s - loss: 0.3051 - acc: 0.940 - ETA: 0s - loss: 0.3058 - acc: 0.941 - ETA: 0s - loss: 0.3042 - acc: 0.941 - ETA: 0s - loss: 0.3046 - acc: 0.940 - ETA: 0s - loss: 0.3062 - acc: 0.940 - ETA: 0s - loss: 0.3068 - acc: 0.939 - ETA: 0s - loss: 0.3067 - acc: 0.939 - ETA: 0s - loss: 0.3075 - acc: 0.938 - ETA: 0s - loss: 0.3084 - acc: 0.937 - 2s 262us/step - loss: 0.3089 - acc: 0.9376 - val_loss: 0.5887 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.59832 to 0.58867, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 0.2238 - acc: 0.966 - ETA: 1s - loss: 0.2778 - acc: 0.960 - ETA: 1s - loss: 0.2812 - acc: 0.960 - ETA: 1s - loss: 0.2805 - acc: 0.952 - ETA: 1s - loss: 0.2792 - acc: 0.946 - ETA: 1s - loss: 0.2756 - acc: 0.948 - ETA: 1s - loss: 0.2802 - acc: 0.949 - ETA: 1s - loss: 0.2877 - acc: 0.944 - ETA: 1s - loss: 0.2920 - acc: 0.944 - ETA: 1s - loss: 0.2955 - acc: 0.941 - ETA: 1s - loss: 0.2992 - acc: 0.941 - ETA: 0s - loss: 0.2997 - acc: 0.938 - ETA: 0s - loss: 0.3016 - acc: 0.937 - ETA: 0s - loss: 0.3002 - acc: 0.939 - ETA: 0s - loss: 0.3024 - acc: 0.939 - ETA: 0s - loss: 0.3013 - acc: 0.939 - ETA: 0s - loss: 0.3011 - acc: 0.939 - ETA: 0s - loss: 0.2996 - acc: 0.940 - ETA: 0s - loss: 0.2956 - acc: 0.941 - ETA: 0s - loss: 0.2955 - acc: 0.941 - ETA: 0s - loss: 0.2975 - acc: 0.941 - ETA: 0s - loss: 0.2985 - acc: 0.941 - ETA: 0s - loss: 0.2999 - acc: 0.941 - ETA: 0s - loss: 0.2996 - acc: 0.940 - ETA: 0s - loss: 0.3017 - acc: 0.939 - ETA: 0s - loss: 0.3016 - acc: 0.938 - ETA: 0s - loss: 0.3014 - acc: 0.938 - ETA: 0s - loss: 0.2981 - acc: 0.940 - ETA: 0s - loss: 0.2994 - acc: 0.940 - 2s 244us/step - loss: 0.2995 - acc: 0.9404 - val_loss: 0.5802 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.58867 to 0.58018, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.2958 - acc: 0.966 - ETA: 1s - loss: 0.3001 - acc: 0.947 - ETA: 1s - loss: 0.2627 - acc: 0.957 - ETA: 1s - loss: 0.2476 - acc: 0.963 - ETA: 1s - loss: 0.2640 - acc: 0.954 - ETA: 1s - loss: 0.2720 - acc: 0.945 - ETA: 1s - loss: 0.2607 - acc: 0.951 - ETA: 1s - loss: 0.2574 - acc: 0.953 - ETA: 1s - loss: 0.2540 - acc: 0.955 - ETA: 1s - loss: 0.2633 - acc: 0.948 - ETA: 1s - loss: 0.2666 - acc: 0.949 - ETA: 1s - loss: 0.2682 - acc: 0.949 - ETA: 1s - loss: 0.2669 - acc: 0.949 - ETA: 1s - loss: 0.2678 - acc: 0.948 - ETA: 1s - loss: 0.2658 - acc: 0.949 - ETA: 0s - loss: 0.2665 - acc: 0.949 - ETA: 0s - loss: 0.2673 - acc: 0.948 - ETA: 0s - loss: 0.2697 - acc: 0.947 - ETA: 0s - loss: 0.2759 - acc: 0.945 - ETA: 0s - loss: 0.2771 - acc: 0.945 - ETA: 0s - loss: 0.2787 - acc: 0.945 - ETA: 0s - loss: 0.2787 - acc: 0.944 - ETA: 0s - loss: 0.2783 - acc: 0.945 - ETA: 0s - loss: 0.2783 - acc: 0.945 - ETA: 0s - loss: 0.2797 - acc: 0.945 - ETA: 0s - loss: 0.2808 - acc: 0.944 - ETA: 0s - loss: 0.2801 - acc: 0.945 - ETA: 0s - loss: 0.2804 - acc: 0.945 - ETA: 0s - loss: 0.2795 - acc: 0.945 - ETA: 0s - loss: 0.2802 - acc: 0.945 - ETA: 0s - loss: 0.2810 - acc: 0.944 - 2s 256us/step - loss: 0.2817 - acc: 0.9440 - val_loss: 0.5757 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.58018 to 0.57568, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3263 - acc: 0.900 - ETA: 1s - loss: 0.2563 - acc: 0.947 - ETA: 1s - loss: 0.2640 - acc: 0.935 - ETA: 1s - loss: 0.2597 - acc: 0.945 - ETA: 1s - loss: 0.2728 - acc: 0.939 - ETA: 1s - loss: 0.2759 - acc: 0.936 - ETA: 1s - loss: 0.2683 - acc: 0.942 - ETA: 1s - loss: 0.2640 - acc: 0.945 - ETA: 1s - loss: 0.2731 - acc: 0.940 - ETA: 1s - loss: 0.2744 - acc: 0.940 - ETA: 1s - loss: 0.2677 - acc: 0.942 - ETA: 0s - loss: 0.2704 - acc: 0.942 - ETA: 0s - loss: 0.2744 - acc: 0.941 - ETA: 0s - loss: 0.2746 - acc: 0.942 - ETA: 0s - loss: 0.2757 - acc: 0.942 - ETA: 0s - loss: 0.2751 - acc: 0.942 - ETA: 0s - loss: 0.2734 - acc: 0.943 - ETA: 0s - loss: 0.2707 - acc: 0.944 - ETA: 0s - loss: 0.2686 - acc: 0.946 - ETA: 0s - loss: 0.2683 - acc: 0.946 - ETA: 0s - loss: 0.2691 - acc: 0.946 - ETA: 0s - loss: 0.2671 - acc: 0.947 - ETA: 0s - loss: 0.2672 - acc: 0.947 - ETA: 0s - loss: 0.2655 - acc: 0.948 - ETA: 0s - loss: 0.2656 - acc: 0.948 - ETA: 0s - loss: 0.2663 - acc: 0.948 - ETA: 0s - loss: 0.2666 - acc: 0.947 - ETA: 0s - loss: 0.2677 - acc: 0.947 - ETA: 0s - loss: 0.2684 - acc: 0.947 - ETA: 0s - loss: 0.2674 - acc: 0.947 - ETA: 0s - loss: 0.2685 - acc: 0.947 - ETA: 0s - loss: 0.2689 - acc: 0.947 - 2s 260us/step - loss: 0.2698 - acc: 0.9464 - val_loss: 0.5704 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.57568 to 0.57037, saving model to saved_models/weights.best1.new_model.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 0.3348 - acc: 0.900 - ETA: 1s - loss: 0.3026 - acc: 0.933 - ETA: 1s - loss: 0.2817 - acc: 0.940 - ETA: 1s - loss: 0.2734 - acc: 0.945 - ETA: 1s - loss: 0.2654 - acc: 0.951 - ETA: 1s - loss: 0.2632 - acc: 0.951 - ETA: 1s - loss: 0.2620 - acc: 0.951 - ETA: 1s - loss: 0.2579 - acc: 0.955 - ETA: 1s - loss: 0.2586 - acc: 0.954 - ETA: 1s - loss: 0.2567 - acc: 0.955 - ETA: 1s - loss: 0.2567 - acc: 0.955 - ETA: 1s - loss: 0.2559 - acc: 0.955 - ETA: 1s - loss: 0.2582 - acc: 0.953 - ETA: 0s - loss: 0.2601 - acc: 0.952 - ETA: 0s - loss: 0.2591 - acc: 0.952 - ETA: 0s - loss: 0.2588 - acc: 0.952 - ETA: 0s - loss: 0.2578 - acc: 0.952 - ETA: 0s - loss: 0.2572 - acc: 0.952 - ETA: 0s - loss: 0.2590 - acc: 0.951 - ETA: 0s - loss: 0.2627 - acc: 0.950 - ETA: 0s - loss: 0.2600 - acc: 0.950 - ETA: 0s - loss: 0.2605 - acc: 0.950 - ETA: 0s - loss: 0.2592 - acc: 0.950 - ETA: 0s - loss: 0.2583 - acc: 0.951 - ETA: 0s - loss: 0.2578 - acc: 0.951 - ETA: 0s - loss: 0.2570 - acc: 0.951 - ETA: 0s - loss: 0.2571 - acc: 0.951 - ETA: 0s - loss: 0.2579 - acc: 0.951 - ETA: 0s - loss: 0.2562 - acc: 0.951 - ETA: 0s - loss: 0.2549 - acc: 0.952 - ETA: 0s - loss: 0.2550 - acc: 0.953 - 2s 257us/step - loss: 0.2548 - acc: 0.9531 - val_loss: 0.5675 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.57037 to 0.56755, saving model to saved_models/weights.best1.new_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x42499be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best1.new_model.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "new_model.fit(train_new_model, train_targets, \n",
    "          validation_data=(valid_new_model, valid_targets),\n",
    "          epochs=20, batch_size=30, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "new_model.load_weights('saved_models/weights.best1.new_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 83.9713%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "new_model_predictions = [np.argmax(new_model.predict(np.expand_dims(feature, axis=0))) for feature in test_new_model]\n",
    "\n",
    "# report test accuracy\n",
    "new_model_test_accuracy = 100*np.sum(np.array(new_model_predictions)==np.argmax(test_targets, axis=1))/len(new_model_predictions)\n",
    "print('Test accuracy: %.4f%%' % new_model_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19  (7,7,512) -> 68%\n",
    "##ResNet50, sdg (1,1,2048) --> 83%\n",
    "##Inception , sdg (5,5,2048) --> 82.4%\n",
    "##Exception, sdg, (7,7,2048), 272K Parameters ---> 84%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Breed Name is Labrador_retriever\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Labrador_retriever'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "\n",
    "def Xception_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = new_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    print(\"Xception Dog Breed Name is\", dog_names[np.argmax(predicted_vector)])\n",
    "    return dog_names[np.argmax(predicted_vector)]\n",
    "\n",
    "###Execute Xception_Predict_breed\n",
    "Xception_predict_breed(\"images/Labrador_retriever_06455.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Labrador_retriever'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "\n",
    "def predict_breed(cnn_model, img_path):\n",
    "    #print(\"CNN Pre Training Model:\", cnn_model)\n",
    "    # extract bottleneck features\n",
    "    if (cnn_model == 'Resnet50'):\n",
    "        bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    elif (cnn_model == \"Inception3D\"):\n",
    "        bottleneck_feature = extract_Inception3D(path_to_tensor(img_path))\n",
    "    else:\n",
    "        bottleneck_feature = extract_Exception(path_to_tensor(img_path))\n",
    "    \n",
    "    # obtain predicted vector\n",
    "    predicted_vector = new_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    #print(\"Resent50 Dog Breed Name is\", dog_names[np.argmax(predicted_vector)])\n",
    "    return dog_names[np.argmax(predicted_vector)]\n",
    "\n",
    "###Execute Resenet50_Predict_breed\n",
    "predict_breed(\"Resnet50\", \"images/Labrador_retriever_06455.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Human face found, The resembling dog Breed is: Norwegian_lundehund\n"
     ]
    }
   ],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def detect_picture(cnn_model, img_path):\n",
    "    ## if dog is detected, find out a breed\n",
    "    if (dog_detector(img_path)):\n",
    "        print(\"Hello, Dog is found. Dog breed is\", predict_breed(cnn_model, img_path))\n",
    "    elif face_detector(img_path):\n",
    "        print(\"Hello Human face found, The resembling dog Breed is:\", predict_breed(cnn_model, img_path))\n",
    "    else:\n",
    "        print(\"Error: Neither Dog nor Human found\")\n",
    "\n",
    "detect_picture(\"Resnet50\",\"images/sample_human_output.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Input file is My  Face:\n",
      "Hello Human face found, The resembling dog Breed is: Basenji\n",
      "2) Input file is Jackie Chan  Face:\n",
      "Hello Human face found, The resembling dog Breed is: Norwegian_lundehund\n",
      "3) Input file is Bulldog:\n",
      "Hello, Dog is found. Dog breed is Bulldog\n",
      "4) Input file is a Dog:\n",
      "Hello, Dog is found. Dog breed is Boxer\n",
      "5) Input file is a Cat:\n",
      "Error: Neither Dog nor Human found\n",
      "6) Input file is a Elephant:\n",
      "Error: Neither Dog nor Human found\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "\n",
    "###Image 1 - my face\n",
    "print(\"1) Input file is My  Face:\")\n",
    "detect_picture(\"Resnet50\",\"images/venkat.jpg\")\n",
    "\n",
    "###Image 2 - Jackie Chan face\n",
    "print(\"2) Input file is Jackie Chan  Face:\")\n",
    "detect_picture(\"Resnet50\",\"images/jackiechan.jpg\")\n",
    "\n",
    "###Image 3 - Bulldog\n",
    "print(\"3) Input file is Bulldog:\")\n",
    "detect_picture(\"Resnet50\",\"images/bulldog.jpg\")\n",
    "\n",
    "###Image 4 - Puggle \n",
    "print(\"4) Input file is a Dog:\")\n",
    "detect_picture(\"Resnet50\",\"images/puggle.jpg\")\n",
    "\n",
    "###Image5 - cat\n",
    "print(\"5) Input file is a Cat:\")\n",
    "detect_picture(\"Resnet50\",\"images/cat1.jpg\")\n",
    "\n",
    "###Image 6 - deer\n",
    "print(\"6) Input file is a Elephant:\")\n",
    "detect_picture(\"Resnet50\",\"images/elephant.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
